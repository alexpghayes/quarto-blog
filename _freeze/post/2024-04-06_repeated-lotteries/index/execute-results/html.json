{
  "hash": "733acd820566da331478f19d556bfb5c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Fairness in repeated lotteries\"\nsubtitle: |\n    a tidymodels workflow in python using list columns in pandas dataframes\ndate: \"2024-04-06\"\njupyter: lottery\ncategories: [python]\n---\n\n\n**TODO**: export requirements.txt file for lottery environment\n\n::: {#5ddb8aa0 .cell execution_count=1}\n``` {.python .cell-code}\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as onp\nimport seaborn as sns\nfrom copy import deepcopy\nfrom jaxopt import ProjectedGradient\n```\n:::\n\n\n::: {#b1762763 .cell execution_count=2}\n``` {.python .cell-code}\ndef randomized_pipage_rounding(x):\n    '''\n    Fast pipage rounding implementation for uniform matroid\n    \n    From https://github.com/bwilder0/clusternet\n    See Randomized Pipage Rounding algorithm in http://arxiv.org/abs/1711.01566\n    '''\n    i = 0\n    j = 1\n    x = onp.array(x) # does this make a deepcopy?\n    x = deepcopy(x) # x.clone()\n    for t in range(len(x)-1):\n        if x[i] == 0 and x[j] == 0:\n            i = max((i,j)) + 1\n        elif x[i] + x[j] < 1:\n            if onp.random.rand() < x[i]/(x[i] + x[j]):\n                x[i] = x[i] + x[j]\n                x[j] = 0\n                j = max((i,j)) + 1\n            else:\n                x[j] = x[i] + x[j]\n                x[i] = 0\n                i = max((i,j)) + 1\n        else:\n            if onp.random.rand() < (1 - x[j])/(2 - x[i] - x[j]):\n                x[j] = x[i] + x[j] - 1\n                x[i] = 1\n                i = max((i,j)) + 1\n\n            else:\n                x[i] = x[i] + x[j] - 1\n                x[j] = 1\n                j = max((i,j)) + 1\n    return x\n```\n:::\n\n\n::: {#61d21b9c .cell execution_count=3}\n``` {.python .cell-code}\nx = jnp.array([0.1, 0.3, 0.4, 0.8, 0.4])\nsum(x)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nArray(2., dtype=float32)\n```\n:::\n:::\n\n\n::: {#eaac1b16 .cell execution_count=4}\n``` {.python .cell-code}\n# quick confirmation the rounding() produces the correct marginals\n\nreps = 1000\ntotal = jnp.zeros_like(x)\nfor i in range(reps):\n    total += randomized_pipage_rounding(x)\ntotal / reps - x\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nArray([-0.013     , -0.00500003,  0.01899999,  0.00199997, -0.00299999],      dtype=float32)\n```\n:::\n:::\n\n\n::: {#7b596b9b .cell execution_count=5}\n``` {.python .cell-code}\ndef prospective_loss(prob_win, winners, num_winners_per_lottery):\n    \"\"\"\n    Bailey: this implements a slightly different loss than the\n    one you wrote up in your initial note. In your initial note,\n    the expected loss at time t is unconditional on the actual\n    outcomes of the first (t-1) rounds, but here I use that\n    information. I'll write this loss at nice at some point.\n    \"\"\"\n    \n    # if jnp.any(prob_win < 0) or jnp.any(prob_win > 1):\n    #     raise ValueError(\"Entries of `prob_win` must be contained in [0, 1]\")\n    \n    # if num_winners_per_lottery != prob_win.sum():\n    #     raise ValueError(f\"`prob_win` ({prob_win.sum()}) must sum to the `num_winners_per_lottery` ({num_winners_per_lottery})\")\n    \n    total_attendances = winners.sum(axis = 1)\n    num_entrants = winners.shape[0]\n    lotteries_so_far = winners.shape[1]\n\n    # fair_allocation_so_far = num_winners_per_lottery * lotteries_so_far / num_entrants\n    # print(f\"num slots per lottery {num_winners_per_lottery}, attendees per lottery: {num_entrants}, lotteries so far: {lotteries_so_far}, fair allocation so far = {fair_allocation_so_far}\")\n    \n    # loss_so_far = jnp.max(jnp.abs(total_attendances - fair_allocation_so_far))\n    expected_attendances = total_attendances + prob_win\n    fair_allocation = num_winners_per_lottery * (lotteries_so_far + 1) / num_entrants\n\n    # we get hit by jensen's inequality when computing this expected loss given prob_win\n    # but i'm going to ignore that for now\n    individual_deviations = expected_attendances - fair_allocation\n    expected_loss = jnp.max(jnp.abs(individual_deviations))\n    return expected_loss\n```\n:::\n\n\n::: {#1e04d01a .cell execution_count=6}\n``` {.python .cell-code}\npi_test = jnp.array([0.6, 0.4, 0.5, 0.1, 0.01, 0.89, 0.5])\nnum_winners_per_lottery = pi_test.sum()\n\nwinners = [randomized_pipage_rounding(jnp.array(pi_test)) for _ in range(100)]\nwinners = jnp.column_stack(winners)\n\nprospective_loss(pi_test, winners, num_winners_per_lottery=3)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nArray(46.604286, dtype=float32)\n```\n:::\n:::\n\n\n::: {#5bf7c151 .cell execution_count=7}\n``` {.python .cell-code}\ndef projection_simplex_clip(r, simplex_sum):\n    \"\"\"\n    Projects a real-valued vector onto the simplex, with the constraint the elements\n    of the simplex sum to ``simplex_sum,`` and all entries of the vector must\n    be between 0 and 1.\n    \n    Modified from http://www.ryanhmckenna.com/2019/10/projecting-onto-probability-simplex.html\n    \n    jaxopt.projection.projection_simplex(scale=num_winners_per_lottery) doesn't quite work for us\n    because it allows potential attendees to be assigned marginal probabilities of attendance\n    larger than one\n    \n    \"\"\"\n    lambdas = jnp.append(-r, 1 - r)\n    idx = jnp.argsort(lambdas)\n    lambdas = lambdas[idx]\n    active = jnp.cumsum((idx < r.size)*2 - 1)[:-1]\n    diffs = jnp.diff(lambdas, n=1)\n    totals = jnp.cumsum(active*diffs)\n    # i = jnp.searchsorted(totals, 1.0)\n    # lam = (1 - totals[i]) / active[i] + lambdas[i+1]\n    i = jnp.searchsorted(totals, simplex_sum)\n    lam = (simplex_sum - totals[i]) / active[i] + lambdas[i+1]\n    # NOTE: i added some additional renormalization here\n    p = jnp.clip(r + lam, 0, 1)\n    return p / p.sum() * simplex_sum\n```\n:::\n\n\n::: {#aa6088aa .cell execution_count=8}\n``` {.python .cell-code}\nnum_winners_per_lottery = 3\n\npg = ProjectedGradient(fun=prospective_loss, projection=projection_simplex_clip)\npg_sol = pg.run(init_params = pi_test, hyperparams_proj=num_winners_per_lottery, winners=winners, num_winners_per_lottery=num_winners_per_lottery)\npg_sol.params\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nArray([0.74833333, 0.5483333 , 0.6483333 , 0.2483334 , 0.15833342,\n       0.        , 0.6483333 ], dtype=float32)\n```\n:::\n:::\n\n\n::: {#c2f1ac72 .cell execution_count=9}\n``` {.python .cell-code}\nprint(pg_sol.params.sum())\nonp.round(randomized_pipage_rounding(pg_sol.params))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n3.0\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([1., 0., 1., 1., 0., 0., 0.], dtype=float32)\n```\n:::\n:::\n\n\n::: {#162ab0c9 .cell execution_count=10}\n``` {.python .cell-code}\n# simulation to investigate properties of procedure\n\nnum_lotteries = 25\nnum_winners_per_lottery = 7\nnum_entrants = 30\n\nwin_expectation = onp.zeros((num_entrants, num_lotteries))\nwinners = onp.zeros_like(win_expectation)\n\nwin_expectation_equal = onp.ones(num_entrants) * num_winners_per_lottery / num_entrants\n\nwin_expectation[:, 0] = win_expectation_equal\n\npg = ProjectedGradient(fun=prospective_loss, projection=projection_simplex_clip)\n\nfor lottery in range(num_lotteries):\n    \n    # run the lottery\n    winners[:, lottery] = onp.round(randomized_pipage_rounding(win_expectation[:, lottery]))\n    \n    # determine fair probabilities for next lottery    \n    next_win_expectations = pg.run(init_params = win_expectation_equal, hyperparams_proj=num_winners_per_lottery, winners=winners, num_winners_per_lottery=num_winners_per_lottery)\n    \n    is_last_lottery = (lottery == num_lotteries - 1)\n    if not is_last_lottery:\n        win_expectation[:, lottery + 1] = onp.array(next_win_expectations.params)\n```\n:::\n\n\n::: {#996542f9 .cell execution_count=11}\n``` {.python .cell-code}\n# attendances per lottery, should be num_winners_per_lottery every time\nwinners.sum(axis = 0)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\narray([7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n       7., 7., 7., 7., 7., 7., 7., 7.])\n```\n:::\n:::\n\n\n::: {#b8681499 .cell execution_count=12}\n``` {.python .cell-code}\n# attendance per attendee. should be fairly even across attendees\nwinners.sum(axis = 1)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\narray([6., 6., 6., 6., 5., 6., 6., 5., 6., 6., 5., 6., 6., 6., 6., 6., 6.,\n       6., 6., 6., 6., 6., 6., 5., 6., 6., 5., 6., 6., 6.])\n```\n:::\n:::\n\n\n::: {#720481a9 .cell execution_count=13}\n``` {.python .cell-code}\nax = sns.heatmap(win_expectation, linewidth=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){width=533 height=420}\n:::\n:::\n\n\n::: {#4dca3684 .cell execution_count=14}\n``` {.python .cell-code}\n# total marginal probability mass assigned each lottery\nwin_expectation.sum(axis=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\narray([7.        , 7.00000039, 7.        , 7.00000012, 7.0000006 ,\n       7.00000003, 7.00000012, 6.99999988, 6.99999949, 6.99999985,\n       6.99999988, 6.99999958, 6.99999958, 7.00000073, 6.99999988,\n       7.00000048, 7.        , 6.99999915, 7.00000048, 7.00000009,\n       6.99999988, 7.0000011 , 7.00000036, 6.99999934, 7.00000048])\n```\n:::\n:::\n\n\n::: {#07cb7c51 .cell execution_count=15}\n``` {.python .cell-code}\n# total marginal probability mass assigned to each attendee\nwin_expectation.sum(axis = 1)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\narray([4.57888748, 6.07532487, 7.18228661, 4.12501261, 6.32683808,\n       4.54247237, 8.76104939, 8.31139519, 4.8232286 , 7.30260447,\n       5.13100523, 5.23352999, 4.9215278 , 4.36148939, 6.94566456,\n       3.99232633, 5.89358636, 7.15387786, 4.91817177, 7.60044921,\n       6.91769838, 6.35924555, 3.48442634, 6.98504689, 5.45616082,\n       3.76619042, 4.88982088, 7.51828817, 6.09043877, 5.35195708])\n```\n:::\n:::\n\n\n::: {#49f46fde .cell execution_count=16}\n``` {.python .cell-code}\nax = sns.heatmap(winners, linewidth=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){width=533 height=420}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}