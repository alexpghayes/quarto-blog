{
  "hash": "509c77802ae4807eb45096f001130f23",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"checking convergence with simulations\"\nsubtitle: |\n  log-log plots are good\ndate: \"2022-07-15\"\ncategories: [statistical software]\ndraft: true\n---\n\n\n\n\n**TODO**: median, median absolute deviation because some estimators converge in distribution but not in $L_p$ norms / moments. Thomas Lumley has a tweet or bluesky post about this.\n\nSuppose you have some estimator $\\hat \\theta$ of $\\theta$ and you prove that \n\n$$\n\\left \\Vert \\hat \\theta - \\theta \\right \\Vert = o_p \\left( \\frac{1}{\\sqrt n} \\right)\n$$\n\nunder some model, or some similar bound. In my experience it's essentially mandatory to explicitly compute $\\left \\Vert \\hat \\theta - \\theta \\right \\Vert$ and check if the estimator actually works, because a surprising amount of the time either things don't and I get to go through the painful process of ✨ learning ✨.\n\nAlso, even when things are working, it can be surprisingly difficult to tell that this is actually the case. Ideally I'd average $\\left \\Vert \\hat \\theta - \\theta \\right \\Vert$ over a bunch of simulations and have a nice estimate of $\\mathbb E \\left \\Vert \\hat \\theta - \\theta \\right \\Vert$ that I can plot against $n$, as below:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\ntheme_set(theme_minimal(14))\n\nn <- seq(10, 100, length.out = 15)\nalpha <- 1/4\n\nnice_error <- n^(-alpha)\n\nggplot() +\n  aes(n, nice_error) +\n  geom_line() +\n  geom_point() +\n  labs(\n    x = \"Number of data points\",\n    y = \"Error\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\nIt turns out that I have horrible intuition about the function $f(x) = 1 / \\sqrt{x}$ though, so I can never tell if a plot like this is showing asymptotic convergence or if there's some possibility of asymptotic bias. This is especially the case when $\\hat \\theta$ is computationally expensive to compute, so I can't push $n$ as high as I'd like, such that my smallest error is far from zero.\n\nOne thing that helps a lot here is to plot $\\log \\left(\\left \\Vert \\hat \\theta - \\theta \\right \\Vert \\right)$ as a function of $\\log n$, and check if the resulting line has constant negative slope. The idea here is that if $\\left \\Vert \\hat \\theta - \\theta \\right \\Vert \\le C n^{-\\alpha}$ with high probability, or whatever bound you prove, then $\\log \\left(\\left \\Vert \\hat \\theta - \\theta \\right \\Vert \\right) \\le -C \\alpha \\log n$. So we can just check if things are linear after transformation, and this is easier to do visually.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  aes(n, nice_error) +\n  geom_line() +\n  geom_point() +\n  scale_x_log10() +\n  scale_y_log10() +\n  labs(\n    x = \"log(Number of data points)\",\n    y = \"log(Error)\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\nThis is particularly helpful when working with noisy estimates of some quantity like $\\mathbb E \\left \\Vert \\hat \\theta - \\theta \\right \\Vert$. In this setting the untransformed plot can be tricky to read. Here's an example of noisy estimates that are converging, but you will never convince yourself or a referee that this is the case:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\n\nset.seed(26)\n\nerror <- abs(rnorm(length(n), mean = n^(-alpha), sd = n^(-2 * alpha)))\nerror[1] <- 1\n\nggplot() +\n  aes(n, error) +\n  geom_line() +\n  geom_point() +\n  labs(\n    x = \"Number of data points\",\n    y = \"Error\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\nwhen do you need a better estimate of the error, and when do you need to push $n$ higher. if you can't push $n$ higher and you think everything else is right, \n\nlaziness in having few reps per $n$ but actually increasing the reps per $n$ is better than pushing $n$ higher. also: increasing the SNR ratio of the simulations.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnoisy_log_log <- ggplot() +\n  aes(n, error) +\n  geom_line() +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  labs(\n    x = \"log(Number of data points)\",\n    y = \"log(Error)\"\n  )\n\nnoisy_log_log\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\nlog-log plot here suggests that things are working, just need to turn up the sims per $n$, instead of increasing $n$ or the SNR, which would be my normal responses, which at some point i just run out of ram and this no longer works\n\nConverging at some rate, but might need to check the slope to know if it's converging at *your* rate. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}