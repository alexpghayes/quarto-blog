{
  "hash": "6a024b4c1baa175a8baca0dbaeaedb7a",
  "result": {
    "markdown": "---\ntitle: \"numerical gradient checks\"\nsubtitle: |\n  how to use a computer to check your derivative calculations\ndate: \"2017-10-18\"\ncategories: [calculus]\nexecute:\n  echo: true\n  message: false\n  warning: false\n---\n\n## Motivation\n\nSuppose you have some loss function $\\mathcal{L}(\\beta) : \\mathbb{R}^n \\to \\mathbb{R}$ you want to minimize with respect to some model parameters $\\beta$. You understand how gradient descent works and you have a correct implementation of $\\mathcal{L}$ but aren't sure if you took the gradient correctly or implemented it correctly in code.\n\n## Solution\n\nWe can compare our implemention of the gradient of $\\mathcal{L}$ to a finite difference approximation of the gradient. Recall that the gradient of $\\mathcal{L}$, $\\nabla_\\mathcal{L}$, in a direction $d \\in \\mathbb{R}^n$ at a point $x \\in \\mathbb{R}^n$ is defined as\n\n$$d^T \\nabla_\\mathcal{L}(x) = \\lim_{\\epsilon \\to 0} \\frac{\\mathcal{L}(x + \\epsilon \\cdot d) - \\mathcal{L}(x - \\epsilon \\cdot d)}{2 \\epsilon}$$\n\nIf we take $\\epsilon$ to be fixed and small, we can use this formula to approximate the gradient in any direction. By approximating the gradient in each unit direction, we construct an approximation of the gradient of $\\mathcal{L}$ at a particular point $x$.\n\n## Example: Checking the gradient of linear regression\n\nSuppose that we have $n = 20$ data points in $\\mathbb{R}^2$ with responses $y \\in \\mathbb{R}$. Linear regression assumes the responses $y$ are related linearly to the data matrix $X$ via the equation\n\n$$y = X \\beta + \\epsilon$$\n\nWe want to find an estimate $\\hat \\beta$ that minimizes the sum of squared error of the predicted values $\\hat y = X \\hat \\beta$\n\n$$\\mathcal{L}(\\beta) = \\frac{1}{2n} \\sum_i (y_i - \\hat y_i)^2 = \\frac{1}{2n} \\sum_i (y_i - x_i \\beta)^2 = \\frac{1}{2n} (y - X \\beta)^T (y - X \\beta)$$\n\nIn the final step above we recognize that the sum of squared residuals can be written as a dot product. Next we'd like to the gradient of this dot product. There's a beautiful explanation of how to take the gradient of a quadratic form [here](http://michael.orlitzky.com/articles/the_derivative_of_a_quadratic_form.xhtml). The gradient (in matrix notation) is\n\n$$\\nabla_\\mathcal{L}(\\beta) = -\\frac{1}{n} (y - X \\beta)^T X$$\n\nWe can now implement an analytical version of $\\nabla_\\mathcal{L}(\\beta)$ and compare it to a finite difference approximation. First we simulate and visualize some data:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nn <- 20\nX <- matrix(rnorm(n * 2), ncol = 2)\ny <- rnorm(n)\n\nggplot(NULL, aes(x = X[, 1], y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"One dimension of the simulated data\", x = expression(X[1])) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\nNext we implement our loss and gradient functions. We assume the `loss` function is implemented correctly but want to check the `analytical_grad` implementation.\n\n::: {.cell}\n\n```{.r .cell-code}\nloss <- function(beta) {\n  resid <- y - X %*% beta\n  sum(resid^2) / (2 * n)\n}\n\nanalytical_grad <- function(beta) {\n  grad <- -t(y - X %*% beta) %*% X / n\n  as.vector(grad)\n}\n```\n:::\n\nTo perform this check, we need get approximate the gradient in a direction $d$:\n\n::: {.cell}\n\n```{.r .cell-code}\n#' @param f function that takes a single vector argument x\n#' @param x point at which to evaluate derivative of f (vector)\n#' @param d direction in which to take derivative of f (vector)\n#' @param eps epsilon to use in the gradient approximation\nnumerical_directional_grad <- function(f, x, d, eps = 1e-8) {\n  (f(x + eps * d) - f(x - eps * d)) / (2 * eps)\n}\n```\n:::\n\nAnd then to approximate the entire gradient, we need to combine directional derivatives in each of the unit directions:\n\n::: {.cell}\n\n```{.r .cell-code}\nzeros_like <- function(x) {\n  rep(0, length(x))\n}\n\nnumerical_grad <- function(f, x, eps = 1e-8) {\n  grad <- zeros_like(x)\n  for (dim in seq_along(x)) {\n    unit <- zeros_like(x)\n    unit[dim] <- 1\n    grad[dim] <- numerical_directional_grad(f, x, unit, eps)\n  }\n  grad\n}\n    \nrelative_error <- function(want, got) {\n  (want - got) / want  # assumes want is not zero\n}\n```\n:::\n\nNow we can check the relative error between our analytical implementation of the gradient and the numerical approximation.\n\n::: {.cell}\n\n```{.r .cell-code}\nb <- c(2, 3)  # point in parameter space to check gradient at\n\nnum_grad <- numerical_grad(loss, b)\nana_grad <- analytical_grad(b)\n\nnum_grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.321173 3.902268\n```\n:::\n\n```{.r .cell-code}\nana_grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.321173 3.902268\n```\n:::\n\n```{.r .cell-code}\nrelative_error(num_grad, ana_grad)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  5.152918e-09 -7.975495e-09\n```\n:::\n:::\n\nThe relative error is small, and we can feel confident that our implementation of the gradient is correct.\n\nThis post is based off of Tim Vieira's [fantastic post](http://timvieira.github.io/blog/post/2017/04/21/how-to-test-gradient-implementations/) on how to use numerical gradient checks in practice, but with `R` code. See also the [numDeriv](https://cran.r-project.org/package=numDeriv) package.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}