{
  "hash": "9821d58a5a7cac2ad73535e5553ff684",
  "result": {
    "markdown": "---\ntitle: \"predictive performance via bootstrap variants\"\nsubtitle: |\n  i got curious so i coded these up\ndate: \"2018-05-03\"\nbibliography: predictive-performance-via-bootstrap-variants.bib\nexecute: \n  echo: true\n---\n\nWhen we build a predictive model, we are interested in how the model will perform on data it hasn't seen before. If we have lots of data, we can split it into training and test sets to assess model performance. If we don't have lots of data, it's better to fit a model using all of the available data and to assess its predictive performance using resampling techniques. The bootstrap is one such resampling technique. This post discusses several variants of the bootstrap that are appropriate for estimating predictive performance.\n\n## A brief introduction to the bootstrap\n\nThe bootstrap isn't a particular procedure so much as a general strategy. We assume that our data comes from some underlying population $F$ that we care about. We're interested in the sampling distribution of some statistic $T$ that we calculate from the data. In our case $T$ is predictive performance.\n\nWe don't know $F$, but we can treat the data as an approximation $\\hat F$ of $F$. Here $\\hat F$ is the empirical distribution of the data, which gives equal probability to each observed data point. So we know how to sample from $\\hat F$. The bootstrap says to sample from $\\hat F$ many times and calculate $T$ using these samples. The sampling distribution of $T$ under $\\hat F$ then approximates the sampling distribution of $T$ under $F$.\n\nThe canonical diagram comes from @efron1994. In the diagram the bootstrap world approximates the real world:\n\n![](bootstrap_world.png){fig-alt=\"TODO\"}\n\nIn terms of assessing model performance, this means that we sample our original data $X \\in \\mathbb{R}^{n, p}$ with replacement to generate new datasets $X^*_1, ..., X^*_B \\in \\mathbb{R}^{n, p}$, and then we estimate model performance on each of the bootstrap samples $X^*_b$.\n\n## Data & Model\n\nIn this post, we'll fit a linear regression[^1] and compare:\n\n[^1]: I feel guilty about using linear regression as an example. OLS on the original data is too simple. There's no feature selection, no parameter tuning, etc. I'm on the lookout for a good canonical modelling example to use in posts like this. Hopefully the work on `recipes` and `parsnip` standardizes interfaces enough to make this reasonable sometime soon.\n\n- bootstrap in-sample error\n- bootstrap out-of-sample error\n- the optimism bootstrap\n- the 0.632 bootstrap\n- the 0.632+ bootstrap\n\nFor data, we'll use average college tuition costs in each state. The original data source is [here](https://onlinembapage.com/average-tuition-and-educational-attainment-in-the-united-states/), but I downloaded it from the [Tidy Tuesday](https://github.com/rfordatascience/tidytuesday) repository. \n\nIn particular, we'll see how predictive tuition costs in 2014-15 are for 2015-16 using a simple linear regression of the form[^2]:\n\n[^2]: I'm trying to get into the habit of always writing out the mathematical form of the model I'm fitting. Richard McElreath writes about [why this is important](http://elevanth.org/blog/2017/08/28/first-world-modeling-problems/).\n\n$$y_i = f(x_i) = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\qquad \\varepsilon_i \\sim \\mathrm{N}(0, \\sigma^2)$$\n\nHere $y_i$ is the average tuition in state $i$ for the 2015-16 academic year[^3], and $x_i$ is the average tuition in state $i$ for the 2014-15 academic year. We have $n = 50$ data points. Once we fit a model, we'll refer to the predicted tuition for state $i$ as $f(x_i)$. The data looks like:\n\n[^3]: Typically, regressions using averaged data have inappropriately small confidence intervals. However, the goal here is to demonstrate various bootstrapping methods, rather than inference, so we'll ignore this for now.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nlibrary(here)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nhere() starts at /dabox/hayes/quarto-blog\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────── tidyverse 1.3.1.9000 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.5.9000     ✔ purrr   0.3.4.9000\n✔ tibble  3.1.7.9000     ✔ dplyr   1.0.8.9000\n✔ tidyr   1.2.0.9000     ✔ stringr 1.4.0.9000\n✔ readr   2.1.2.9000     ✔ forcats 0.5.1.9000\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(rsample)\nlibrary(Metrics)\n\nset.seed(27)\n\ndata <- read_xlsx(here(\"blog\", \"2018-05-03_performance-assessments-via-bootstrap-variants\", \"us_avg_tuition.xlsx\")) %>% \n  transmute(state = State,\n            avg_tuition_15_16 = `2015-16`,\n            avg_tuition_14_15 = `2014-15`) %>% \n  mutate_if(is.numeric, round)\n\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 3\n   state       avg_tuition_15_16 avg_tuition_14_15\n   <chr>                   <dbl>             <dbl>\n 1 Alabama                  9751              9496\n 2 Alaska                   6571              6149\n 3 Arizona                 10646             10414\n 4 Arkansas                 7867              7606\n 5 California               9270              9187\n 6 Colorado                 9748              9299\n 7 Connecticut             11397             10664\n 8 Delaware                11676             11515\n 9 Florida                  6360              6345\n10 Georgia                  8447              8063\n# … with 40 more rows\n```\n:::\n:::\n\nWe quickly plot the data with a linear regression overlaid:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data, aes(avg_tuition_14_15, avg_tuition_15_16)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_point(size = 2) +\n  labs(title = \"Average College Tuition\",\n       subtitle = \"Each point represents one state\",\n       y = \"2015-2016 Average Tuition (dollars)\",\n       x = \"2014-2015 Average Tuition (dollars)\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nTo assess model performance, we'll use squared error, which we calculate as:\n\n$$\\mathcal{L(y_i, f(x_i))} = (y_i - f(x_i))^2$$\n\nwhere we treat $y$ as a vector in $\\mathbb{R}^n$. Our first measure of model performance is the *in sample* performance, which is the mean squared error (MSE) on all the training data:\n\n$$\\texttt{in sample error} = {1 \\over n} \\sum_{i = 1}^n \\mathcal{L}(y_i, f(x_i))$$\n\nWe can calculate this in R like so:\n\n::: {.cell}\n\n```{.r .cell-code}\nformula <- avg_tuition_15_16 ~ avg_tuition_14_15\nmodel <- lm(formula, data)\n\n# utility to extract response vector from formula and dataframe\nresponse <- function(formula, data) model.response(model.frame(formula, data))\n\npred <- predict(model, data)\ntrue <- response(formula, data)\n\n# in the code I use root mean squared error (RMSE) as a metric rather than \n# MSE. this keeps the presentation of the math slightly nicer, since I can\n# omit square roots, but keeps the errors on a more interpretable scale.\n\n# I use `Metrics::rmse` for predictions that live in vectors\n# and `yardstick::rmse` for predictions that live in dataframes\nin_sample <- Metrics::rmse(pred, true)\n```\n:::\n\nThe in sample error on the original data is 186 dollars.\n\n### Apparent and out of sample bootstrap error\n\nThere are several ways to calculate the error on our bootstrap samples. One first step is to calculate the *bootstrap in sample error* by estimating the performance of a model fit on each bootstrap sample on the each bootstrap sample $X^*_b$ itself.\n\nTo write this out more formally, let $f_b$ be the model fit to the $b^{th}$ bootstrapped dataset, let $I_b$ be the data points that made it into the $b^{th}$ bootstrap sample and let $n_b$ be the total number of data points in the $b^{th}$ bootstrap sample\n\n$$\\texttt{bootstrap in sample error} = \\\\ {1 \\over B } \\sum_{b = 1}^B {1 \\over n_b} \\sum_{i \\in I_b} \\mathcal{L}(y_i, f_b(x_i))$$\n\nHowever, we also know that for example bootstrap sample, some of the original data didn't get used to fit $f_b$. We can use that data to calculate *bootstrap out of sample error*:\n\n$$\\texttt{bootstrap out of sample error} = \\\\ {1 \\over B } \\sum_{b = 1}^B {1 \\over n - n_b} \\sum_{i \\notin I_b} \\mathcal{L}(y_i, f_b(x_i))$$\n\nWhen someone tells me that they used the bootstrap to evaluate their model, I typically assume that they're reporting the *bootstrap out of sample error*, especially if they're from the machine learning community.\n\nThe *bootstrap in sample error* typically underestimates prediction error, and the *bootstrap out of sample error* typically overestimates prediction error. There are several variants of the bootstrap that try to correct these biases.\n\n### The optimism bootstrap\n\nThe first of these is the optimism bootstrap. First we define the optimism of the $b^{th}$ bootstrap model.\n\n$$\\mathcal{O}_b = {1 \\over n} \\sum_{i = 1}^n \\mathcal{L}(y_i, f_b(x_i)) - {1 \\over n_b} \\sum_{i \\in I_b} \\mathcal{L}(y_i, f_b(x_i))$$\n\nThis is the same as calculating the average error of $f_b$ on the entire original sample, and then subtracting the bootstrap in sample error. To get a better estimate of the overall error we take the average optimism and add it to the *in sample error* estimate:\n\n$$\\texttt{optimism bootstrap error} = \\\\ \\texttt{in sample error} + {1 \\over B} \\sum_{b=1}^B \\mathcal{O}_b$$\n\n### The 0.632 and 0.632+ bootstrap\n\nInterestingly, the *bootstrap out of sample error* is somewhat pessimistic (@efron1994, @efron1997). The *0.632 bootstrap* estimator tries to address this problem by combining the `in sample` performance estimate with the *bootstrap out of sample* performance estimate:\n\n$$\\texttt{0.632 bootstrap error} = \\\\ \\quad \\\\ 0.632 \\cdot \\texttt{bootstrap out of sample error} + \\\\ 0.368 \\cdot \\texttt{in sample error}$$\n\nWhere does the 0.632 come from? On average a bootstrap sample contains 63.2\\% of the data points in the original dataset. [This Cross Validated answer](https://stats.stackexchange.com/questions/96739/what-is-the-632-rule-in-bootstrapping) has some nice discussion.\n\nSimilarly, the *0.632+ bootstrap estimator* tries to find a good balance between the in sample error and the bootstrap out of sample error. To do this, it considers the no information error rate:\n\n$$\\texttt{no info error rate} = {1 \\over n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\mathcal{L}(y_i, f(x_j))$$\n\nwhich is the expected error rate when data points and responses are randomly assigned. Averaging over all $i, j$ you get a measure of how well you can predict when you know pretty much nothing. Then you can estimate the relative overfitting of a model with:\n\n$$\\texttt{overfit} = {\\texttt{bootstrap out of sample error} - \\texttt{in sample error} \\over \\texttt{no info error rate} - \\texttt{in sample error}}$$\n\nThe 0.632+ uses this to weight the bootstrap out of sample error and the in sample error according to \n\n$$\\texttt{w} = \\texttt{weight on out of sample bootstrap error} \\\\ = {0.632 \\over 1 - 0.368 \\cdot \\texttt{overfit}}$$\n\nand the final 0.632+ estimator is\n\n$$\\texttt{0.632+ bootstrap error} = \\\\ \\quad \\\\ \\texttt{w} \\cdot \\texttt{out of sample bootstrap error} + \\\\ (1 - \\texttt{w}) \\cdot \\texttt{in sample error}$$\n\nThe idea is that the 0.632 estimator can be optimistic, and to take overfitting into account to correct this. If the relative overfitting is zero, the 0.632+ estimator reduces to the 0.632 estimator.\n\n## Actually calculating these things in R\n\nBefore anything else, we need bootstrap samples. Luckily, the `rsample` package makes this super convenient. \n\n::: {.cell}\n\n```{.r .cell-code}\n# create an `rset` object: a data frame with a list-column full\n# of bootstrap resamples\nboots <- bootstraps(data, times = 25)\nboots\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Bootstrap sampling \n# A tibble: 25 × 2\n   splits          id         \n   <list>          <chr>      \n 1 <split [50/19]> Bootstrap01\n 2 <split [50/20]> Bootstrap02\n 3 <split [50/17]> Bootstrap03\n 4 <split [50/18]> Bootstrap04\n 5 <split [50/14]> Bootstrap05\n 6 <split [50/13]> Bootstrap06\n 7 <split [50/15]> Bootstrap07\n 8 <split [50/20]> Bootstrap08\n 9 <split [50/17]> Bootstrap09\n10 <split [50/13]> Bootstrap10\n# … with 15 more rows\n```\n:::\n:::\n\nThe individual bootstrap samples are contained in the `rsplit` objects:\n\n::: {.cell}\n\n```{.r .cell-code}\none_boot <- boots$splits[[1]]\none_boot\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Analysis/Assess/Total>\n<50/19/50>\n```\n:::\n:::\n\nThis print method is to help track which data went into the bootstrap sample. The format here is `<# data points in resampled data set / # original data points not in the resampled data set / # original data points>`.\n\nIf we want to see the bootstrap sample itself, we can:\n\n::: {.cell}\n\n```{.r .cell-code}\nanalysis(one_boot)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 3\n   state          avg_tuition_15_16 avg_tuition_14_15\n   <chr>                      <dbl>             <dbl>\n 1 California                  9270              9187\n 2 Wyoming                     4891              4654\n 3 Florida                     6360              6345\n 4 South Carolina             11816             11470\n 5 Maine                       9573              9560\n 6 Kansas                      8530              8270\n 7 North Carolina              6973              6685\n 8 Missouri                    8564              8409\n 9 North Carolina              6973              6685\n10 Alabama                     9751              9496\n# … with 40 more rows\n```\n:::\n:::\n\nIf we want to see all the data points that *didn't* go into the bootstrap sample, we can use:\n\n::: {.cell}\n\n```{.r .cell-code}\nassessment(one_boot)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 19 × 3\n   state         avg_tuition_15_16 avg_tuition_14_15\n   <chr>                     <dbl>             <dbl>\n 1 Alaska                     6571              6149\n 2 Colorado                   9748              9299\n 3 Connecticut               11397             10664\n 4 Delaware                  11676             11515\n 5 Georgia                    8447              8063\n 6 Massachusetts             11588             10987\n 7 Michigan                  11991             11618\n 8 Nebraska                   7608              7348\n 9 New Jersey                13303             13027\n10 New York                   7644              7306\n11 Ohio                      10196             10104\n12 Oregon                     9371              8949\n13 Pennsylvania              13395             13157\n14 Rhode Island              11390             10977\n15 Tennessee                  9263              8941\n16 Vermont                   14993             14501\n17 Virginia                  11819             11202\n18 Washington                10288             10703\n19 West Virginia              7171              6698\n```\n:::\n:::\n\nNow we want to fit models on each of the bootstrap samples and assess various performance metrics. We write some helper functions to smooth things along.\n\n::: {.cell}\n\n```{.r .cell-code}\n# evaluate model performance on a particular dataset\nperformance <- function(model, data, metric = Metrics::rmse) {\n  \n  # aside: it would be nice to be able to extract a data/design object from\n  # the model, and then extract the response from that. someday.\n  \n  true <- response(formula, data)\n  pred <- predict(model, data)\n  metric(true, pred)\n}\n\n# get the no info error rate. should be used on the model\n# fit to full original data\nno_info_error_rate <- function(model, formula, data, metric = Metrics::rmse) {\n  \n  true <- response(formula, data)\n  pred <- predict(model, data)\n  \n  crossed <- crossing(true, pred)\n  with(crossed, metric(true, pred))\n}\n\n# NOTE: `bs` in variable names is an abbreviation for `bootstrap`\n\n# 0.632 bootstrap estimate of performance\nbs_632 <- function(in_sample, bs_out_of_sample) {\n  0.368 * in_sample + 0.632 * bs_out_of_sample\n}\n\n# 0.632+ bootstrap estimate of performance\nbs_632p <- function(in_sample, bs_out_of_sample, no_info_error_rate) {\n  relative_overfit <- (bs_out_of_sample - in_sample) /\n    (no_info_error_rate - in_sample)\n  w <- 0.632 / (1 - 0.368 * relative_overfit)\n  w * bs_out_of_sample + (1 - w) * in_sample\n}\n```\n:::\n\nNow we can fit models on each bootstrapped dataset:\n\n::: {.cell}\n\n```{.r .cell-code}\nno_info_perf <- no_info_error_rate(model, formula, data)\n\nboot_performance <- boots %>% \n  mutate(\n    # fit a model on *bootstrap sample* of data\n    model = map(splits, ~lm(formula, analysis(.x))),\n    \n    # bootstrap in sample error\n    bs_is_perf = map2_dbl(model, splits, ~performance(.x, analysis(.y))),\n    \n    # bootstrap out of sample error\n    bs_os_perf = map2_dbl(model, splits, ~performance(.x, assessment(.y))),\n    \n    # bootstrap error on full data\n    data_perf = map_dbl(model, ~performance(.x, data)),\n    \n    # optimism\n    optimism = bs_is_perf - data_perf,\n    \n    # optimism corrected error estimate\n    bs_optimism = in_sample - optimism,\n    \n    # 0.632 bootstrap error estimate\n    bs_632_perf = bs_632(bs_is_perf, bs_os_perf),\n    \n    # 0.632+ bootstrap error estimate\n    bs_632p_perf = bs_632p(bs_is_perf, bs_os_perf, no_info_perf))\n```\n:::\n\nWe can calculate the point estimates we discussed above:\n\n::: {.cell}\n\n```{.r .cell-code}\nclean_performance <- boot_performance %>% \n  select_if(is.numeric) %>% \n  select(-optimism, -data_perf) %>% \n  gather(model, statistic) %>% \n  transmute(measure = recode(\n    model,\n    \"bs_is_perf\" = \"bootstrap in sample\",\n    \"bs_os_perf\" = \"bootstrap out of sample\" ,\n    \"bs_optimism\" = \"optimism bootstrap\",\n    \"bs_632_perf\" = \"0.632 bootstrap\",\n    \"bs_632p_perf\" = \"0.632+ bootstrap\"),\n    error = statistic\n  )\n\nclean_performance %>% \n  group_by(measure) %>% \n  summarize_if(is.numeric, mean) %>% \n  mutate_if(is.numeric, round) %>% \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|measure                 | error|\n|:-----------------------|-----:|\n|0.632 bootstrap         |   188|\n|0.632+ bootstrap        |   188|\n|bootstrap in sample     |   180|\n|bootstrap out of sample |   192|\n|optimism bootstrap      |   195|\n:::\n:::\n\nHowever, point estimates aren't good ways to compare the performance of two models because they don't give us a sense of how much model performance might vary on different data sets. Visualizing the sampling distribution of each of the metrics gives us a more complete view of model performance:\n\n::: {.cell}\n\n```{.r .cell-code}\nclean_performance %>% \n  ggplot(aes(measure, error, color = measure)) +\n  geom_boxplot() +\n  geom_jitter() +\n  scale_color_brewer(type = \"qual\") +\n  coord_flip() +\n  labs(title = \"Sampling distributions of bootstrap performance metrics\",\n       y = \"rmse error (dollars)\") +\n  theme(axis.title.y = element_blank(),\n        legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\nFor this particular model, we see that the 0.632 and 0.632+ estimators are pretty much the same. This makes sense because a linear model fits the data quick well and so there should be little overfitting. We can also see that the bootstrap in sample error rate has the lowest median.\n\nGiven how it can be difficult to keep track of which metric should be calculated on which dataset, I should probably write some tests to confirm that my code does what I want it to. Since this is a blog post, I'm going to pass on that for the moment.\n\n## Using the bootstrap in practice\n\nWhen you have fewer than 20,000 data points, it's reasonable to use the optimism bootstrap with $B = 200$ to $B = 400$. When you have more data, cross-validation[^4] or simply splitting the data becomes more reliable. Any steps taken to develop a model should be performed on each bootstrap dataset. For example, if you use LASSO for feature selection and then fit a model, the feature selection step needs to be included in the bootstrap procedure.\n\n[^4]: Cross validation is much less stable than the bootstrap on data sets with less than 20,000 data points (@steyerberg2001). Frank Harrell has run a number of simulations showing that you need at least 100 repetitions of 10-fold cross-validation to get accurate error estimates at this data size (@harrell2015). Here's [one such simulation](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/RmS/logistic.val.pdf).\n\nIn some studies (@steyerberg2001) the 0.632 and 0.632+ estimators have similar performance. Asymptotically, the 0.632 estimator is equivalent to the optimism bootstrap (@efron1994)[^5]. It isn't clear if one is preferrable over the other in finite samples, so I'd just stick with the estimator your audience is most likely to already be familiar with. You should be fine so long as you avoid using the *bootstrap in sample error* estimate.\n\n[^5]: The relevant chapter can be [found online](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.473.2742&rep=rep1&type=pdf). You may also enjoy the draft of Frank Harrell's [new book](http://www.fharrell.com/doc/bbr.pdf).\n\nIf you are interested in comparing several models, it probably isn't sufficient to compare the sampling distributions of these error measures. If you fit multiple models on the same bootstrap datasets, the bootstrap samples will have an impact on model performance. Hierarchicals models are a good way to model this. Once you have your bootstrapped error estimates, Max Kuhn's [tidyposterior](https://topepo.github.io/tidyposterior/articles/Getting_Started.html) package can help you take the next step when comparing models.\n\nIf you're interested, stick around for future posts, where I'll cover the process of building predictive models and how to select from several predictive models.\n\nFeedback is always welcome!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}