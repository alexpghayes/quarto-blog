<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Hayes">
<meta name="dcterms.date" content="2019-08-31">

<title>consistency and the linear probability model – alex hayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/all.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/latex-fontsize.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/academicons-1.9.4/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/academicons-1.9.4/size.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-G6PQFW2XSK"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-G6PQFW2XSK', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="consistency and the linear probability model – alex hayes">
<meta property="og:description" content="an explainer about ordinary least squares regression and when it is an acceptable estimator">
<meta property="og:image" content="https://www.alexpghayes.com/post/2019-08-31_consistency-and-the-linear-probability-model/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==">
<meta property="og:site_name" content="alex hayes">
<meta name="twitter:title" content="consistency and the linear probability model – alex hayes">
<meta name="twitter:description" content="an explainer about ordinary least squares regression and when it is an acceptable estimator">
<meta name="twitter:image" content="https://www.alexpghayes.com/post/2019-08-31_consistency-and-the-linear-probability-model/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==">
<meta name="twitter:creator" content="@alexpghayes">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">alex hayes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">about</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../code/index.html"> 
<span class="menu-text">code</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching/index.html"> 
<span class="menu-text">teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../talks/index.html"> 
<span class="menu-text">talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://bsky.app/profile/alexpghayes.com"> 
<span class="menu-text"><i class="fa-brands fa-bluesky" aria-label="bluesky"></i></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://scholar.google.com/citations?user=6g1T3WIAAAAJ"> 
<span class="menu-text"><i class="ai  ai-google-scholar ai-xl" aria-label="google-scholar"></i></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/alexpghayes"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/alexpghayes/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">consistency and the linear probability model</h1>
            <p class="subtitle lead"></p><p>an explainer about ordinary least squares regression and when it is an acceptable estimator</p><p></p>
                                <div class="quarto-categories">
                <div class="quarto-category">m estimation</div>
                <div class="quarto-category">math stat</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://www.alexpghayes.com">Alex Hayes</a> <a href="https://orcid.org/0000-0002-4985-5160" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 31, 2019</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#summary" id="toc-summary" class="nav-link active" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#what-is-the-linear-probability-model" id="toc-what-is-the-linear-probability-model" class="nav-link" data-scroll-target="#what-is-the-linear-probability-model">What is the linear probability model?</a></li>
  <li><a href="#the-ols-estimator" id="toc-the-ols-estimator" class="nav-link" data-scroll-target="#the-ols-estimator">The OLS estimator</a></li>
  <li><a href="#a-bare-minimum-for-inference" id="toc-a-bare-minimum-for-inference" class="nav-link" data-scroll-target="#a-bare-minimum-for-inference">A bare minimum for inference</a></li>
  <li><a href="#consistency-of-the-ols-estimator" id="toc-consistency-of-the-ols-estimator" class="nav-link" data-scroll-target="#consistency-of-the-ols-estimator">Consistency of the OLS estimator</a></li>
  <li><a href="#what-if-logistic-regression-is-the-true-model" id="toc-what-if-logistic-regression-is-the-true-model" class="nav-link" data-scroll-target="#what-if-logistic-regression-is-the-true-model">What if logistic regression is the true model?</a></li>
  <li><a href="#where-the-estimator-comes-from-doesnt-matter" id="toc-where-the-estimator-comes-from-doesnt-matter" class="nav-link" data-scroll-target="#where-the-estimator-comes-from-doesnt-matter">Where the estimator comes from doesn’t matter</a></li>
  <li><a href="#what-about-uncertainty-in-our-estimator" id="toc-what-about-uncertainty-in-our-estimator" class="nav-link" data-scroll-target="#what-about-uncertainty-in-our-estimator">What about uncertainty in our estimator?</a></li>
  <li><a href="#takeaways" id="toc-takeaways" class="nav-link" data-scroll-target="#takeaways">Takeaways</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/alexpghayes/quarto-blog/edit/main/post/2019-08-31_consistency-and-the-linear-probability-model/index.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/alexpghayes/quarto-blog/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>A while back Twitter once again lost its collective mind and decided to rehash the logistic regression versus linear probability model debate for the umpteenth time. The genesis for this new round of chaos was <span class="citation" data-cites="gomila_logistic_2019">Gomila (<a href="#ref-gomila_logistic_2019" role="doc-biblioref">2019</a>)</span>, a pre-print by Robin Gomila, a grad student in psychology at Princeton.</p>
<p>So I decided to learn some more about the linear probability model (LPM), which has been on my todo list since Michael Weylandt suggested it as an example for a paper I’m working on. In this post I’ll introduce the LPM, and then spend a bunch of time discussing when OLS is a consistent estimator.</p>
<p><strong>Update</strong>: I turned this blog post into <a href="https://github.com/alexpghayes/linear-probability-model">somewhat flippant, meme-heavy slides</a>.</p>
</section>
<section id="what-is-the-linear-probability-model" class="level2">
<h2 class="anchored" data-anchor-id="what-is-the-linear-probability-model">What is the linear probability model?</h2>
<p>Suppose we have outcomes <span class="math inline">\(Y \in \{0, 1\}\)</span> and fixed covariate vectors <span class="math inline">\(X\)</span>. The linear probability model is a <em>model</em>, that is, a set of probability distributions that might have produced our observed data. In particular, the linear probability assumes that the data generating process looks like:</p>
<p><span class="math display">\[\begin{align}
P(Y = 1 | X) =
  \begin{cases}
  1 &amp; \beta_0 + \beta_1 X_1 + ... + \beta_k X_k &gt; 1 \\
  \beta_0 + \beta_1 X_1 + ... + \beta_k X_k &amp; \beta_0 + \beta_1 X_1 + ... + \beta_k X_k \in [0, 1] \\
  0 &amp; \beta_0 + \beta_1 X_1 + ... + \beta_k X_k &lt; 0
  \end{cases}
\end{align}\]</span></p>
<p>Essentially we clip <span class="math inline">\(P(Y = 1 | X) = X \beta\)</span> to <span class="math inline">\([0, 1]\)</span> to make sure we get valid probabilities. We can contrast the LPM with the binomial GLM, where the assumption is that:</p>
<p><span class="math display">\[\begin{align}
P(Y = 1 | X) = \frac{1}{1 + \exp(-(\beta_0 + \beta_1 X_1 + ... + \beta_k X_k))}
\end{align}\]</span></p>
<p>At points throughout this post we will also be interested in Gaussian GLMs. In slightly different notation, we can write all of these models as follows:</p>
<p><span class="math display">\[\begin{align}
Y_i | X_i &amp;\sim \mathrm{Bernoulli}(\min(1, \max(0, X_i \beta)))
  &amp; \text{linear probability model} \\
Y_i | X_i &amp;\sim \mathrm{Bernoulli}(\mathrm{logit}^{-1} (X_i \beta))
  &amp; \text{logistic regression / binomial GLM} \\
Y_i | X_i &amp;\sim \mathrm{Normal}(X_i \beta, \sigma^2)
  &amp; \text{linear regression / Gaussian GLM}
\end{align}\]</span></p>
</section>
<section id="the-ols-estimator" class="level2">
<h2 class="anchored" data-anchor-id="the-ols-estimator">The OLS estimator</h2>
<p>When people refer to the linear probability model, they are referring to using the Ordinary Least Squares estimator as an estimator for <span class="math inline">\(\beta\)</span>, or using <span class="math inline">\(X \hat \beta_\text{OLS}\)</span> as an estimator for <span class="math inline">\(\mathbb{E}(Y|X) = P(Y = 1|X)\)</span>. The OLS estimator is:</p>
<p><span class="math display">\[\begin{align}
\hat \beta_\text{OLS} = (X'X)^{-1} X' Y.
\end{align}\]</span></p>
<p>Most people have seen the OLS estimator derived as the MLE of a <em>Gaussian</em> linear model. Here we have binary data, which is definitely non-Gaussian, and this is aesthetically somewhat unappealing. The big question is whether or not using <span class="math inline">\(\hat \beta_\text{OLS}\)</span> on binary data actually works.</p>
</section>
<section id="a-bare-minimum-for-inference" class="level2">
<h2 class="anchored" data-anchor-id="a-bare-minimum-for-inference">A bare minimum for inference</h2>
<p>When we do inference, we pretty much always want two things to happen. Given our model (the assumptions we are willing to make about the data generating process):</p>
<ol type="1">
<li>our estimand should be identified, and</li>
<li>our estimator should be consistent.</li>
</ol>
<p>Identification means that each possible value of the estimand should correspond to a distinct distribution in our probability model. When we say that we want a consistent estimator, we mean that our estimator should recover the estimand exactly with infinite data. All the estimands in the LPM debate are identified (to my knowledge), so this isn’t the big deal here. But consistency matters a lot!</p>
<p><span class="citation" data-cites="gomila_logistic_2019">Gomila (<a href="#ref-gomila_logistic_2019" role="doc-biblioref">2019</a>)</span> claims that <span class="math inline">\(\hat \beta_\text{OLS}\)</span> is unbiased and consistent for <span class="math inline">\(\beta\)</span>, and attempts to demonstrate this two ways: (1) analytically, and (2) via simulation.</p>
<p>And this is the point that I got pretty confused, because the big question is: consistent under what model? Depending on who you ask, we could conceivably be assuming that the data comes from:</p>
<ul>
<li>a Gaussian GLM (linear regression),</li>
<li>the linear probability model, or</li>
<li>a binomial GLM (logistic regression).</li>
</ul>
</section>
<section id="consistency-of-the-ols-estimator" class="level2">
<h2 class="anchored" data-anchor-id="consistency-of-the-ols-estimator">Consistency of the OLS estimator</h2>
<p>The easiest case is when we assume that a Gaussian GLM (linear regression model) holds. In this case, <span class="math inline">\(\hat \beta_\text{OLS}\)</span> is unbiased and consistent. My preferred reference for this is <span class="citation" data-cites="rencher_linear_2008">Rencher and Schaalje (<a href="#ref-rencher_linear_2008" role="doc-biblioref">2008</a>)</span>.</p>
<p>When the linear probability model holds, <span class="math inline">\(\hat \beta_\text{OLS}\)</span> is in general biased and inconsistent (<span class="citation" data-cites="horrace_new_2003">Horrace and Oaxaca (<a href="#ref-horrace_new_2003" role="doc-biblioref">2003</a>)</span>). However, <span class="citation" data-cites="gomila_logistic_2019">Gomila (<a href="#ref-gomila_logistic_2019" role="doc-biblioref">2019</a>)</span> claims that OLS is unbiased and consistent. Over Twitter DM I clarified that this claim is with respect to the linear probability model. Gomila referred me to <span class="citation" data-cites="wooldridge_econometric_2001">Wooldridge (<a href="#ref-wooldridge_econometric_2001" role="doc-biblioref">2001</a>)</span> for an analytic proof, and to his simulation results for empirical confirmation.</p>
<p>At this point the main result of <span class="citation" data-cites="horrace_new_2003">Horrace and Oaxaca (<a href="#ref-horrace_new_2003" role="doc-biblioref">2003</a>)</span> becomes germane. It goes like this: the OLS estimator is consistent and unbiased under the linear probability model if <span class="math inline">\(X_i^T \beta \in [0, 1]\)</span> for <em>all</em> <span class="math inline">\(i\)</span>, otherwise the OLS estimator is biased and inconsistent. In fact, <span class="citation" data-cites="wooldridge_econometric_2001">Wooldridge (<a href="#ref-wooldridge_econometric_2001" role="doc-biblioref">2001</a>)</span> makes the same point:</p>
<blockquote class="blockquote">
<p>Unless the range of <span class="math inline">\(X\)</span> is severely restricted, the linear probability model cannot be a good description of the population response probability <span class="math inline">\(P(Y = 1|X)\)</span>.</p>
</blockquote>
<p><a href="https://davegiles.blogspot.com/2012/06/another-gripe-about-linear-probability.html">Here are some simulations</a> demonstrating this bias and inconsistency when the <span class="math inline">\(X_i^T \beta \in [0, 1]\)</span> condition is violated. It’s pretty clear that OLS is in general biased and inconsistent under the linear probability model. I was curious why this wasn’t showing up in Gomila’s simulations, so I took a look at his code and it turned out he wasn’t simulating from diverse enough data generating processes.</p>
<p>We discussed this over Twitter DM, and Gomila has since <a href="https://osf.io/dxzun/">updated the code</a>, but I believe the new simulations still do not seriously violate the <span class="math inline">\(X_i^T \beta \in [0, 1]\)</span> condition. I briefly played around with the code but then it was 2 AM and I didn’t understand <a href="https://declaredesign.org/"><code>DeclareDesign</code></a> particularly well so I gave up. Many kudos to Gomila for posting his code for public review.</p>
<p>Anyway, the gist is that OLS is consistent under the linear probability model if <span class="math inline">\(X_i^T \beta\)</span> is between zero and one for all <span class="math inline">\(i\)</span>.</p>
</section>
<section id="what-if-logistic-regression-is-the-true-model" class="level2">
<h2 class="anchored" data-anchor-id="what-if-logistic-regression-is-the-true-model">What if logistic regression is the true model?</h2>
<p>Another reasonable question is: what happens to <span class="math inline">\(\hat \beta_\text{OLS}\)</span> under a binomial GLM? The story here is pretty much the same: <span class="math inline">\(\hat \beta_\text{OLS}\)</span> is not consistent for <span class="math inline">\(\beta\)</span>, but it <a href="https://www.brodrigues.co/blog/2019-08-14-lpm/">often does a decent job</a> of estimating <span class="math inline">\(\mathbb{E}(Y|X)\)</span> anyway <span class="citation" data-cites="battey_linear_2019 cox_regression_1958">(<a href="#ref-battey_linear_2019" role="doc-biblioref">Battey, Cox, and Jackson 2019</a>; <a href="#ref-cox_regression_1958" role="doc-biblioref">Cox 1958</a>)</span>.</p>
<p>The intuition behind this comes from M-estimation<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, which we digress into momentarily. The idea is to observe that <span class="math inline">\(\hat \beta_\text{OLS}\)</span> is an M-estimator. To see this, recall that OLS is based on minimizing</p>
<p><span class="math display">\[\begin{align*}
  \mathcal{L}(X, Y, \beta) = \frac 12 \Vert Y - X \beta \Vert_2^2
\end{align*}\]</span></p>
<p>which has a global minimizer <span class="math inline">\(\beta_0\)</span>. The gradient,</p>
<p><span class="math display">\[\begin{align*}
  \nabla \mathcal{L}(X, Y, \beta) = \left( Y - X \beta \right)' X,
\end{align*}\]</span></p>
<p>should be zero at <span class="math inline">\(\beta_0\)</span> under any model with linear expectation (provided that <span class="math inline">\(X_i \neq 0\)</span> for all <span class="math inline">\(i\)</span>). So we take <span class="math inline">\(\psi = \nabla \mathcal{L}(X, Y, \beta_0)\)</span> as the function in our estimating equation, since <span class="math inline">\(\mathbb{E} (\psi) = 0\)</span>. This lets us leverage standard results from M-estimation theory.</p>
<p>In particular, M-estimation theory tells us that <span class="math inline">\(\hat \beta_\text{OLS}\)</span> is consistent under any regular distribution <span class="math inline">\(F\)</span> such that <span class="math inline">\(\beta_0\)</span> is the unique solution to</p>
<p><span class="math display">\[\begin{align*}
  \mathbb{E}_F \left[ \left( Y_i - X_i^T \beta_0 \right)' X_i \right] = 0.
\end{align*}\]</span></p>
<p>We can use this to derive a sufficient condition for consistency; namely OLS is consistent for <span class="math inline">\(\beta_0\)</span> if</p>
<p><span class="math display">\[\begin{align*}
  0
  &amp;= \mathbb{E}_F \left[ \left( Y - X_i^T \beta_0 \right)' X_i \right] \\
  &amp;= \mathbb{E}_F \left[ \mathbb{E}_F \left[ \left( Y - X_i^T \beta_0 \right)' X_i \Big \vert X_i \right] \right] \\
  &amp;= \mathbb{E}_F \left[ \mathbb{E}_F \left[ Y - X_i^T \beta_0 \Big \vert X_i \right]' X_i \right].
\end{align*}\]</span></p>
<p>So a sufficient condition for the consistency of OLS is that</p>
<p><span class="math display">\[\begin{align*}
  \mathbb{E}_F(Y | X_i) = X_i^T \beta_0.
\end{align*}\]</span></p>
<p>That is, if the expectation is linear in some model parameter <span class="math inline">\(\beta_0\)</span>, OLS is consistent for that parameter. If you are an economist you’ve probably seen this written as</p>
<p><span class="math display">\[\begin{align*}
  \mathbb{E}_F(\varepsilon_i | X_i) = 0,
\end{align*}\]</span></p>
<p>where we take <span class="math inline">\(\varepsilon_i = Y_i - \mathbb{E}(Y_i | X_i)\)</span>.</p>
<p>The sufficient condition is also a necessary condition, and if it is violated <span class="math inline">\(\hat \beta_\text{OLS}\)</span> will generally be asymptotically biased.</p>
<p>Returning from our digression into M-estimation, we note the condition that <span class="math inline">\(\mathbb{E}_F(Y | X_i) = X_i^T \beta_0\)</span> is shockingly weaker than the assumption we used to derive <span class="math inline">\(\hat \beta_\text{OLS}\)</span>, where we used the fact that <span class="math inline">\(Y\)</span> followed a Gaussian distribution. For consistency, we only need an assumption on the first moment of the distribution of <span class="math inline">\(Y\)</span>, rather than the full parametric form of the model, which specifies <em>every</em> moment <span class="math inline">\(\mathbb{E}(Y^2), \mathbb{E}(Y^3), \mathbb{E}(Y^4), ...\)</span> of <span class="math inline">\(Y\)</span>.</p>
<p>Anyway, the expectation of a binomial GLM is <span class="math inline">\(\mathrm{logit}^{-1} (X_i \beta)\)</span>, and the expectation of the LPM is <span class="math inline">\(\min(1, \max(0, X_i \beta))\)</span>. For certain <span class="math inline">\(X_i\)</span> and <span class="math inline">\(\beta\)</span>, these expectations are very close to <span class="math inline">\(X_i \beta\)</span>. In these cases, estimates for <span class="math inline">\(\mathbb{E}(Y|X)\)</span> based on <span class="math inline">\(\hat \beta_\text{OLS}\)</span> will do well<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>Looking at the expectations of the models, we can see they aren’t wildly different:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Note that, under the LPM, <span class="math inline">\(\hat \beta_\text{OLS}\)</span> can give us good estimates for <span class="math inline">\(\beta\)</span>, but under logistic regression, we only get good estimates of <span class="math inline">\(\mathbb{E}(Y|X)\)</span><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>So maybe don’t toss the LPM estimator out with the bath water. Sure, the thing is generally inconsistent and aesthetically offensive, but whatever, it works on occasion, and sometimes there will be other practical considerations that make this tradeoff reasonable.</p>
</section>
<section id="where-the-estimator-comes-from-doesnt-matter" class="level2">
<h2 class="anchored" data-anchor-id="where-the-estimator-comes-from-doesnt-matter">Where the estimator comes from doesn’t matter</h2>
<p>Bayes Twitter in particular had a number of fairly vocal LPM opponents, on the basis that <span class="math inline">\(\hat \beta_\text{OLS}\)</span> was derived under a model that can’t produce the observed data.</p>
<p>This might seem like a dealbreaker, but it doesn’t bother me. Where the estimator comes from doesn’t actually matter. If it has nice properties given the assumptions you are willing to make, you should use it! Estimators derived under unrealistic models<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> often turn out to be good!</p>
<p>In a bit more detail, here’s how I think of model checking:</p>
<ol type="1">
<li>There’s an estimand I want to know</li>
<li>I make some assumptions about my data generating process</li>
<li>I pick an estimator that has nice properties given this data generating process</li>
</ol>
<p>The issue here is that my assumptions about the data generating process can be wrong. And if my modeling assumptions are wrong, then my estimator might not have the nice properties I want it to have, and this is bad.</p>
<p>The concern is that using <span class="math inline">\(\hat \beta_\text{OLS}\)</span> corresponds to making a bad modeling assumption. In particular, using a Gaussian model for binary data isn’t defensible.</p>
<p>That’s not really what’s going on though. Instead, we start by <em>deriving</em> an estimator under the linear regression model. Then, we show this estimator has nice properties <em>under a new, different model</em>. To do model checking, we need to test whether the new, different model holds. Whether or not the data comes from a linear regression is immaterial.</p>
<p>Nobody who likes using <span class="math inline">\(\hat \beta_\text{OLS}\)</span> is arguing that it’s reasonable to assume a Gaussian generative model for binary data. LPM proponents argue that the Gaussian modeling assumption is violated, but it isn’t violated in an important way. Look, they say, the key assumptions of a Gaussian model that we used to derive consistency results can still hold, even if some other assumptions get violated. This is exactly what the M-estimation approach formalizes.</p>
</section>
<section id="what-about-uncertainty-in-our-estimator" class="level2">
<h2 class="anchored" data-anchor-id="what-about-uncertainty-in-our-estimator">What about uncertainty in our estimator?</h2>
<p>So far we have established that <span class="math inline">\(\hat \beta_\text{OLS}\)</span> is at least occasionally a consistent estimator for <span class="math inline">\(P(Y=1|X)\)</span> under the LPM and logistic regression.</p>
<p>In practice, we also care about the uncertainty in <span class="math inline">\(\hat \beta\)</span>. We might want a confidence interval, or, God forbid, a p-value. So we should think about consistent estimators for <span class="math inline">\(\mathbf{Cov} (\hat \beta)\)</span>. In the grand LPM debate, most people suggest using robust standard errors. For a while, it was unclear to me how these robust standard errors were derived and under what conditions they are consistent. The answer again comes from <span class="citation" data-cites="boos_essential_2013">Boos and Stefanski (<a href="#ref-boos_essential_2013" role="doc-biblioref">2013</a>)</span>, and all we need for the consistency of robust standard errors for <span class="math inline">\(\mathbf{Cov} (\hat \beta)\)</span> is some moment conditions (the existence of (7.5) and (7.6) in the text), which linear regression, logistic regression, and the LPM all satisfy.</p>
<p>It only took several weeks of misreading <span class="citation" data-cites="boos_essential_2013">Boos and Stefanski (<a href="#ref-boos_essential_2013" role="doc-biblioref">2013</a>)</span> and asking dumb questions on Twitter to figure this out. Thanks to Achim Zeileis, James E. Pustejovsky, Cyrus Samii for answering those questions. Note that <span class="citation" data-cites="white_heteroskedasticity-consistent_1980">White (<a href="#ref-white_heteroskedasticity-consistent_1980" role="doc-biblioref">1980</a>)</span> is another canonical paper on robust standard errors, but it doesn’t click for me like the M-estimation framework does.</p>
</section>
<section id="takeaways" class="level2">
<h2 class="anchored" data-anchor-id="takeaways">Takeaways</h2>
<p>This post exists because I wasn’t sure when <span class="math inline">\(\hat \beta_\text{OLS}\)</span> was a consistent estimator, and the rest of the LPM debate seemed like a lot of noise until I could figure that out. I have three takeaways.</p>
<ol type="1">
<li><p>Properties of estimators are always with respect to models, and it’s hard to discuss which estimator is best if you don’t clarify modeling assumptions.</p></li>
<li><p>If there are multiple reasonable estimators, fit them all. If they result in substantively different conclusions: congratulations! Your data is trash and you can move on to a new project!</p></li>
<li><p>If you really care about getting efficient, consistent estimates under weak assumptions, you should be doing TMLE or Double ML or burning your CPU to a crisp with BART<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p></li>
</ol>
<p>Anyway, this concludes the “I taught myself about the linear probability model and hope to never mention it again” period of my life. I look forward to my mentions being a trash fire.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-battey_linear_2019" class="csl-entry" role="listitem">
Battey, H. S., D. R. Cox, and M. V. Jackson. 2019. <span>“On the Linear in Probability Model for Binary Data.”</span> <em>Royal Society Open Science</em> 6 (5): 190067. <a href="https://doi.org/10.1098/rsos.190067">https://doi.org/10.1098/rsos.190067</a>.
</div>
<div id="ref-boos_essential_2013" class="csl-entry" role="listitem">
Boos, Dennis D, and L. A Stefanski. 2013. <em>Essential <span>Statistical</span> <span>Inference</span></em>. Vol. 120. Springer <span>Texts</span> in <span>Statistics</span>. New York, NY: Springer New York. <a href="https://doi.org/10.1007/978-1-4614-4818-1">https://doi.org/10.1007/978-1-4614-4818-1</a>.
</div>
<div id="ref-cox_regression_1958" class="csl-entry" role="listitem">
Cox, D. R. 1958. <span>“The <span>Regression</span> <span>Analysis</span> of <span>Binary</span> <span>Sequences</span>.”</span> <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 20 (2): 215–42. <a href="http://www.jstor.org/stable/2983890">http://www.jstor.org/stable/2983890</a>.
</div>
<div id="ref-gomila_logistic_2019" class="csl-entry" role="listitem">
Gomila, Robin. 2019. <span>“Logistic or <span>Linear</span>? <span>Estimating</span> <span>Causal</span> <span>Effects</span> of <span>Binary</span> <span>Outcomes</span> <span>Using</span> <span>Regression</span> <span>Analysis</span>.”</span> Preprint. PsyArXiv. <a href="https://doi.org/10.31234/osf.io/4gmbv">https://doi.org/10.31234/osf.io/4gmbv</a>.
</div>
<div id="ref-horrace_new_2003" class="csl-entry" role="listitem">
Horrace, William C, and Ronald L Oaxaca. 2003. <span>“New <span>Wine</span> in <span>Old</span> <span>Bottles</span>: <span>A</span> <span>Sequential</span> <span>Estimation</span> <span>Technique</span> for the <span>LPM</span>,”</span> 43. <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=383102">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=383102</a>.
</div>
<div id="ref-rencher_linear_2008" class="csl-entry" role="listitem">
Rencher, Alvin C., and G. Bruce Schaalje. 2008. <em>Linear Models in Statistics</em>. 2nd ed. Hoboken, N.J: Wiley-Interscience. <a href="http://www.utstat.toronto.edu/~brunner/books/LinearModelsInStatistics.pdf">http://www.utstat.toronto.edu/~brunner/books/LinearModelsInStatistics.pdf</a>.
</div>
<div id="ref-white_heteroskedasticity-consistent_1980" class="csl-entry" role="listitem">
White, Halbert. 1980. <span>“A <span>Heteroskedasticity</span>-<span>Consistent</span> <span>Covariance</span> <span>Matrix</span> <span>Estimator</span> and a <span>Direct</span> <span>Test</span> for <span>Heteroskedasticity</span>.”</span> <em>Econometrica</em> 48 (4): 817. <a href="https://doi.org/10.2307/1912934">https://doi.org/10.2307/1912934</a>.
</div>
<div id="ref-wooldridge_econometric_2001" class="csl-entry" role="listitem">
Wooldridge, Jeffrey M. 2001. <em>Econometric <span>Analysis</span> of <span>Cross</span> <span>Section</span> and <span>Panel</span> <span>Data</span></em>. <a href="https://mitpress.mit.edu/books/econometric-analysis-cross-section-and-panel-data-second-edition">https://mitpress.mit.edu/books/econometric-analysis-cross-section-and-panel-data-second-edition</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>An M-estimator <span class="math inline">\(\hat \theta\)</span> is a solution to</p>
<p><span class="math display">\[\begin{align*}
  \sum_{i=1}^n \psi(Y_i, \hat \theta) = 0
\end{align*}\]</span></p>
<p>for some function <span class="math inline">\(\psi\)</span>. Think of <span class="math inline">\(\psi\)</span> as a score function or the derivative of a proper scoring rule. If the true value <span class="math inline">\(\theta_0\)</span> is the unique solution to</p>
<p><span class="math display">\[\begin{align*}
  \mathbb{E} \left( \sum_{i=1}^n \psi(Y_i, \theta_0) \right) = 0
\end{align*}\]</span></p>
<p>then <span class="math inline">\(\hat \theta \to \theta_0\)</span>. So it’s relatively easy to check if an M-estimator is consistent. Additionally, under some regularity conditions, provided that</p>
<p><span class="math display">\[\begin{align*}
  A(\theta_0) = \mathbb{E} \left( - \psi' (Y_i, \theta_0) \right)
\end{align*}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align*}
  B(\theta_0) = \mathbb{E} \left( \psi(Y_i, \theta_0) \psi(Y_i, \theta_0)^T \right)
\end{align*}\]</span></p>
<p>exist for all <span class="math inline">\(i = 1, ..., n\)</span>, then <span class="math inline">\(\hat \theta\)</span> is also asymptotically normal with variance <span class="math inline">\(A(\theta_0)^{-1} \cdot B(\theta_0) \cdot \{ A(\theta_0)^{-1} \}^T\)</span>. We also have that</p>
<p><span class="math display">\[\begin{align*}
  A(\hat \theta)^{-1} \cdot B(\hat \theta) \cdot \{ A(\hat \theta)^{-1} \}^T
\rightarrow A(\theta_0)^{-1} \cdot B(\theta_0) \cdot \{ A(\theta_0)^{-1} \}^T
\end{align*}\]</span></p>
<p>so life is pretty great and we can get asymptotic estimates of uncertainty for <span class="math inline">\(\hat \theta\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In a lot of experimental settings that compare categorical treatments, you fit a fully saturated model that allocates a full parameter to the mean of each combination of treatments. <span class="math inline">\(\hat \beta_\text{OLS}\)</span> is consistent in this setting under both logistic regression and the LPM.</p>
<p>Let’s prove this for logistic regression first. Suppose we have groups <span class="math inline">\(1, ..., k\)</span> and the data comes from a logistic regression model. We represent the groups in the model via one hot coding, omitting an intercept for identifability. That is, we assume</p>
<p><span class="math display">\[\begin{align}
  P(Y = 1 | X) = \frac{1}{1 + \exp(-(\alpha_1 X_1 + ... + \alpha_k X_k))}
\end{align}\]</span></p>
<p>where <span class="math inline">\(X_j = 1\)</span> if the observation comes from group <span class="math inline">\(j\)</span> and is zero otherwise. Note that, if <span class="math inline">\(X_j = 1\)</span>, then <span class="math inline">\(X_i = 0\)</span> for all <span class="math inline">\(i \neq j\)</span>. To verify that the mean function is correctly specified, and thus that OLS is consistent, we need <span class="math inline">\(\mathbb{E}(Y|X) = X^T \beta\)</span>.</p>
<p>Put <span class="math inline">\(\beta_j = \frac{1}{1 + \exp(-\alpha_j)}\)</span>, which is the mean of the <span class="math inline">\(j^{th}\)</span> group. Recall that <span class="math inline">\(X_j\)</span> is fixed, so that <span class="math inline">\(P(X_j = 1) = X_j\)</span>. Then:</p>
<p><span class="math display">\[\begin{align}
  \mathbb{E} (Y | X)
  &amp;= P(Y = 1 | X) \\
  &amp;= P(Y = 1|X_1 = 1) \cdot P(X_1 = 1) + ... +
    P(Y = 1|X_k = 1) \cdot P(X_k = 1) \\
  &amp;= P(Y = 1|X_1 = 1) \cdot X_1 + ... + P(Y = 1|X_k = 1) \cdot X_k \\
  &amp;= \frac{1}{1 + \exp(-\alpha_1)} \cdot X_1 + ... +
    \frac{1}{1 + \exp(-\alpha_k)} \cdot X_k \\
  &amp;= \beta_1 X_1 + ... + \beta_k X_k \\
  &amp;= X^T \beta
\end{align}\]</span></p>
<p>Thus <span class="math inline">\(\hat \beta_\text{OLS} \to \beta\)</span>. So the OLS estimates are consistent for the group means, and <span class="math inline">\(\left(\hat \beta_\text{OLS} \right)_j \to \frac{1}{1 + \exp(-\alpha_j)}\)</span>.</p>
<p>Okay, now we’re halfway done but we still need to show consistency under the LPM. We assume the model is</p>
<p><span class="math display">\[\begin{align}
  P(Y = 1 | X) = \min(1, \max(0, \gamma_1 X_1 + ... + \gamma_k X_k))
\end{align}\]</span></p>
<p>Putting <span class="math inline">\(\beta_j = \min(1, \max(0, \gamma_j))\)</span> we repeat the same procedure almost verbatim and see:</p>
<p><span class="math display">\[\begin{align}
  \mathbb{E} (Y|X)
  &amp;= P(Y = 1|X) \\
  &amp;= P(Y = 1|X_1 = 1) \cdot P(X_1 = 1) + ... +
    P(Y = 1|X_k = 1) \cdot P(X_k = 1) \\
  &amp;= P(Y = 1|X_1 = 1) \cdot X_1 + ... + P(Y = 1|X_k = 1) \cdot X_k \\
  &amp;= \min(1, \max(0, \gamma_1)) \cdot X_1 + ... +
     \min(1, \max(0, \gamma_k)) \cdot X_k \\
  &amp;= \beta_1 X_1 + ... + \beta_k X_k \\
  &amp;= X^T \beta
\end{align}\]</span></p>
<p>Thus <span class="math inline">\(\hat \beta_\text{OLS} \to \beta\)</span>. So the OLS estimates are again consistent for the group means <span class="math inline">\(\beta_j = \min(1, \max(0, \gamma_j))\)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Michael Weylandt suggested the following example as an illustration of this. Suppose we are going to compare just two groups, so <span class="math inline">\(X_i \in \{0, 1\}\)</span>, and the true model is <span class="math inline">\(P(Y = 1|X) = \mathrm{logit}^{-1} (1 - 2 X_i)\)</span>. Consider the infinite data limit so we don’t have to worry about estimation error.</p>
<p>We know that <span class="math inline">\(P(Y_i = 1 | X_i = 0) = 0.731\)</span> and <span class="math inline">\(P(Y_i = 1 | X_i = 1) = 0.269\)</span>. <span class="math inline">\(\hat \beta_\text{logistic MLE} = \beta = (1, -2)\)</span> by the consistency of the MLE. The OLS fit will yield <span class="math inline">\(P(Y_i = 1 | X_i) = 0.731 - 0.462 X_i\)</span>. The predictions for <span class="math inline">\(\mathbb{E}(Y|X) = P(Y=1|X)\)</span> agree, but <span class="math inline">\(\hat \beta_\text{OLS} = (0.731, -0.4621)\)</span> while <span class="math inline">\(\beta = (1, -2)\)</span>.</p>
<p>This is a concrete application of the result in the previous footnote: <span class="math inline">\(\hat \beta_\text{OLS}\)</span> being consistent for group means only results in <span class="math inline">\(\hat \beta_\text{OLS}\)</span> being consistent for the model parameters <span class="math inline">\(\beta\)</span> if the model parameters are themselves the group means.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>I.I.D. assumptions are often violated in real life, for example!</p>
<p>My advisor made an interesting point about critiquing assumptions the other day. He said that it isn’t enough to say “I don’t think that assumption holds” when evaluating a model or an estimator. Instead, to show than the assumption is unacceptable, he claims you need to demonstrate that making a particular assumption results in some form of undesirable inference.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Gomila also makes an argument against using <span class="math inline">\(\hat \beta_\text{logistic MLE}\)</span> based on some papers that show it handles interactions poorly. This sort of misses the forest for the trees in my opinion. GLMs are generically dubious models because few empirical conditional expectations are actually linear. And maybe linear main effects won’t kill you, but modeling interactions with linear terms amounts to fitting global hyperplanes for interactions. This is undesirable because interactions are almost always local, not global. If you want to model interactions carefully, you need something flexible like a GAM or kernels or whatnot.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/www\.alexpghayes\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="alexpghayes/quarto-blog" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/alexpghayes/quarto-blog/edit/main/post/2019-08-31_consistency-and-the-linear-probability-model/index.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/alexpghayes/quarto-blog/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://github.com/alexpghayes/quarto-blog">
<p>website source</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>