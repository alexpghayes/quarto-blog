<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Hayes">
<meta name="dcterms.date" content="2020-01-06">

<title>overfitting: a guided tour – alex hayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/all.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-G6PQFW2XSK"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-G6PQFW2XSK', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="overfitting: a guided tour – alex hayes">
<meta property="og:description" content="fleshing out intuition about structure in random processes beyond the standard bias-variance decomposition">
<meta property="og:image" content="https://www.alexpghayes.com/post/2020-01-06_overfitting-a-guide-tour/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==">
<meta property="og:site_name" content="alex hayes">
<meta name="twitter:title" content="overfitting: a guided tour – alex hayes">
<meta name="twitter:description" content="fleshing out intuition about structure in random processes beyond the standard bias-variance decomposition">
<meta name="twitter:image" content="https://www.alexpghayes.com/post/2020-01-06_overfitting-a-guide-tour/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==">
<meta name="twitter:creator" content="@alexpghayes">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">alex hayes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">about</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../code/index.html"> 
<span class="menu-text">code</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching/index.html"> 
<span class="menu-text">teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../talks/index.html"> 
<span class="menu-text">talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resume/index.html"> 
<span class="menu-text">resume</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://bsky.app/profile/alexpghayes.com"> 
<span class="menu-text"><i class="fa-brands fa-bluesky" aria-label="bluesky"></i></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/alexpghayes"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/alexpghayes/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">overfitting: a guided tour</h1>
            <p class="subtitle lead"></p><p>fleshing out intuition about structure in random processes beyond the standard bias-variance decomposition</p><p></p>
                                <div class="quarto-categories">
                <div class="quarto-category">math stat</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://www.alexpghayes.com">Alex Hayes</a> <a href="https://orcid.org/0000-0002-4985-5160" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 6, 2020</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#summary" id="toc-summary" class="nav-link active" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#some-intuition-via-a-guessing-game" id="toc-some-intuition-via-a-guessing-game" class="nav-link" data-scroll-target="#some-intuition-via-a-guessing-game">Some intuition via a guessing game</a></li>
  <li><a href="#sources-of-structure" id="toc-sources-of-structure" class="nav-link" data-scroll-target="#sources-of-structure">Sources of structure</a>
  <ul class="collapse">
  <li><a href="#example-polynomial-regression" id="toc-example-polynomial-regression" class="nav-link" data-scroll-target="#example-polynomial-regression">Example: polynomial regression</a></li>
  <li><a href="#example-model-selection-amongst-polynomial-regressions" id="toc-example-model-selection-amongst-polynomial-regressions" class="nav-link" data-scroll-target="#example-model-selection-amongst-polynomial-regressions">Example: model selection amongst polynomial regressions</a></li>
  <li><a href="#example-estimating-cluster-memberships" id="toc-example-estimating-cluster-memberships" class="nav-link" data-scroll-target="#example-estimating-cluster-memberships">Example: estimating cluster memberships</a></li>
  </ul></li>
  <li><a href="#how-to-handle-overfitting" id="toc-how-to-handle-overfitting" class="nav-link" data-scroll-target="#how-to-handle-overfitting">How to handle overfitting</a></li>
  <li><a href="#overfitting-in-prediction" id="toc-overfitting-in-prediction" class="nav-link" data-scroll-target="#overfitting-in-prediction">Overfitting in prediction</a></li>
  <li><a href="#pulling-it-all-together" id="toc-pulling-it-all-together" class="nav-link" data-scroll-target="#pulling-it-all-together">Pulling it all together</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading">Further reading</a>
  <ul class="collapse">
  <li><a href="#acknowledgements" id="toc-acknowledgements" class="nav-link" data-scroll-target="#acknowledgements">Acknowledgements</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/alexpghayes/quarto-blog/edit/main/post/2020-01-06_overfitting-a-guide-tour/index.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/alexpghayes/quarto-blog/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>This post introduces overfitting, describes how overfitting influences both prediction and inference problems, provides supervised and unsupervised examples of overfitting, and presents a fundamental relationship between train and test error. The goal is to provide some additional intuition beyond material covered in introductory machine learning resources.</p>
</section>
<section id="some-intuition-via-a-guessing-game" class="level2">
<h2 class="anchored" data-anchor-id="some-intuition-via-a-guessing-game">Some intuition via a guessing game</h2>
<p>Before we begin, I want to play a guessing game. Here’s how it works: I show you two sequences of coin flips. You have to guess which sequence is random and which one I made up.</p>
<p>Okay, here are the first two sequences. Which one do you think is random?</p>
<pre><code>A. THHTTHTTHTHHTHTHHHTT 
B. HTTHTHTHHHHHHHHHTTHT</code></pre>
<p>Let’s play again. Now the sequences are:</p>
<pre><code>A. HHTHTTHHTTTHHHTTHTTT
B. HTHTTHHHTHHHHTTHHHTT</code></pre>
<p>One last time.</p>
<pre><code>A. HTTHTTHHHTHTHHTTTTHH
B. HHTHTTHHTTTHHTTTHHTH</code></pre>
<p>The answers are in a footnote<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. In any case, there’s a simple rule you can use to tell the random sequences from the human generated sequences: the random sequences are the sequences with the longest substring of all heads or all tails.</p>
<p>The gist is that human intuition is bad at solving this problem. Long sequences of all heads or all tails don’t <em>look</em> random–they appear overly structured. But long sequences of heads and tails are in fact quite probable under independent coin flipping! This example illustrates a fundamental fact that human brains often struggle with:</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Random processes produce highly structured data.</p>
</div>
</div>
<p>Until we understand this, it’s hard to build any intuition for overfitting.</p>
</section>
<section id="sources-of-structure" class="level2">
<h2 class="anchored" data-anchor-id="sources-of-structure">Sources of structure</h2>
<p>A major challenge in probability modeling is that there are two sources of structure in data:</p>
<ol type="1">
<li><p>apparent, happenstance structure that originates in the randomness of the data generating process, and</p></li>
<li><p>systematic structure inherent in the data generating process.</p></li>
</ol>
<p>When we observe structure in data, we don’t know where it came from. If the structure comes from randomness in the data generating process, we would like to ignore it. If the structure is the result of some fundamental latent characteristic of the phenomena we are studying, we want to study it or leverage it to improve our estimates. When we mistakenly confused random, apparent structure for true, systematic structure, we call this mistake <strong>overfitting</strong>.</p>
<section id="example-polynomial-regression" class="level3">
<h3 class="anchored" data-anchor-id="example-polynomial-regression">Example: polynomial regression</h3>
<p>Let’s consider a common statistical problem: prediction. Suppose we have twenty pairs of observations <span class="math inline">\((X_i, Y_i)\)</span>, and we believe that a Gaussian linear model is appropriate. We would like to estimate <span class="math inline">\(\mathbb{E}(Y|X)\)</span>, the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>.</p>
<p>If we use an overly flexible estimator that assumes the data has a more complicated data generating process than it truly does, we can quickly run into overfitting. Consider 20 i.i.d. observations from the simple model:</p>
<p><span class="math display">\[
\begin{align*}  
  Y_i \sim 2 - 0.3 \cdot X_i + \varepsilon_i \\
  \varepsilon_i \sim \mathrm{Normal}(0, 0.3^2)
\end{align*}
\]</span></p>
<p>The simulated data looks like:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>If we consider a <span class="math inline">\(7^{th}\)</span> degree polynomial fit, our estimate for the conditional expectation is shown in maroon below and looks like:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Our predictions have conformed to random variation in the data rather than systematic variation in the data, and using the polynomial fit for inference or prediction is a bad idea. By contrast, a correctly specified linear model does much better.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="example-model-selection-amongst-polynomial-regressions" class="level3">
<h3 class="anchored" data-anchor-id="example-model-selection-amongst-polynomial-regressions">Example: model selection amongst polynomial regressions</h3>
<p>Overfitting is a major concern especially when we are choosing between several models. This is because overfit models will look good with respect to some measures of goodness of fit. For example, suppose we want to use the same data from above, and we want to pick a model from several polynomial regression models with differing degrees. We might propose the following estimation procedure:</p>
<ol type="1">
<li>For <span class="math inline">\(k \in \{1, 2, ..., 20\}\)</span>
<ul>
<li>Fit a degree <span class="math inline">\(k\)</span> polynomial regression under the assumption that <span class="math inline">\(y = \beta_0 + \beta_1 \cdot x + \beta_2 \cdot x^2 + ... + \beta_k \cdot x^k + \varepsilon\)</span></li>
<li>Record the <span class="math inline">\(R^2\)</span> of the model</li>
</ul></li>
<li>Pick the model that maximizes the <span class="math inline">\(R^2\)</span> of the linear regression model</li>
</ol>
<p>We plot <span class="math inline">\(R^2\)</span> versus degree below, and see that we would select a very high degree, even though the true data generating process has degree one.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We can visualize some of the models, and see that high order polynomials fit the data better and better while doing a worse and worse job at estimating the systematic structure in the data.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>So overfitting is a concern when we consider a single model, and also when we want to compare many different models. Note that we don’t have to perfectly interpolate the data to overfit! Any of the models with degree larger than one results in a bad estimate of <span class="math inline">\(\mathbb{E}(Y|X)\)</span>. Typically, models with more parameters are more flexible and more prone to overfitting, but this is not always the case<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
</section>
<section id="example-estimating-cluster-memberships" class="level3">
<h3 class="anchored" data-anchor-id="example-estimating-cluster-memberships">Example: estimating cluster memberships</h3>
<p>Before we move on, I want to reiterate my point that overfitting occurs in all contexts, not just prediction. Suppose we want to find clusters in bivariate data, and we have reason to believe that a Gaussian mixture model is appropriate. We don’t know how many components are in the mixture, so we start off by trying a model with five components.</p>
<p>When the data has two components, that is, it comes from a simpler data generating process, our estimator will overfit and make several errors at once:</p>
<ul>
<li>It will get the centers of the clusters wrong</li>
<li>It will get the covariances of the clusters wrong</li>
<li>It will assign data points that belong to the same cluster to different clusters</li>
</ul>
<p>For a concrete example we simulate some bivariate data with two clusters.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>But if we assume that the data comes from a more flexible model, our estimated cluster memberships are wonky and off:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="how-to-handle-overfitting" class="level2">
<h2 class="anchored" data-anchor-id="how-to-handle-overfitting">How to handle overfitting</h2>
<p>Very roughly, there are three tricks we can use to mitigate overfitting.</p>
<ol type="1">
<li>Use domain knowledge to consider appropriate classes of models</li>
<li>Penalize model complexity</li>
<li>Sample splitting</li>
</ol>
<p>For the rest of this post, I want to present some basic results on overfitting in a supervised learning context and to give you some intuition on why sample splitting helps.</p>
<p>If you’re interested in penalization approaches, key search phrases are “regularization”, “degrees of freedom” and “bias-variance tradeoff”. Many machine learning textbooks describe estimation procedures and model selection procedures that use some form of penalization. See <span class="citation" data-cites="belkin_reconciling_2018">Belkin et al. (<a href="#ref-belkin_reconciling_2018" role="doc-biblioref">2018</a>)</span> for an overview of some interesting recent developments concerning model complexity. For a more theoretical treatment of penalization in a model selection context you may enjoy the first couple chapters of <span class="citation" data-cites="massart_concentration_2003">Massart (<a href="#ref-massart_concentration_2003" role="doc-biblioref">2003</a>)</span>.</p>
</section>
<section id="overfitting-in-prediction" class="level2">
<h2 class="anchored" data-anchor-id="overfitting-in-prediction">Overfitting in prediction</h2>
<p>If you come from the machine learning community, you may think overfitting is the difference between predictive performance on training data and test data. To use more statistical language, think of a flexible machine learning estimator for <span class="math inline">\(\mathbb{E}(Y|X)\)</span> like a random forest. We fit the random forest on training data, which is sampled from some data generating process. We hope the random forest only finds systematic structure in noisy, observed conditionals mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, but it will also conform to random variations in the conditional mean. These random variations will not be present in the test set, which will have only the underlying systematic structure plus new random variation. Since random variation in the training set results in a poor estimate of the systematic structure in the data generating process, our overfit estimate will make mistakes when looking for systematic structure in the test set. Thus overfitting will reduce predictive performance<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. This is sometimes described as the random forest “memorizing the training set”.</p>
<p>It turns out that we can study this phenomena more formally. The math is easiest in a restricted setting, but the intuition generalizes well. First, we assume that our data is independent and identically distributed, where</p>
<p><span class="math display">\[
Y_i = f(X_i) + \varepsilon_i.
\]</span></p>
<p>Here <span class="math inline">\(f(X_i)\)</span> describes how <span class="math inline">\(Y_i\)</span> varies systematically with <span class="math inline">\(X_i\)</span>, which we assume is fixed (this is the restrictive assumption). <span class="math inline">\(\varepsilon_i\)</span> represents random error, which we take to be mean zero with variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>We consider a training set <span class="math inline">\(X_1, ..., X_n\)</span>, and obtain predicted values <span class="math inline">\(\widehat Y_1, ..., \widehat Y_n\)</span>. Then we consider a test set, <em>observed at the same values</em> <span class="math inline">\(X_i\)</span>, but with new random errors <span class="math inline">\(\varepsilon_i^*\)</span>. So our test set is the set of observations</p>
<p><span class="math display">\[
Y_i^* = f(X_i) + \varepsilon_i^*.
\]</span></p>
<p>Let our predicted values at <span class="math inline">\(X_i\)</span>, obtained using only the training set, be <span class="math inline">\(\widehat Y_i = \hat f (X_i)\)</span>, where <span class="math inline">\(\hat f\)</span> represents our estimator for the conditional mean (i.e.&nbsp;predictive model). Note that these are not just the predicted values for the training set, but <em>also the predicted values for the test set, since</em> <span class="math inline">\(X_i\)</span> is fixed.</p>
<p>Under <span class="math inline">\(\ell_2\)</span> loss, the training error is</p>
<p><span class="math display">\[
\frac 1n \sum_{i=1}^n (Y_i - \widehat Y_i)^2
\]</span></p>
<p>and the test error is</p>
<p><span class="math display">\[
\frac 1n \sum_{i=1}^n (Y_i^* - \widehat Y_i)^2.
\]</span></p>
<p>We know that the training error should be less than the test error, and we can in fact formalize the relationship between these two measures. In particular, we have:</p>
<p><span class="math display">\[
\begin{align}
  \underbrace{
    \mathbb{E} \left[ \frac 1n \sum_{i=1}^n \left( Y_i^* - \widehat Y_i \right)^2 \right]
  }_\text{test error}
  =
  \underbrace{
    \mathbb{E} \left[ \frac 1n \sum_{i=1}^n \left( Y_i - \hat Y_i \right)^2 \right]
  }_\text{training error} +
  \underbrace{
    \frac 2n \cdot \sum_{i=1}^n \mathrm{Cov} \left( \widehat Y_i, Y_i \right)
  }_\text{optimism}
\end{align}
\]</span> This relationship holds for most important loss functions. It means tells us that test error, or generalization error, is almost always higher than in-sample error evaluated on the training set. We call the amount by which the training error underestimates the test error the optimism; the more optimism, the greater the discrepancy between in-sample and out-of-sample error.</p>
<p>We’ll discuss the implications the train-test error relationship more in a moment, but first let’s prove it. Feel free to skip the proof, although it relies only on basic properties of the expectation and variance.</p>
<p><strong>Proof</strong>. We’ll follow the proof in <a href="http://www.stat.cmu.edu/~larry/=stat401/lecture-21.pdf">these course notes</a> by Larry Wasserman. Consider the <span class="math inline">\(i^{th}\)</span> observation. Then</p>
<p><span class="math display">\[
\begin{align}
  \underbrace{
    \mathbb{E} \left[ \left( Y_i - \hat Y_i \right)^2 \right]
  }_\text{training error at $X_i$}
&amp;= \mathrm{Var} \left[ Y_i - \widehat Y_i \right]
  + \left( \mathbb{E} \left[ Y_i - \widehat Y_i \right] \right)^2 \\
&amp;= \mathrm{Var} \left[ Y_i \right]
  + \mathrm{Var} \left[ \widehat Y_i \right]
  - 2 \, \mathrm{Cov} \left[ Y_i, \widehat Y_i \right]
  + \left(
    \mathbb{E} \left[ Y_i \right]
    - \mathbb{E} \left[\widehat Y_i \right]
  \right)^2
\end{align}
\]</span></p>
<p>and also</p>
<p><span class="math display">\[
\begin{align}
  \underbrace{
    \mathbb{E} \left[ \left( Y_i^* - \widehat Y_i \right)^2 \right]
  }_\text{test error at $X_i$}
&amp;= \mathrm{Var} \left[ Y_i^* - \widehat Y_i \right]
  + \left( \mathbb{E} \left[ Y_i^* - \widehat Y_i \right] \right)^2 \\
&amp;= \mathrm{Var} \left[ Y_i^* \right]
  + \mathrm{Var} \left[ \widehat Y_i \right]
  - 2 \, \mathrm{Cov} \left[ Y_i^*, \widehat Y_i \right]
  + \left(
    \mathbb{E} \left[ Y_i^* \right]
    - \mathbb{E} \left[\widehat Y_i \right]
  \right)^2.
\end{align}
\]</span></p>
<p>Now we consider several implications of the fact that <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(Y_i^*\)</span> are independent and identically distributed. In particular, we have <span class="math inline">\(\mathbb{E}(Y_i) = \mathbb{E}(Y_i^*)\)</span>, <span class="math inline">\(\mathrm{Var}(Y_i) = \mathrm{Var}(Y_i^*)\)</span>, and most importantly <span class="math inline">\(\mathrm{Cov} \left[ Y_i^*, \widehat Y_i \right] = \mathrm{Cov} \left[ f(X_i) + \varepsilon_i^*, \hat f(X_i) \right] = \mathrm{Cov} \left[ \varepsilon_i^*, \hat f(X_i) \right] = 0\)</span>. Thus we see</p>
<p><span class="math display">\[
\begin{align}
  \underbrace{
    \mathbb{E} \left[ \left( Y_i^* - \widehat Y_i \right)^2 \right]
  }_\text{test error at $X_i$}
  &amp;= \mathrm{Var} \left[ Y_i \right]
    + \mathrm{Var} \left[ \widehat Y_i \right]
    + \left(
      \mathbb{E} \left[ Y_i^* \right]   
      - \mathbb{E} \left[\widehat Y_i \right]
    \right)^2 \\
  &amp;=
    \underbrace{
      \mathbb{E} \left[ \left( Y_i - \widehat Y_i \right)^2 \right]
    }_\text{training error at $X_i$}
    + 2 \, \underbrace{
      \mathrm{Cov} \left[ Y_i, \widehat Y_i \right]
    }_\text{how much $\hat f$ memorized $Y_i$}
\end{align}
\]</span></p>
<p>where in the last equality we substitute based on our previous decomposition of the training error. Summing over all <span class="math inline">\(i\)</span> and dividing by <span class="math inline">\(n\)</span> <strong>finishes the proof</strong>.</p>
<p>Let’s consider the implications of the train-test error relationship in two extreme cases. First, suppose that the <span class="math inline">\(\widehat Y_i = Y_i\)</span>. This means that our estimator <span class="math inline">\(\hat f\)</span> has perfectly memorized the training set. In this case, there is zero training error, but the optimism is <span class="math inline">\(2 \sigma^2\)</span>, which is pretty much the worst possible case amongst reasonable estimators (in the fixed <span class="math inline">\(X_i\)</span> setting).</p>
<p>In the flip case, the estimator doesn’t memorize the training set at all, so there is no dependence (and thereby no covariance) between the predictions <span class="math inline">\(\widehat Y_i\)</span> and the training labels <span class="math inline">\(Y_i\)</span> (really, the errors <span class="math inline">\(\varepsilon_i\)</span>, since that’s the only random component of <span class="math inline">\(Y_i\)</span>). This means the estimator <span class="math inline">\(\hat f\)</span> has ignored the random errors <span class="math inline">\(\varepsilon_i\)</span> and has learned only generalizable knowledge!</p>
<p>In fact, it’s often useful to treat</p>
<p><span class="math display">\[
\frac{1}{\sigma^2} \sum_{i=1}^n \mathrm{Cov}
\left[ \widehat Y_i, Y_i \right]
\]</span></p>
<p>as a generalized notion of “effective number of parameters” or “degrees of freedom” that measures the complexity of a predictive estimator and its capacity to memorize the training set<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<p>Finally, this theorem suggests why sample splitting can give us good estimates of test error. If we have an independent dataset, and we assess the performance of the estimator <span class="math inline">\(\hat f\)</span> on the independent dataset, the predictions on this dataset will be independent from the training data, the covariance will be zero, and the optimism term will disappear. Thus we get an unbiased estimate of the loss on new data. This is sometimes called “unbiased risk estimation”<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<p>Cross validation now is a natural generalization of our hold out estimator. We have an unbiased estimate, so all the error in our estimate of the generalization error comes from variance. If we can generate <span class="math inline">\(k\)</span> independent-ish estimates of generalization error and average them, then we will reduce the variance in the risk estimate. It’s natural to partition the data into <span class="math inline">\(k\)</span> non-overlapping sets, fit a model <span class="math inline">\(\hat f_j\)</span> on all but the <span class="math inline">\(j^{th}\)</span> partition, and estimate the validation error on the unseen <span class="math inline">\(j^{th}\)</span> portion of the data. In terms of intuition, we then average these “unbiased” and “independent” estimates get a new “unbiased estimate with smaller variance”. In practice, the situation is more complicated, but cross-validation nonetheless turns out to be a good idea<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>
</section>
<section id="pulling-it-all-together" class="level2">
<h2 class="anchored" data-anchor-id="pulling-it-all-together">Pulling it all together</h2>
<p>Data has structure. Some of this structure is systematic, and some is random. We care about the systematic structure, but we don’t want to confuse the random structure for systematic structure.</p>
<p>Once we start estimating things, we need to be careful about how flexible we allow our models to be. This is true for both inferential and predictive modeling. If we allow our models to be more flexible than the true data generating process, we will mistake random structure for systematic structure. On the flip side, if we use models that don’t contain the true data generating process, we won’t capture all the systematic structure in the data (and we can also confuse the systematic and random structure).</p>
<p>In the prediction context, we saw that test error is higher than training error due to covariance between predictions and random errors in the training set. Hopefully this example demonstrates how easy it is to confuse systematic error and random error, and provides some intuition that you can use when analyzing data down the line.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further reading</h2>
<p>When <span class="math inline">\(X_i\)</span> is random, the intuition about the train-test performance relationship is pretty much the same as for the fixed <span class="math inline">\(X_i\)</span> case, but the randomness in <span class="math inline">\(X_i\)</span> contributes some additional terms to the test set error. <span class="citation" data-cites="rosset_fixed-x_2017">Rosset and Tibshirani (<a href="#ref-rosset_fixed-x_2017" role="doc-biblioref">2017</a>)</span> discusses these additional terms.</p>
<p>Note that getting good estimates of risk is a key element of model selection, but that cross validation is not a silver bullet. For example <span class="citation" data-cites="shao_linear_1993">Shao (<a href="#ref-shao_linear_1993" role="doc-biblioref">1993</a>)</span> proves the cross validation isn’t consistent for selecting amongst linear models, and <span class="citation" data-cites="benavoli_time_2017">Benavoli et al. (<a href="#ref-benavoli_time_2017" role="doc-biblioref">2017</a>)</span> and <span class="citation" data-cites="lei_cross-validation_2017">Lei (<a href="#ref-lei_cross-validation_2017" role="doc-biblioref">2017</a>)</span> discuss the need for additional modeling of cross-validated risk estimates to find the most predictive model. Section 2 of <span class="citation" data-cites="lei_cross-validation_2017">Lei (<a href="#ref-lei_cross-validation_2017" role="doc-biblioref">2017</a>)</span> is an especially eloquent introduction to cross-validation<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. <span class="citation" data-cites="roberts_cross-validation_2017">Roberts et al. (<a href="#ref-roberts_cross-validation_2017" role="doc-biblioref">2017</a>)</span> is a nice introduction to specialized types of cross validation that respect independence structures found in spatial, temporal and network data. <span class="citation" data-cites="vehtari_practical_2017">Vehtari, Gelman, and Gabry (<a href="#ref-vehtari_practical_2017" role="doc-biblioref">2017</a>)</span> presents modern Bayesian approaches to sample splitting for model selection, and <a href="https://statmodeling.stat.columbia.edu/2018/06/21/i-am-the-supercargo/">this blog post</a> by Dan Simpson discusses the same work for non-i.i.d. data.</p>
<p>Chapter 7 of <span class="citation" data-cites="hastie_elements_2008">Hastie, Tibshirani, and Friedman (<a href="#ref-hastie_elements_2008" role="doc-biblioref">2008</a>)</span> presents an overview of cross-validation in the machine learning context. <span class="citation" data-cites="arlot_survey_2010">Arlot and Celisse (<a href="#ref-arlot_survey_2010" role="doc-biblioref">2010</a>)</span> is a comprehensive review of results in the cross-validation literature, in particular discussing how cross-validation procedures different when the goal is risk estimation (i.e.&nbsp;determining predictive performance) versus model selection (i.e.&nbsp;choosing hyperparameters). <span class="citation" data-cites="dudoit_asymptotics_2005">Dudoit and Laan (<a href="#ref-dudoit_asymptotics_2005" role="doc-biblioref">2005</a>)</span> proves an oracle efficiency result about model selection with cross-validation. Finally, cross-validation was introduced in the now classic <span class="citation" data-cites="stone_cross-validatory_1974">Stone (<a href="#ref-stone_cross-validatory_1974" role="doc-biblioref">1974</a>)</span>.</p>
<section id="acknowledgements" class="level3">
<h3 class="anchored" data-anchor-id="acknowledgements">Acknowledgements</h3>
<p>I’d like to thank <a href="https://juliasilge.com/">Julia Silge</a> for providing feedback on a draft of this post!</p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-arlot_survey_2010" class="csl-entry" role="listitem">
Arlot, Sylvain, and Alain Celisse. 2010. <span>“A Survey of Cross-Validation Procedures for Model Selection.”</span> <em>Statistics Surveys</em> 4 (0): 40–79. <a href="https://doi.org/10.1214/09-SS054">https://doi.org/10.1214/09-SS054</a>.
</div>
<div id="ref-belkin_reconciling_2018" class="csl-entry" role="listitem">
Belkin, Mikhail, Daniel Hsu, Siyuan Ma, and Soumik Mandal. 2018. <span>“Reconciling Modern Machine Learning and the Bias-Variance Trade-Off.”</span> <em>arXiv:1812.11118 [Cs, Stat]</em>, December. <a href="http://arxiv.org/abs/1812.11118">http://arxiv.org/abs/1812.11118</a>.
</div>
<div id="ref-benavoli_time_2017" class="csl-entry" role="listitem">
Benavoli, Alessio, Giorgio Corani, Janez Demˇsar, and Marco Zaﬀalon. 2017. <span>“Time for a <span>Change</span>: A <span>Tutorial</span> for <span>Comparing</span> <span>Multiple</span> <span>Classiﬁers</span> <span>Through</span> <span>Bayesian</span> <span>Analysis</span>,”</span> 36.
</div>
<div id="ref-bengio_no_2004" class="csl-entry" role="listitem">
Bengio, Yoshua, and Yves Grandvalet. 2004. <span>“No <span>Unbiased</span> <span>Estimator</span> of the <span>Variance</span> of <span>K</span>-<span>Fold</span> <span>Cross</span>-<span>Validation</span>,”</span> 17.
</div>
<div id="ref-dudoit_asymptotics_2005" class="csl-entry" role="listitem">
Dudoit, Sandrine, and Mark J. van der Laan. 2005. <span>“Asymptotics of Cross-Validated Risk Estimation in Estimator Selection and Performance Assessment.”</span> <em>Statistical Methodology</em> 2 (2): 131–54. <a href="https://doi.org/10.1016/j.stamet.2005.02.003">https://doi.org/10.1016/j.stamet.2005.02.003</a>.
</div>
<div id="ref-hastie_elements_2008" class="csl-entry" role="listitem">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2008. <em>The <span>Elements</span> of <span>Statistical</span> <span>Learning</span></em>.
</div>
<div id="ref-lei_cross-validation_2017" class="csl-entry" role="listitem">
Lei, Jing. 2017. <span>“Cross-<span>Validation</span> with <span>Confidence</span>.”</span> <em>arXiv:1703.07904 [Stat]</em>, March. <a href="http://arxiv.org/abs/1703.07904">http://arxiv.org/abs/1703.07904</a>.
</div>
<div id="ref-massart_concentration_2003" class="csl-entry" role="listitem">
Massart, Pascal. 2003. <em>Concentration <span>Inequalities</span> and <span>Model</span> <span>Selection</span></em>. <a href="http://www.cmap.polytechnique.fr/~merlet/articles/probas_massart_stf03.pdf">http://www.cmap.polytechnique.fr/~merlet/articles/probas_massart_stf03.pdf</a>.
</div>
<div id="ref-piantadosi_one_2018" class="csl-entry" role="listitem">
Piantadosi, Steven T. 2018. <span>“One Parameter Is Always Enough.”</span> <em>AIP Advances</em> 8 (9): 095118. <a href="https://doi.org/10.1063/1.5031956">https://doi.org/10.1063/1.5031956</a>.
</div>
<div id="ref-roberts_cross-validation_2017" class="csl-entry" role="listitem">
Roberts, David R., Volker Bahn, Simone Ciuti, Mark S. Boyce, Jane Elith, Gurutzeta Guillera-Arroita, Severin Hauenstein, et al. 2017. <span>“Cross-Validation Strategies for Data with Temporal, Spatial, Hierarchical, or Phylogenetic Structure.”</span> <em>Ecography</em> 40 (8): 913–29. <a href="https://doi.org/10.1111/ecog.02881">https://doi.org/10.1111/ecog.02881</a>.
</div>
<div id="ref-rosset_fixed-x_2017" class="csl-entry" role="listitem">
Rosset, Saharon, and Ryan J. Tibshirani. 2017. <span>“From <span>Fixed</span>-<span>X</span> to <span>Random</span>-<span>X</span> <span>Regression</span>: <span>Bias</span>-<span>Variance</span> <span>Decompositions</span>, <span>Covariance</span> <span>Penalties</span>, and <span>Prediction</span> <span>Error</span> <span>Estimation</span>.”</span> <em>arXiv:1704.08160 [Stat]</em>, April. <a href="http://arxiv.org/abs/1704.08160">http://arxiv.org/abs/1704.08160</a>.
</div>
<div id="ref-shao_linear_1993" class="csl-entry" role="listitem">
Shao, Jun. 1993. <span>“Linear <span>Model</span> <span>Selection</span> by <span>Cross</span>-Validation.”</span> <em>Journal of the American Statistical Association</em> 88 (422): 486–94. <a href="https://doi.org/10.1080/01621459.1993.10476299">https://doi.org/10.1080/01621459.1993.10476299</a>.
</div>
<div id="ref-stone_cross-validatory_1974" class="csl-entry" role="listitem">
Stone, M. 1974. <span>“Cross-<span>Validatory</span> <span>Choice</span> and <span>Assessment</span> of <span>Statistical</span> <span>Predictions</span>.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 36 (2): 111–33. <a href="https://doi.org/10.1111/j.2517-6161.1974.tb00994.x">https://doi.org/10.1111/j.2517-6161.1974.tb00994.x</a>.
</div>
<div id="ref-vehtari_practical_2017" class="csl-entry" role="listitem">
Vehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. <span>“Practical <span>Bayesian</span> Model Evaluation Using Leave-One-Out Cross-Validation and <span>WAIC</span>.”</span> <em>Statistics and Computing</em> 27 (5): 1413–32. <a href="https://doi.org/10.1007/s11222-016-9696-4">https://doi.org/10.1007/s11222-016-9696-4</a>.
</div>
<div id="ref-wager_cross-validation_2019" class="csl-entry" role="listitem">
Wager, Stefan. 2019. <span>“Cross-<span>Validation</span>, <span>Risk</span> <span>Estimation</span>, and <span>Model</span> <span>Selection</span>.”</span> <em>arXiv:1909.11696 [Stat]</em>, September. <a href="http://arxiv.org/abs/1909.11696">http://arxiv.org/abs/1909.11696</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The random sequences are B, B, A. First my mom and I made up the three sequences that looked random to us. Then I generated the truly random sequences with the following <code>R</code> code:</p>
<pre><code>set.seed(27)

get_sequence_sample &lt;- function() {
  heads_and_tails &lt;- c("H", "T")
  seq &lt;- sample(heads_and_tails, size = 20, replace = TRUE)
  cat(seq, "\n", sep = "")
}

for (i in 1:3) {
  get_sequence_sample()
}</code></pre>
<p>Thanks to Dave Anderson for performing this demonstration in his probability class.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>See <span class="citation" data-cites="piantadosi_one_2018">Piantadosi (<a href="#ref-piantadosi_one_2018" role="doc-biblioref">2018</a>)</span> for an example of model with a single parameter that can nonetheless perfectly interpolates any finite number of observations.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Random variation in the test set will case irreducible error that we cannot make disappear. More often than we would like to admit, the irreducible error is large.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>There is also the bizarre scenario when an estimator produces predictions that are anticorrelated with the errors <span class="math inline">\(\varepsilon_i\)</span>, so that <span class="math inline">\(\mathrm{Cov} \left[ \widehat Y_i, Y_i \right]\)</span> is negative and test set performance is <em>better</em> than training set performance. If you observe this phenomena on an applied problem, I recommend buying a lottery ticket.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Alternatively, if we know the form of <span class="math inline">\(\hat f\)</span> and <span class="math inline">\(f\)</span> we could estimate the optimism directly and subtract it off. This is one way to motivate AIC, for example. As a general rule, estimating the optimism and subtracting it off requires much stronger assumptions than cross-validation that are unlikely to hold in practice.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>The risk estimates across the <span class="math inline">\(k\)</span> folds are both biased and dependent. If you fit a model on 80 percent of the full training data, you don’t get an unbiased estimate of the performance when the model is fit on the full data. The bias is simply because more data results in a better estimates. Second, the held-out risk estimates are not independent because the <span class="math inline">\(\hat f_j\)</span> and <span class="math inline">\(\hat f_k\)</span> are largely trained on the same data. But averaging them works out okay <span class="citation" data-cites="bengio_no_2004">(<a href="#ref-bengio_no_2004" role="doc-biblioref">Bengio and Grandvalet 2004</a>)</span> for risk estimation, and accounting for blocking across the folds with something like a random intercept model works well for model selection <span class="citation" data-cites="wager_cross-validation_2019">(<a href="#ref-wager_cross-validation_2019" role="doc-biblioref">Wager 2019</a>)</span>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>I highly recommend reading anything by Jing Lei.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/www\.alexpghayes\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="alexpghayes/quarto-blog" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/alexpghayes/quarto-blog/edit/main/post/2020-01-06_overfitting-a-guide-tour/index.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/alexpghayes/quarto-blog/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://github.com/alexpghayes/quarto-blog">
<p>website source</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>