{
  "hash": "a535a0cd7865de2e89fa6e9363013c54",
  "result": {
    "markdown": "---\ntitle: \"understanding multinomial regression with partial dependence plots\"\nsubtitle: |\n  multinomial regression wasn't clicking so i wrote code to help\ndate: \"2018-10-23\"\nexecute:\n  echo: true\n  message: false\n  warning: false\n---\n\n## Motivation\n\nThis post assumes you are familiar with logistic regression and that you just fit your first or second multinomial logistic regression model. While there is an interpretation for the coefficients in a multinomial regression, that interpretation is relative to a base class, which may not be the most useful. Partial dependence plots are an alternative way to understand multinomial regression, and in fact can be used to understand any predictive model. This post explains what partial dependence plots are and how to create them using R.\n\n## Data\n\nI'll use the built in `iris` dataset for this post. If you've already seen the iris dataset a hundred times, I apologize. Our goal will be to predict the `Species` of an iris flower based on four numerical measures of the flower: `Sepal.Length`, `Speal.Width`, `Petal.Length` and `Petal.Width`. There are 150 measurements and three species of iris: `setosa`, `versicolor` and `virginica`.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(skimr)\n\ndata <- as_tibble(iris)\nglimpse(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n```\n:::\n:::\n\n## The multinomial logistic regression model\n\nRecall that the probability of an event $y = 1$ given data $x \\in \\mathbb R^p$ in a logistic regression model is:\n\n$$\nP(y = 1|x) = {1 \\over 1 + \\exp(-\\beta^T x)}\n$$\nwhere $\\beta \\in \\mathbb R^p$ is a coefficient vector. Multinomial logistic regression generalizes this relation by assuming that we have $y \\in \\{1, 2, ..., K\\}$. Then we have coefficient vectors $\\beta_1, ..., \\beta_{k-1}$ such that\n\n$$\nP(y = k|x) = {\\exp(\\beta_k^T x) \\over 1 + \\sum_{k=1}^{K - 1} \\exp(\\beta_k^T x)}\n$$\n\nand \n\n$$\nP(y = K|x) = {1 \\over 1 + \\sum_{k=1}^{K - 1} \\exp(\\beta_k^T x)}\n$$\n\nThere are only $K-1$ coefficient vectors in order to prevent overparameterization[^1]. The purpose here isn't to describe the model in any meaningful detail, but rather to remind you of what it looks like. I strongly encourage you to read [this fantastic derivation](http://data.princeton.edu/wws509/notes/c6s3.html) of multinomial logistic regression, which follows the work that lead to McFadden's Noble prize in economics in 2000.\n\n[^1]: Some machine learning courses present multinomial regression using a $K \\times p$ coefficient matrix, but then estimate the coefficients with some sort of penalty. The penalty is necessary to prevent the likelihood from becoming infinite (in the $k \\times p$ parameterization, multiplying $\\beta$ by any constant $c$ retains the same class probabilities while inflating the likelihood). Statisticians are typically more interested in unbiased estimators and present the $(K-1) \\times p$ parameterization.\n\nIf you'd like to interpret the coefficients, I recommend reading the [Stata page](https://stats.idre.ucla.edu/stata/output/multinomial-logistic-regression-2/), but I won't rehash that here. Instead we'll explore partial dependence plots as a way of understanding the fit model.\n\n## Partial dependence plots\n\nPartial dependence plots are a way to understand the marginal effect of a variable $x_s$ on the response. The gist goes like this:\n\n1. Pick some interesting grid of points in the $x_s$ dimension\n    - Typically the observed values of $x_s$ in the training set\n2. For each point $x$ in the grid:\n    - Replace the $x_s$ with a bunch of repeated $x$s in the training set\n    - Calculate the average response (class probabilities in our case)\n  \nMore formally, suppose that we have a data set $X = [x_s \\, x_c] \\in \\mathbb R^{n \\times p}$ where $x_s$ is a matrix of variables we want to know the partial dependencies for and $x_c$ is a matrix of the remaining predictors. Suppose we estimate some fit $\\hat f$.\n\nThen $\\hat f_s (x)$, the partial dependence of $\\hat f$ *at* $x$ (here $x$ lives in the same space as $x_s$), is defined as:\n\n$$\\hat f_s(x) = {1 \\over n} \\sum_{i=1}^n \\hat f(x, x_{c_i})$$\n\nThis says: hold $x$ constant for the variables of interest and take the average prediction over all other combinations of other variables in the training set. So we need to pick variables of interest, and also to pick a region of the space that $x_s$ lives in that we are interested in. Be careful extrapolating the marginal mean of $f(x)$ outside of this region!\n\nHere's an example implementation in R. We start by fitting a multinomial regression to the `iris` dataset.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nnet)\n\nfit <- multinom(Species ~ ., data, trace = FALSE)\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nmultinom(formula = Species ~ ., data = data, trace = FALSE)\n\nCoefficients:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    18.69037    -5.458424   -8.707401     14.24477   -3.097684\nvirginica    -23.83628    -7.923634  -15.370769     23.65978   15.135301\n\nResidual Deviance: 11.89973 \nAIC: 31.89973 \n```\n:::\n:::\n\nNext we pick the feature we're interested in estimating partial dependencies for:\n\n::: {.cell}\n\n```{.r .cell-code}\nvar <- quo(Sepal.Length)\n```\n:::\n\nNow we can split the dataset into this predictor and other predictors:\n\n::: {.cell}\n\n```{.r .cell-code}\nx_s <- select(data, !!var)   # grid where we want partial dependencies\nx_c <- select(data, -!!var)  # other predictors\n```\n:::\n\nThen we create a dataframe of all combinations of these datasets[^2]:\n\n[^2]: At one point I began wondering how to get a small but representative subset of $x_c$, which lead me down the rabbit hole of sampling from convex sets (for this problem I was imagining using the convex hulls of $x_c$). There's an interesting observation that you can use the Dirichlet distribution for this in an [old R-help thread](https://stat.ethz.ch/pipermail/r-help/2007-March/128299.html). Then I stumbled across [hit and run samplers](http://sci-hub.tw/https://www.tandfonline.com/doi/abs/10.1080/15326349808807500), which are intuitively satisfying, and finally the [`walkr`](https://github.com/andyyao95/walkr) package and the more sophisticated methods it implements. I imagine sampling this is just a hard problem in high dimensions, but if anybody can show me how to convert the convex hull of a dataset calculated using `chull()` into a format suitable for `walkr`, please email me!\n\n::: {.cell}\n\n```{.r .cell-code}\n# if the training dataset is large, use a subsample of x_c instead\ngrid <- crossing(x_s, x_c)\n```\n:::\n\nWe want to know the predictions of $\\hat f$ at each point on this grid. I define a helper in the spirit of `broom::augment()` for this:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\n\naugment.multinom <- function(object, newdata) {\n  newdata <- as_tibble(newdata)\n  class_probs <- predict(object, newdata, type = \"prob\")\n  bind_cols(newdata, as_tibble(class_probs))\n}\n\nau <- augment(fit, grid)\nau\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5,005 × 8\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   setosa versicolor\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>      <dbl>      <dbl>\n 1          4.3         2            3.5         1   versic… 2.15e-11   1.00    \n 2          4.3         2.2          4           1   versic… 9.89e-14   1.00    \n 3          4.3         2.2          4.5         1.5 versic… 4.76e-17   0.127   \n 4          4.3         2.2          5           1.5 virgin… 3.96e-22   0.00131 \n 5          4.3         2.3          1.3         0.3 setosa  9.99e- 1   0.000732\n 6          4.3         2.3          3.3         1   versic… 5.06e- 9   1.00    \n 7          4.3         2.3          4           1.3 versic… 5.98e-13   0.999   \n 8          4.3         2.3          4.4         1.3 versic… 1.94e-15   0.965   \n 9          4.3         2.4          3.3         1   versic… 1.21e- 8   1.00    \n10          4.3         2.4          3.7         1   versic… 4.05e-11   1.00    \n# … with 4,995 more rows, and 1 more variable: virginica <dbl>\n```\n:::\n:::\n    \nNow we have the predictions and we marginalize by taking the average for each point in $x_s$:\n\n::: {.cell}\n\n```{.r .cell-code}\npd <- au %>%\n  gather(class, prob, setosa, versicolor, virginica) %>% \n  group_by(class, !!var) %>%\n  summarize(marginal_prob = mean(prob))\npd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 105 × 3\n# Groups:   class [3]\n   class  Sepal.Length marginal_prob\n   <chr>         <dbl>         <dbl>\n 1 setosa          4.3         0.322\n 2 setosa          4.4         0.322\n 3 setosa          4.5         0.322\n 4 setosa          4.6         0.322\n 5 setosa          4.7         0.322\n 6 setosa          4.8         0.322\n 7 setosa          4.9         0.322\n 8 setosa          5           0.322\n 9 setosa          5.1         0.322\n10 setosa          5.2         0.322\n# … with 95 more rows\n```\n:::\n:::\n\nWe can visualize this as well:\n\n::: {.cell}\n\n```{.r .cell-code}\npd %>%\n  ggplot(aes(!!var, marginal_prob, color = class)) +\n  geom_line(size = 1) +\n  labs(title = paste(\"Partial dependence plot for\", quo_name(var)),\n       y = \"Average class probability across all other predictors\",\n       x = quo_name(var)) +\n  theme_bw() +\n  scale_color_viridis_d()\n```\n\n::: {.cell-output-display}\n![](understanding-multinomial-regression-with-partial-dependence-plots_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\nI won't show it here, but these values agree exactly with the implementation in the `pdp` package, which is a good sanity check on our code.\n\n## Partial dependence plots for all the predictors at once\n\nIn practice it's useful to look at partial dependence plots for all of the predictors at once. We can do this by wrapping the code we've written so far into a helper function and then mapping over all the predictors.\n\n::: {.cell}\n\n```{.r .cell-code}\npartial_dependence <- function(predictor) {\n  \n  var <- ensym(predictor)\n  x_s <- select(data, !!var)\n  x_c <- select(data, -!!var)\n  grid <- crossing(x_s, x_c)\n\n  augment(fit, grid) %>% \n    gather(class, prob, setosa, versicolor, virginica) %>% \n    group_by(class, !!var) %>%\n    summarize(marginal_prob = mean(prob))\n}\n\nall_dependencies <- colnames(iris)[1:4] %>% \n  map_dfr(partial_dependence) %>% \n  gather(feature, feature_value, -class, -marginal_prob) %>% \n  na.omit()\n\nall_dependencies\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 369 × 4\n# Groups:   class [3]\n   class  marginal_prob feature      feature_value\n   <chr>          <dbl> <chr>                <dbl>\n 1 setosa         0.322 Sepal.Length           4.3\n 2 setosa         0.322 Sepal.Length           4.4\n 3 setosa         0.322 Sepal.Length           4.5\n 4 setosa         0.322 Sepal.Length           4.6\n 5 setosa         0.322 Sepal.Length           4.7\n 6 setosa         0.322 Sepal.Length           4.8\n 7 setosa         0.322 Sepal.Length           4.9\n 8 setosa         0.322 Sepal.Length           5  \n 9 setosa         0.322 Sepal.Length           5.1\n10 setosa         0.322 Sepal.Length           5.2\n# … with 359 more rows\n```\n:::\n:::\n\nThen we can plot everything at once!\n\n::: {.cell}\n\n```{.r .cell-code}\nall_dependencies %>% \n  ggplot(aes(feature_value, marginal_prob, color = class)) +\n  geom_line(size = 1) +\n  facet_wrap(vars(feature), scales = \"free_x\") +\n  scale_color_viridis_d() +\n  labs(title = \"Partial dependence plots for all features\",\n       y = \"Marginal probability of class\",\n       x = \"Value of feature\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](understanding-multinomial-regression-with-partial-dependence-plots_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\nHere we see that `Sepal.Length` and `Sepal.Width` don't influence class probabilites that much on average, but that `Petal.Length` and `Petal.Width` do.\n\n## Takeaways\n\nPartial dependence plots are useful tool to understand the marginal behavior of models. The plots are especially helpful when telling a story about what your model means. In this post, I've only worked with continuous predictors, but you can calculate partial dependencies for categorical predictors as well, although you'll probably want to plot them slightly differently. Additionally, it's natural to consider the partial dependencies of a model when $x_s$ is multidimensional, in which case you can visualize marginal response surfaces.\n\nI recommend using the [`pdp`](https://bgreenwell.github.io/pdp/index.html) package to calculate partial dependencies in practice, and refer you to Christoph Molnar's excellect [book on interpretable machine learning](https://christophm.github.io/interpretable-ml-book/pdp.html) for additional reading.",
    "supporting": [
      "understanding-multinomial-regression-with-partial-dependence-plots_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}