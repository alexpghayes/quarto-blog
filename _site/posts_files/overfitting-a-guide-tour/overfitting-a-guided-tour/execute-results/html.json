{
  "hash": "fc484ff6e428fed96df9a785bef4a9fc",
  "result": {
    "markdown": "---\ntitle: \"overfitting: a guided tour\"\nsubtitle: |\n  A little bit more than the standard bias-variance decomposition.\ndate: \"2020-01-06\"\nbibliography: overfitting-a-guided-tour.bib\nexecute:\n  echo: false\n  message: false\n  warning: false\neditor_options: \n  markdown: \n    wrap: 72\n---\n\n## Summary\n\nThis post introduces overfitting, describes how overfitting influences\nboth prediction and inference problems, provides supervised and\nunsupervised examples of overfitting, and presents a fundamental\nrelationship between train and test error. The goal is to provide some\nadditional intuition beyond material covered in introductory machine\nlearning resources.\n\n## Some intuition via a guessing game\n\nBefore we begin, I want to play a guessing game. Here's how it works: I\nshow you two sequences of coin flips. You have to guess which sequence\nis random and which one I made up.\n\n::: {.cell}\n\n:::\n\nOkay, here are the first two sequences. Which one do you think is\nrandom?\n\n    A. THHTTHTTHTHHTHTHHHTT \n    B. HTTHTHTHHHHHHHHHTTHT\n\nLet's play again. Now the sequences are:\n\n    A. HHTHTTHHTTTHHHTTHTTT\n    B. HTHTTHHHTHHHHTTHHHTT\n\nOne last time.\n\n    A. HTTHTTHHHTHTHHTTTTHH\n    B. HHTHTTHHTTTHHTTTHHTH\n\nThe answers are in a footnote[^1]. In any case, there's a simple rule\nyou can use to tell the random sequences from the human generated\nsequences: the random sequences are the sequences with the longest\nsubstring of all heads or all tails.\n\n\n[^1]: The random sequences are B, B, A. First my mom and I made up the\n    three sequences that looked random to us. Then I generated the truly\n    random sequences with the following `R` code:\n\n        set.seed(27)\n\n        get_sequence_sample <- function() {\n          heads_and_tails <- c(\"H\", \"T\")\n          seq <- sample(heads_and_tails, size = 20, replace = TRUE)\n          cat(seq, \"\\n\", sep = \"\")\n        }\n\n        for (i in 1:3) {\n          get_sequence_sample()\n        }\n\n    Thanks to Dave Anderson for performing this demonstration in his\n    probability class.\n\nThe gist is that human intuition is bad at solving this problem. Long\nsequences of all heads or all tails don't *look* random--they appear\noverly structured. But long sequences of heads and tails are in fact\nquite probable under independent coin flipping! This example illustrates\na fundamental fact that human brains often struggle with:\n\n:::{.callout-important}\nRandom processes produce highly structured data.\n:::\n\nUntil we understand this, it's hard to build any intuition for\noverfitting.\n\n## Sources of structure\n\nA major challenge in probability modeling is that there are two sources\nof structure in data:\n\n1.  apparent, happenstance structure that originates in the randomness\n    of the data generating process, and\n\n2.  systematic structure inherent in the data generating process.\n\nWhen we observe structure in data, we don't know where it came from. If\nthe structure comes from randomness in the data generating process, we\nwould like to ignore it. If the structure is the result of some\nfundamental latent characteristic of the phenomena we are studying, we\nwant to study it or leverage it to improve our estimates. When we\nmistakenly confused random, apparent structure for true, systematic\nstructure, we call this mistake **overfitting**.\n\n### Example: polynomial regression\n\nLet's consider a common statistical problem: prediction. Suppose we have\ntwenty pairs of observations $(X_i, Y_i)$, and we believe that a\nGaussian linear model is appropriate. We would like to estimate\n$\\mathbb{E}(Y|X)$, the conditional expectation of $Y$ given $X$.\n\nIf we use an overly flexible estimator that assumes the data has a more\ncomplicated data generating process than it truly does, we can quickly\nrun into overfitting. Consider 20 i.i.d. observations from the simple\nmodel:\n\n$$\n\\begin{align*}  \n  Y_i \\sim 2 - 0.3 \\cdot X_i + \\varepsilon_i \\\\\n  \\varepsilon_i \\sim \\mathrm{Normal}(0, 0.3^2)\n\\end{align*}\n$$\n\nThe simulated data looks like:\n\n::: {.cell}\n::: {.cell-output-display}\n![](overfitting-a-guided-tour_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\nIf we consider a $7^{th}$ degree polynomial fit, our estimate for the\nconditional expectation is shown in maroon below and looks like:\n\n::: {.cell}\n::: {.cell-output-display}\n![](overfitting-a-guided-tour_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\nOur predictions have conformed to random variation in the data rather\nthan systematic variation in the data, and using the polynomial fit for\ninference or prediction is a bad idea. By contrast, a correctly\nspecified linear model does much better.\n\n::: {.cell}\n::: {.cell-output-display}\n![](overfitting-a-guided-tour_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n### Example: model selection amongst polynomial regressions\n\nOverfitting is a major concern especially when we are choosing between\nseveral models. This is because overfit models will look good with\nrespect to some measures of goodness of fit. For example, suppose we\nwant to use the same data from above, and we want to pick a model from\nseveral polynomial regression models with differing degrees. We might\npropose the following estimation procedure:\n\n1.  For $k \\in \\{1, 2, ..., 20\\}$\n    -   Fit a degree $k$ polynomial regression under the assumption that\n        $y = \\beta_0 + \\beta_1 \\cdot x + \\beta_2 \\cdot x^2 + ... + \\beta_k \\cdot x^k + \\varepsilon$\n    -   Record the $R^2$ of the model\n2.  Pick the model that maximizes the $R^2$ of the linear regression\n    model\n\nWe plot $R^2$ versus degree below, and see that we would select a very\nhigh degree, even though the true data generating process has degree\none.\n\n::: {.cell}\n::: {.cell-output-display}\n![](overfitting-a-guided-tour_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\nWe can visualize some of the models, and see that high order polynomials\nfit the data better and better while doing a worse and worse job at\nestimating the systematic structure in the data.\n\n::: {.cell}\n::: {.cell-output-display}\n![](overfitting-a-guided-tour_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\nSo overfitting is a concern when we consider a single model, and also\nwhen we want to compare many different models. Note that we don't have\nto perfectly interpolate the data to overfit! Any of the models with\ndegree larger than one results in a bad estimate of $\\mathbb{E}(Y|X)$.\nTypically, models with more parameters are more flexible and more prone\nto overfitting, but this is not always the case[^2].\n\n[^2]: See @piantadosi_one_2018 for an example of model with a single\n    parameter that can nonetheless perfectly interpolates any finite\n    number of observations.\n\n### Example: estimating cluster memberships\n\nBefore we move on, I want to reiterate my point that overfitting occurs\nin all contexts, not just prediction. Suppose we want to find clusters\nin bivariate data, and we have reason to believe that a Gaussian mixture\nmodel is appropriate. We don't know how many components are in the\nmixture, so we start off by trying a model with five components.\n\nWhen the data has two components, that is, it comes from a simpler data\ngenerating process, our estimator will overfit and make several errors\nat once:\n\n-   It will get the centers of the clusters wrong\n-   It will get the covariances of the clusters wrong\n-   It will assign data points that belong to the same cluster to\n    different clusters\n\nFor a concrete example we simulate some bivariate data with two\nclusters.\n\n::: {.cell}\n::: {.cell-output-display}\n![](overfitting-a-guided-tour_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\nBut if we assume that the data comes from a more flexible model, our\nestimated cluster memberships are wonky and off:\n\n::: {.cell}\n::: {.cell-output-display}\n![](overfitting-a-guided-tour_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n## How to handle overfitting\n\nVery roughly, there are three tricks we can use to mitigate overfitting.\n\n1.  Use domain knowledge to consider appropriate classes of models\n2.  Penalize model complexity\n3.  Sample splitting\n\nFor the rest of this post, I want to present some basic results on\noverfitting in a supervised learning context and to give you some\nintuition on why sample splitting helps.\n\nIf you're interested in penalization approaches, key search phrases are\n\"regularization\", \"degrees of freedom\" and \"bias-variance tradeoff\".\nMany machine learning textbooks describe estimation procedures and model\nselection procedures that use some form of penalization. See\n@belkin_reconciling_2018 for an overview of some interesting recent\ndevelopments concerning model complexity. For a more theoretical\ntreatment of penalization in a model selection context you may enjoy the\nfirst couple chapters of @massart_concentration_2003.\n\n## Overfitting in prediction\n\nIf you come from the machine learning community, you may think\noverfitting is the difference between predictive performance on training\ndata and test data. To use more statistical language, think of a\nflexible machine learning estimator for $\\mathbb{E}(Y|X)$ like a random\nforest. We fit the random forest on training data, which is sampled from\nsome data generating process. We hope the random forest only finds\nsystematic structure in noisy, observed conditionals mean of $Y$ given\n$X$, but it will also conform to random variations in the conditional\nmean. These random variations will not be present in the test set, which\nwill have only the underlying systematic structure plus new random\nvariation. Since random variation in the training set results in a poor\nestimate of the systematic structure in the data generating process, our\noverfit estimate will make mistakes when looking for systematic\nstructure in the test set. Thus overfitting will reduce predictive\nperformance[^3]. This is sometimes described as the random forest\n\"memorizing the training set\".\n\n[^3]: Random variation in the test set will case irreducible error that\n    we cannot make disappear. More often than we would like to admit,\n    the irreducible error is large.\n\nIt turns out that we can study this phenomena more formally. The math is\neasiest in a restricted setting, but the intuition generalizes well.\nFirst, we assume that our data is independent and identically\ndistributed, where\n\n$$\nY_i = f(X_i) + \\varepsilon_i.\n$$\n\nHere $f(X_i)$ describes how $Y_i$ varies systematically with $X_i$,\nwhich we assume is fixed (this is the restrictive assumption).\n$\\varepsilon_i$ represents random error, which we take to be mean zero\nwith variance $\\sigma^2$.\n\nWe consider a training set $X_1, ..., X_n$, and obtain predicted values\n$\\widehat Y_1, ..., \\widehat Y_n$. Then we consider a test set,\n*observed at the same values* $X_i$, but with new random errors\n$\\varepsilon_i^*$. So our test set is the set of observations\n\n$$\nY_i^* = f(X_i) + \\varepsilon_i^*.\n$$\n\nLet our predicted values at $X_i$, obtained using only the training set,\nbe $\\widehat Y_i = \\hat f (X_i)$, where $\\hat f$ represents our\nestimator for the conditional mean (i.e. predictive model). Note that\nthese are not just the predicted values for the training set, but *also\nthe predicted values for the test set, since* $X_i$ is fixed.\n\nUnder $\\ell_2$ loss, the training error is\n\n$$\n\\frac 1n \\sum_{i=1}^n (Y_i - \\widehat Y_i)^2\n$$\n\nand the test error is\n\n$$\n\\frac 1n \\sum_{i=1}^n (Y_i^* - \\widehat Y_i)^2.\n$$\n\nWe know that the training error should be less than the test error, and\nwe can in fact formalize the relationship between these two measures. In\nparticular, we have:\n\n\n$$\n\\begin{align}\n  \\underbrace{\n    \\mathbb{E} \\left[ \\frac 1n \\sum_{i=1}^n \\left( Y_i^* - \\widehat Y_i \\right)^2 \\right]\n  }_\\text{test error}\n  = \n  \\underbrace{\n    \\mathbb{E} \\left[ \\frac 1n \\sum_{i=1}^n \\left( Y_i - \\hat Y_i \\right)^2 \\right]\n  }_\\text{training error} +\n  \\underbrace{\n    \\frac 2n \\cdot \\sum_{i=1}^n \\mathrm{Cov} \\left( \\widehat Y_i, Y_i \\right) \n  }_\\text{optimism}\n\\end{align}\n$$\nThis relationship holds for most important loss functions. It means\ntells us that test error, or generalization error, is almost always\nhigher than in-sample error evaluated on the training set. We call the\namount by which the training error underestimates the test error the\noptimism; the more optimism, the greater the discrepancy between\nin-sample and out-of-sample error.\n\nWe'll discuss the implications the train-test error relationship more in\na moment, but first let's prove it. Feel free to skip the proof,\nalthough it relies only on basic properties of the expectation and\nvariance.\n\n**Proof**. We'll follow the proof in [these course\nnotes](http://www.stat.cmu.edu/~larry/=stat401/lecture-21.pdf) by Larry\nWasserman. Consider the $i^{th}$ observation. Then\n\n$$\n\\begin{align}\n  \\underbrace{\n    \\mathbb{E} \\left[ \\left( Y_i - \\hat Y_i \\right)^2 \\right]\n  }_\\text{training error at $X_i$} \n&= \\mathrm{Var} \\left[ Y_i - \\widehat Y_i \\right] \n  + \\left( \\mathbb{E} \\left[ Y_i - \\widehat Y_i \\right] \\right)^2 \\\\\n&= \\mathrm{Var} \\left[ Y_i \\right] \n  + \\mathrm{Var} \\left[ \\widehat Y_i \\right]\n  - 2 \\, \\mathrm{Cov} \\left[ Y_i, \\widehat Y_i \\right]\n  + \\left(\n    \\mathbb{E} \\left[ Y_i \\right]\n    - \\mathbb{E} \\left[\\widehat Y_i \\right]\n  \\right)^2\n\\end{align} \n$$\n\nand also\n\n$$\n\\begin{align}\n  \\underbrace{\n    \\mathbb{E} \\left[ \\left( Y_i^* - \\widehat Y_i \\right)^2 \\right]\n  }_\\text{test error at $X_i$}\n&= \\mathrm{Var} \\left[ Y_i^* - \\widehat Y_i \\right] \n  + \\left( \\mathbb{E} \\left[ Y_i^* - \\widehat Y_i \\right] \\right)^2 \\\\\n&= \\mathrm{Var} \\left[ Y_i^* \\right] \n  + \\mathrm{Var} \\left[ \\widehat Y_i \\right]\n  - 2 \\, \\mathrm{Cov} \\left[ Y_i^*, \\widehat Y_i \\right]\n  + \\left(\n    \\mathbb{E} \\left[ Y_i^* \\right]\n    - \\mathbb{E} \\left[\\widehat Y_i \\right]\n  \\right)^2.\n\\end{align} \n$$\n\nNow we consider several implications of the fact that $Y_i$ and $Y_i^*$\nare independent and identically distributed. In particular, we have\n$\\mathbb{E}(Y_i) = \\mathbb{E}(Y_i^*)$,\n$\\mathrm{Var}(Y_i) = \\mathrm{Var}(Y_i^*)$, and most importantly\n$\\mathrm{Cov} \\left[ Y_i^*, \\widehat Y_i \\right] = \\mathrm{Cov} \\left[ f(X_i) + \\varepsilon_i^*, \\hat f(X_i) \\right] = \\mathrm{Cov} \\left[ \\varepsilon_i^*, \\hat f(X_i) \\right] = 0$.\nThus we see\n\n$$\n\\begin{align}\n  \\underbrace{\n    \\mathbb{E} \\left[ \\left( Y_i^* - \\widehat Y_i \\right)^2 \\right]\n  }_\\text{test error at $X_i$}\n  &= \\mathrm{Var} \\left[ Y_i \\right] \n    + \\mathrm{Var} \\left[ \\widehat Y_i \\right]\n    + \\left(\n      \\mathbb{E} \\left[ Y_i^* \\right]   \n      - \\mathbb{E} \\left[\\widehat Y_i \\right]\n    \\right)^2 \\\\\n  &= \n    \\underbrace{\n      \\mathbb{E} \\left[ \\left( Y_i - \\widehat Y_i \\right)^2 \\right]\n    }_\\text{training error at $X_i$} \n    + 2 \\, \\underbrace{\n      \\mathrm{Cov} \\left[ Y_i, \\widehat Y_i \\right]\n    }_\\text{how much $\\hat f$ memorized $Y_i$}\n\\end{align}\n$$\n\nwhere in the last equality we substitute based on our previous\ndecomposition of the training error. Summing over all $i$ and dividing\nby $n$ **finishes the proof**.\n\nLet's consider the implications of the train-test error relationship in\ntwo extreme cases. First, suppose that the $\\widehat Y_i = Y_i$. This\nmeans that our estimator $\\hat f$ has perfectly memorized the training\nset. In this case, there is zero training error, but the optimism is\n$2 \\sigma^2$, which is pretty much the worst possible case amongst\nreasonable estimators (in the fixed $X_i$ setting).\n\nIn the flip case, the estimator doesn't memorize the training set at\nall, so there is no dependence (and thereby no covariance) between the\npredictions $\\widehat Y_i$ and the training labels $Y_i$ (really, the\nerrors $\\varepsilon_i$, since that's the only random component of\n$Y_i$). This means the estimator $\\hat f$ has ignored the random errors\n$\\varepsilon_i$ and has learned only generalizable knowledge!\n\nIn fact, it's often useful to treat\n\n\\[ \\frac{1}{\\sigma^2} \\sum\\_{i=1}\\^n \\mathrm{Cov}\n\\left[ \\widehat Y_i, Y_i \\right]\\]\n\nas a generalized notion of \"effective number of parameters\" or \"degrees\nof freedom\" that measures the complexity of a predictive estimator and\nits capacity to memorize the training set[^4].\n\n[^4]: There is also the bizarre scenario when an estimator produces\n    predictions that are anticorrelated with the errors $\\varepsilon_i$,\n    so that $\\mathrm{Cov} \\left[ \\widehat Y_i, Y_i \\right]$ is negative\n    and test set performance is *better* than training set performance.\n    If you observe this phenomena on an applied problem, I recommend\n    buying a lottery ticket.\n\nFinally, this theorem suggests why sample splitting can give us good\nestimates of test error. If we have an independent dataset, and we\nassess the performance of the estimator $\\hat f$ on the independent\ndataset, the predictions on this dataset will be independent from the\ntraining data, the covariance will be zero, and the optimism term will\ndisappear. Thus we get an unbiased estimate of the loss on new data.\nThis is sometimes called \"unbiased risk estimation\"[^5].\n\n[^5]: Alternatively, if we know the form of $\\hat f$ and $f$ we could\n    estimate the optimism directly and subtract it off. This is one way\n    to motivate AIC, for example. As a general rule, estimating the\n    optimism and subtracting it off requires much stronger assumptions\n    than cross-validation that are unlikely to hold in practice.\n\nCross validation now is a natural generalization of our hold out\nestimator. We have an unbiased estimate, so all the error in our\nestimate of the generalization error comes from variance. If we can\ngenerate $k$ independent-ish estimates of generalization error and\naverage them, then we will reduce the variance in the risk estimate.\nIt's natural to partition the data into $k$ non-overlapping sets, fit a\nmodel $\\hat f_j$ on all but the $j^{th}$ partition, and estimate the\nvalidation error on the unseen $j^{th}$ portion of the data. In terms of\nintuition, we then average these \"unbiased\" and \"independent\" estimates\nget a new \"unbiased estimate with smaller variance\". In practice, the\nsituation is more complicated, but cross-validation nonetheless turns\nout to be a good idea[^6].\n\n[^6]: The risk estimates across the $k$ folds are both biased and\n    dependent. If you fit a model on 80 percent of the full training\n    data, you don't get an unbiased estimate of the performance when the\n    model is fit on the full data. The bias is simply because more data\n    results in a better estimates. Second, the held-out risk estimates\n    are not independent because the $\\hat f_j$ and $\\hat f_k$ are\n    largely trained on the same data. But averaging them works out okay\n    [@bengio_no_2004] for risk estimation, and accounting for blocking\n    across the folds with something like a random intercept model works\n    well for model selection [@wager_cross-validation_2019].\n\n## Pulling it all together\n\nData has structure. Some of this structure is systematic, and some is\nrandom. We care about the systematic structure, but we don't want to\nconfuse the random structure for systematic structure.\n\nOnce we start estimating things, we need to be careful about how\nflexible we allow our models to be. This is true for both inferential\nand predictive modeling. If we allow our models to be more flexible than\nthe true data generating process, we will mistake random structure for\nsystematic structure. On the flip side, if we use models that don't\ncontain the true data generating process, we won't capture all the\nsystematic structure in the data (and we can also confuse the systematic\nand random structure).\n\nIn the prediction context, we saw that test error is higher than\ntraining error due to covariance between predictions and random errors\nin the training set. Hopefully this example demonstrates how easy it is\nto confuse systematic error and random error, and provides some\nintuition that you can use when analyzing data down the line.\n\n## Further reading\n\nWhen $X_i$ is random, the intuition about the train-test performance\nrelationship is pretty much the same as for the fixed $X_i$ case, but\nthe randomness in $X_i$ contributes some additional terms to the test\nset error. @rosset_fixed-x_2017 discusses these additional terms.\n\nNote that getting good estimates of risk is a key element of model\nselection, but that cross validation is not a silver bullet. For example\n@shao_linear_1993 proves the cross validation isn't consistent for\nselecting amongst linear models, and @benavoli_time_2017 and\n@lei_cross-validation_2017 discuss the need for additional modeling of\ncross-validated risk estimates to find the most predictive model.\nSection 2 of @lei_cross-validation_2017 is an especially eloquent\nintroduction to cross-validation[^7]. @roberts_cross-validation_2017 is\na nice introduction to specialized types of cross validation that\nrespect independence structures found in spatial, temporal and network\ndata. @vehtari_practical_2017 presents modern Bayesian approaches to\nsample splitting for model selection, and [this blog\npost](https://statmodeling.stat.columbia.edu/2018/06/21/i-am-the-supercargo/)\nby Dan Simpson discusses the same work for non-i.i.d. data.\n\n[^7]: I highly recommend reading anything by Jing Lei.\n\nChapter 7 of @hastie_elements_2008 presents an overview of\ncross-validation in the machine learning context. @arlot_survey_2010 is\na comprehensive review of results in the cross-validation literature, in\nparticular discussing how cross-validation procedures different when the\ngoal is risk estimation (i.e. determining predictive performance) versus\nmodel selection (i.e. choosing hyperparameters).\n@dudoit_asymptotics_2005 proves an oracle efficiency result about model\nselection with cross-validation. Finally, cross-validation was\nintroduced in the now classic @stone_cross-validatory_1974.\n\n### Acknowledgements\n\nI'd like to thank [Julia Silge](https://juliasilge.com/) for providing\nfeedback on a draft of this post!",
    "supporting": [
      "overfitting-a-guided-tour_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}