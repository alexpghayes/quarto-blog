---
title: "what i want from academic code"
subtitle: |
  the purpose, audience, and value of methods software
date: "2024-11-14"
categories: [statistical software]
bibliography: references.bib
---

It is a bit of meme that academic software is disappointing, because it either fails to solve applied problems or behaves unreliably in production. This is not the right goal for academic code, primarily because it is unrealistic and somewhat at odds with academic incentive structures. Rather, academic code might function best as a medium for precise communication, as a baseline for correctness, to demonstrate proof-of-concept, or as hacky tool to solve problems intermittently and semi-reliably. I think that these alternative goals are valuable, and there are incentive-compatible reasons for various academic groups to produce code for these alternative ends. 

Understanding why these alternative goals are incentive-compatible for academics requires us to consider stakeholders of methodological software. I think the key stakeholders are (1) theorists who develop methods but only intermittently use them, (2) practitioners who constantly use methods in applied work, and (3) translational workers who take theoretical work and turn it into pragmatic tooling for practitioners. Incentives are different for each of these groups, and I argue that theorists, applied folks, and translators should all write very different kinds of code. I propose a substantial re-orientation for theorists and methodologists in particular: rather than trying and failing to write user facing code, these folks should focus on the less stressful and more achievable task of writing psuedo-code, reference implementations and careful documentation that software engineers can then later use as the basis for user facing software. 

This re-assignment of software responsibilities relaxes the pressure on methods producers to be perfect unicorn researcher-programmers. It acknowledges that software engineering labor is distinct from research labor, despite the fact that software is often a form of research infrastructure. It acknowledges that academics are largely paid to produce research rather than code, and also that academics are typically much better at producing research. It also requires some humility from academics, to accept that large portions of academic code are not particularly valuable, and that is isn't worthwhile to write code intended for other people if they can't actually use it.

I think that a re-assessment of software responsibilities might enable new opportunities for producing high-quality, user-facing software. Applied academics could make smaller, more reasonable requests of their theoretical colleagues. Academic departments could hire research software engineers, and allocate their resources in ways that reflect their relative interest in research products and research infrastructure. And the folks who eventually do translate methods into software could start with a useful leg up.

### A typology of methods software

My (normative) claim is there are a couple different kinds of valuable software contributions:

1. Pseudo-code
2. Reference implementations
3. Proofs-of-concept
4. Hobbyist code (please help me come up with a better name!)
5. Production code

Pseudo-code, reference implementations and proofs-of-concept are not user-facing, but hobbyist code and production code are. In more detail:

1. **Pseudo-code**. Pseudo-code describes an algorithm in formal and unambiguous language. The purpose of psuedo-code is to communicate and help others understand what an algorithm does. Pseudo-code is useful to methods producers who want to study theoretical properties of an algorithm, and also to software engineers or translational workers who want to actually implement a method. Pseudo-code is not runnable code. 

    An example of nice pseudo-code is given in @allen2019b:

    ![](sfpca-alg.png){fig-alt="Psuedo-code for the SFPCA algorithm." width=70% fig-align="center"}

    This pseudo-code gives a clear conceptual overview of the algorithm, and most of the information needed for implementation. In practice, there are a number of minor implementation details that do not appear here: how to compute the singular values, the implementation of the proximal operator, and how to test for convergence. This small amount of remaining ambiguity is exactly why it's valuable to complement the pseudo-code with a reference implementation.

<!-- ![](fastrg.png){fig-alt="Psuedo-code for the SFPCA algorithm." width=70% fig-align="center"} -->

2. **Reference implementation**. A reference implementation is a basic translation of pseudo-code into runnable code. The purpose of a reference implementation is to generate results on toy problems that can be used to test future implementations of an algorithm. Like pseudo-code, reference implementations should to be exceptionally readable. It is best if reference implementations run in a limited number of concrete settings and avoid complicated language features. Both the original author of a reference implementation and third parties should feel very confident about correctness. The notation in a reference implementation should also closely match the notation in the paper or document that proposes the algorithm. Inputs and outputs should be carefully documented, and tests on some key examples are valuable. However, reference implementations primarily designed to be read rather than run. Ideally, reference implementations are written in freely available languages.

    The SFPCA algorithm above was [originally accompanied](https://web.archive.org/web/20170803083332/http://www.stat.rice.edu/~gallen/software/sfpca/sfpca_fixed.m) by the following MATLAB reference implementation:

    ![](sfpca-reference.png){fig-alt="Screenshot of the SFPCA reference implementation."}

    This reference implementation has all the computational details you need to implement SFPCA. I know this because I ended up [implementing](https://github.com/alexpghayes/gsoc_moma_application) SFPCA in both R and C++ and [testing against]((https://github.com/alexpghayes/gsoc_moma_application/blob/master/tests/testthat/test-sfpca.R)) the MATLAB above.

3. **Proof-of-concept**. Proof-of-concept implementations are designed to run on research problems. They include features that researchers need to get their research done. Proof-of-concept implementations do not need to be fully featured or well-designed or computationally efficient. They should typically be released once and un-maintained. The code should primarily be useful for *other researchers* who also want to use the code *for research*. Proof-of-concept implementations can take a substantial amount of development labor, but this labor is unlikely to benefit anyone beyond the original authors of the scripts. Sharing proof-of-concept implementations is important for reproducibility purposes, but a reference implementation (easier to understand, at less risk of bit rot) or hobbyist implementation (actively maintained, documented, user-friendly) is a more meaningful contribution to the community at large. While proof-of-concept implementations enable future research, that future research is typically quite painful^[Proof-of-concept code, in case it isn't clear, is the minimum viable product need to successfully author an academic manuscript.].

    My [CitationImpute](https://github.com/RoheLab/fastadi/blob/main/R/citation-impute2.R) code is an example of proof-of-concept code. That worked enough for me to publish @hayes2024b. Some of the code and documentation is even pretty nice, but the point is that this code is primarily for myself. It's not particularly ready for someone else to use. It breaks in unexpected and barely documented ways: don't, for instance, pass in any missing data. There are actually two separate implementations of the underlying algorithm, with no explanation for why this is the case. The other day someone asked for a demonstration of how to actually use the code, and it took me four or five hours to write a (still very terse) [vignette](https://gist.github.com/alexpghayes/9ab8c37e75e5d91932e79037ba108371). I would not put this code in production. Do I trust the code and the results in my paper? Absolutely, I wrote a lot of correctness tests and simulated to show that estimation error goes down! Do I think that *someone else* could use this code to do something useful? Maybe, but I think it's likely that they'd have a bad time. Another example of a proof-of-concept implementation of `AdaptiveImpute` is [here](https://github.com/chojuhee/hello-world). This codes works and was enough to publish a paper, but computationally, there are a lot of unexplained things happening and there's a small bug or two!

4. **Hobbyist software**. Hobbyist software, for lack of a better name, is designed for methods consumers. The goal is to solve practical but technically approachable problems. Hobbyist code should be usable, well-documented, tested and released in an easily installable form. The authors should provide at least some ongoing maintenance for the code. They should also make a genuine effort to disseminate the software by advertising it and providing tutorials on its use. The internal structure of the code does not have to be perfect, but it needs to be understandable enough to have moderate faith in its correctness. Hobbyist software doesn't necessarily need to have the reliability of software for a big production system, but it should solve most problems that an applied researcher might tackle on a personal computer. Note that hobbyist software takes a substantial amount of labor to develop, and the primary benefit is to the community of methods consumers. 

    I consider my [`fastRG`](https://github.com/RoheLab/fastRG) package to be hobbyist level code. It's documented, somewhat tested, and I use it myself on a frequent basis. I also know that other people use the package on a frequent basis, and presumably they would not if it were unbearably buggy or dysfunctional. Most importantly, in contrast to `fastadi::citation_impute()`, I feel *good* about people using `fastRG`. 

5. **Production code**. Lastly, there is software that is ready for production, by which I mean that is reliable, feature-complete and scalable to technically challenging settings. These implementations should follow software engineering best practices, use good defaults, and have responsive maintenance teams, as well as a semi-frequent release cycle and extensive documentation. The primary differences between a hobbyist implementation and a production implementation are the internal design of the software, increased UX polish, and thorough testing. Production implementations should handle errors and edge cases gracefully and produce results that are reliably correct. The purpose of production code is to enable developers to build large and automated systems. Due to system and scaling constraints, production implementations often look very different from pseudo-code. Documentation should clearly describe how and why the implementation deviates from the pseudo-code.

    Some examples of production-grade code include packages in the [tidyverse](https://www.tidyverse.org/), [Stan](https://mc-stan.org/), and [Jax](https://jax.readthedocs.io/en/latest/index.html).

### A modest proposal

My first proposal is that academics writing methodological code should state the **goals** and the **audience** of their code. One way to do this would be with Github badges. For instance:

- `purpose: reference-implementation`,
- `purpose: proof-of-concept-implementation`,
- `purpose: reliable-on-small-to-moderate-problems`, or
- `purpose: production-ready`.

Or potentially:

- `audience: theorists doing methodological research`,
- `audience: software engineers who want to make this useful to the broader community`, and
- `audience: applied scientists`.

I think the language here needs a moderate amount of work, but by adding these labels to the software we produce, we can then evaluate code by asking if it achieves it's stated goal, rather than some other goal that someone else made up.

### What I want from methods producers

I want to methods producers to:

- share pseudo-code for their methods,
- share a proof-of-concept implementation that is sufficient to reproduce computational aspects of their work, and
- share a reference implementation  *distinct* from the proof-of-concept code, which is exceptionally clear and readable.

The goal of these outputs is to facilitate communication, understanding, and future software development. I consider these products to be an essential part of research. In particular, I personally think of these code products as a minimum standard, and I don't assign any intellectual credit for achieving this basic goal. If an academic researcher doesn't share these outputs, it actively detracts from capacity to continue the research enterprise and methods dissemination efforts, and I consider the intellectual contribution incomplete.

In contrast, I don't think that it is a good use of method-producer time to write hobbyist or production-ready code. Especially in academia, methods producers are not incentivized to make tools for methods consumers. Instead, methods-producers should set realistic expectations about the goal of their code, and we should evaluate them based on whether or not they achieve those goals. If methods producers go above and beyond and do in fact create hobbyist or production grade code, we should clearly acknowledge this as a meaningful contribution. However, this contribution is a software development contribution rather than a research contribution.

I want to free method producers from unreasonable expectations, which are sometimes explicit and sometimes implicit. There are many genuinely constructive attempts to improve the quality of statistical software^[See, for example, the ROpenSci [stats software review](https://stats-devguide.ropensci.org/) and [tidymodels implementation principles](https://github.com/tidymodels/model-implementation-principles) projects, both of which I was briefly involved in, as well as a plethora of academic papers such as @lee2021a, @peer2020, @taschuk2017, and @wilson2017. Beyond that, there's  software checklists and standards for journals such as the [Journal of Open Source Software](https://joss.theoj.org/), the [Journal of Statistical Software](https://www.jstatsoft.org/index), the [R Journal](https://journal.r-project.org/), etc, etc]. To my eye, the aspirational goal of these attempts is often to get people to write production-ready code. I think it's perfectly okay not to do this, especially if you set expectations about what you actually want your code to do^[My attitude on this has shifted substantially over the last several years, as I have started to produce methods. When I spent more time in open-source and primarily acted as a methods consumer, I used to be frustrated by papers that didn't come accompanied by production-ready code. In contrast, these days I find myself frustrated by papers without easy-to-read reference implementations.]. 

On the other hand, I want to clarify that not all code is created equal, and I am not actually giving most academics additional credit for the code that they release. To be exceptionally clear on this last point: if you share a barely-legible, thousand-line spaghetti script, you don't get to pretend you've done anything meaningful to disseminate your method. You have simply completed the research, and have done so in a way that will greatly frustrate future researchers.

### Who should write user-facing software?

In short, paid professionals. I think that people who want a nice implementation of method should pay for the nice implementation of that method. This can take a number of different forms:

- industry paying software engineers to produce in-house or open-source implementations of methods,^[I find it somewhat entertaining when a software engineer at FAANG, paid a quarter of a million dollars per year to write code, is frustrated that an academic has not implemented code for them already. That's where your job security comes from! Instead of pushing the academic to write code for you, ask the academic to share material that makes it easier for you to write the code yourself.]
- academic departments hiring research software engineers,
- philanthropic efforts subsidizing open-source work, and
- applied researchers with software chops, implementing methods to facilitate their own research.

This view is somewhat less expansive than other visions of methodological dissemination. My opinion is that not every method is important or useful enough to justify a high quality implementation, and we mainly want quality software for methods that are obviously important and get used a lot. I think this follows naturally from a desire to use resources in an efficient manner, but also because software that people use is better software, and software that people don't use is subject to bugs and bit-rot and death.

You'll note that academics are not really on this list, mostly because I think it's incentive-incompatible for most academics to write software for the masses. I don't think this is uniformly the case, but as a general rule I think it holds. The issue is two-fold: first, academics are ill-prepared to write software for methods consumers. Writing code for methods consumers is tough. First of all, it takes a lot of software engineering labor. But beyond this, once you start writing hobbyist or production grade code, you start getting evaluated by two different sets of standards at the same time. Methods producers evaluate the technical and theoretical quality of your work. But methods consumers evaluate the software design and usability of your software. This is rough, and also as it should be: methodological software needs to both correct and usable. A lot of existing software clears only one of these bars.

The second reason why I don't think that academics should write user-facing methods software is that the credit that academics get for code is inconsistent and somewhat volatile and a rough thing to stake a tenure case on, depending on your precise discipline and department. If you write research software, not only are the methods producers and the methods consumers both holding your work to high standards, neither is giving you research credit for your work: good software is neither a novel method, nor is it an intriguing piece of empirical work. Translating methods into software enables research, but it is rarely research itself. I think it's pretty fair for tenure committees to take any number of opinions on how to value research proper versus research infrastructure. Research infrastructure definitely is something valuable, but exactly how valuable it is relative to research is ambiguous to me.

The best way to get academic credit for writing software, I think, is to get authorship on the original methods paper. Then both the paper and the software accumulate citations (the software almost always get cited orders of magnitude more often than the content of the paper does) and this is good for everybody involved. The trick is timing: the software has to come out at the same time as the paper. This is a pretty rare occurrence, because lots of software is developed aftermarket of the original methods paper. And there's a very weird dynamic here, because software engineering labor is much less specialized than methodological labor, but being able to *read* methods papers is still a pretty specialized skill, to the extent that it's quite hard to find folks who both have software engineering chops are also able to translate methods work into R and Python packages. One idea that has become somewhat popular in recent years is to cite software packages in academic papers. I am somewhat ambivalent about this. It seems like a fine thing to do, but I'm not sure how much it actually helps people get tenure. I guess do it in the hopes that eventually people do get credit for citations to their software packages.

On the whole, I don't anticipate a large influx of translational labor in the near future. I also don't anticipate that methods will disseminate quickly and effectively without this labor, because methods dissemination requires user-facing software. I'm at peace with this, since academic compensation for user-friendly software is weird and often inadequate, and I think the current state of affairs is actual pretty reasonable for a certain priorization of research relative to research infrastructure. It's very possible this leaves us with an infrastructure problem, where people want to use methods but the software doesn't exist. My argument: if we don't have the capacity to write production-ready software ourselves, the next-best attainable thing is to create material that helps other people write production-ready software in our stead.

#### Acknowledgements

My thoughts have been heavily influenced by conversations with [Ben Listyg](https://github.com/benlistyg), who also provided extensive feedback on this post.
