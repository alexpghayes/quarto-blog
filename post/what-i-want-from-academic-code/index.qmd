---
title: "what i want from academic code"
subtitle: |
  the purpose, audience, and value of methods software
date: "2024-10-24"
categories: [statistical software]
bibliography: references.bib
draft: true
---

### Summary

There are many valuable reasons to write code beyond creating large, automated systems. Code is also valuable as a medium for precise communication, as a baseline for correctness, to demonstrate proof-of-concept, and as a tool to use intermittently and inefficiently. 

Researchers in particular will often value these alternative goals more than developing large, automated systems. By differentiating between various kinds of code contributions, we may be able to better incentive narrow, purpose-specific academic software. Further, we may make it easier to eventually productionize code originally written for a different purpose.

### A normative typology of software, goals, and audiences

To my eye, there are roughly three audiences for statistical software:

1. methods producers,
2. methods consumers, and
3. methods translators.

Methods producers tend to be researchers, applied or theoretical, who develop solutions to open problems. Methods consumers have some problem, and that problem is solved already, and they would like the solution. Lastly, methods translators are tool builders and tutorial writers. They are interested in helping methods consumers get things done, and they understand that methods consumers may not have the time, inclination or background to implement solutions in code. Methods producers are rarely methods consumers themselves; methods translators are often methods consumers who want their daily workflows to be more ergonomic.

Each of these groups has different software needs. With this in mind, here are five distinct kinds of software products, each of which serves some groups but not others.

<!-- the type, the goal of the type, key features, things the contribution does not need to do, what kind of labor it takes to make the thing, the audience, maintained -->

1. **Pseudo-code**. Pseudo-code describes an algorithm in formal and unambiguous language. The purpose of psuedo-code is to communicate and help others understand what an algorithm does. Pseudo-code is useful to methods producers who want to study theoretical properties of an algorithm, and also to methods translators who want to implement an algorithm. Pseudo-code does not need to be runnable software. Producing pseudo-code primarily requires theoretical labor.

2. **Reference implementation**. A reference implementation is a basic translation of pseudo-code into runnable code. The purpose of a reference implementation is to generate results on toy problems that can be used to test future implementations of an algorithm. Like pseudo-code, reference implementations should to be exceptionally readable. It is best if reference implementations run in a limited number of concrete settings and avoid complicated language features. Both the original author of a reference implementation and third parties should feel very confident about correctness. The notation in a reference implementation should also closely match the notation in the paper or document that proposes the algorithm. Inputs and outputs should be carefully documented, and tests on some key examples are valuable. Producing reference implementations requires software development labor. This labor may not feel particularly worthwhile, because maximizing readable of the code often comes at the cost of user friendliness. However, reference implementations primarily designed to be read rather than run.

3. **Proof-of-concept**. Proof-of-concept implementations are designed to run on research problems. They include features that researchers need to get their research done. Proof-of-concept implementations do not need to be fully featured or well-designed or computationally efficient. They should typically be released once and un-maintained. The code should primarily be useful for *other researchers* who also want to use the code *for research*. Proof-of-concept implementations can take a substantial amount of development labor, but this labor is unlikely to benefit anyone beyond the original authors of the scripts. Sharing proof-of-concept implementations is important for reproducibility purposes, but a reference implementation (easier to understand, at less risk of bit rot) or hobbyist implementation (actively maintained, documented, user-friendly) is a more meaningful contribution to the community at large. While proof-of-concept implementations enable future research, that future research is typically quite painful^[Proof-of-concept code, in case it isn't clear, is the minimum viable product need to successfully author an academic manuscript.].

4. **Hobbyist software**. Hobbyist software, for lack of a better name, is designed for methods consumers. The goal is to solve practical but technically approachable problems. Hobbyist code should be usable, well-documented, tested and released in an easily installable form. The authors should provide at least some ongoing maintenance for the code. They should also make a genuine effort to disseminate the software by advertising it and providing tutorials on its use. The internal structure of the code does not have to be perfect, but it needs to be understandable enough to have moderate faith in its correctness. Hobbyist software doesn't necessarily need to have the reliability of software for a big production system, but it should solve most problems that an applied researcher might tackle on a personal computer. Note that hobbyist software takes a substantial amount of labor to develop, and the primary benefit is to the community of methods consumers. 

5. **Production code**. Lastly, there is software that is ready for production, by which I mean that is reliable, feature-complete and scalable to technically challenging settings. These implementations should follow software engineering best practices, use good defaults, and have responsive maintenance teams, as well as a semi-frequent release cycle and extensive documentation. The primary differences between a hobbyist implementation and a production implementation are the internal design of the software, increased UX polish, and thorough testing. Production implementations should handle errors and edge cases gracefully and produce results that are reliably correct. The purpose of production code is to enable developers to build large and automated systems. Production implementations often look very different from pseudo-code, due to system and scaling constraints. Documentation should clearly describe how and why the implementation deviates from the pseudo-code.

I believe that each of these different types of software represents a valuable and worthwhile code contribution.

::: {.callout-tip icon=false}
# Proposal

Software developers should state the **purpose** and **audience** of their code.

I rather like the idea of Github badges:

- `goal: reference-implementation`,
- `goal: proof-of-concept`,
- `goal: hobbyist-ready`, and 
- `goal: production-ready`.

Also Github badges:

- `audience: methods researchers`,
- `audience: methods translators`, and
- `audience: methods consumers`.
:::

We can then evaluate code by asking if it achieves it's stated goal, rather than some other goal that we made up ourselves.

### Academic credit for methods producers

I want to methods producers to:

- share pseudo-code for their methods,
- share a reference implementation, and
- share a proof-of-concept implementation, *distinct* from the reference implementation, that is enough to replicate any computational experiments they run.

The goal of these outputs is to facilitate communication, understanding, and future software development. I consider these products to be an essential part of research. In particular, I personally think of these code products as a minimum standard, and I don't assign any intellectual credit for achieving this basic goal. If an academic researcher doesn't share these outputs, it actively detracts from capacity to continue the research enterprise and methods dissemination effort, and I consider the intellectual contribution incomplete.

In contrast, I don't think that it is a good use of method-producer time to write hobbyist or production-ready code. Especially in academia, methods producers are not incentivized to make tools for methods consumers, and so there is little reason to pressure to method producers to be unicorn programmer-researchers. Instead, methods-producers should set realistic expectations about the goal of their code, and we should evaluate them based on whether or not they achieve those goals. If methods producers go above and beyond and do in fact create hobbyist or production grade code, we should clearly acknowledge this as a meaningful contribution. However, this contribution is a software development contribution rather than a research contribution.

On one hand, I want to free method producers from unreasonable expectations, which are sometimes explicit and sometimes implicit. There are many genuinely constructive attempts to improve the quality of statistical software^[See, for example, the ROpenSci [stats software review](https://stats-devguide.ropensci.org/) and [tidymodels implementation principles](https://github.com/tidymodels/model-implementation-principles) projects, both of which I was briefly involved in, as well as a plethora of academic papers such as @lee_barely_2021, @peer_active_2020, @taschuk_ten_2017, and @wilson_good_2017. Beyond that, there's  software checklists and standards for journals such as the [Journal of Open Source Software](https://joss.theoj.org/), the [Journal of Statistical Software](https://www.jstatsoft.org/index), the [R Journal](https://journal.r-project.org/), etc, etc]. To my eye, the aspirational goal of these attempts is often to get people to write production-ready code. I think it's perfectly okay not to do this, especially if you set expectations about what you actually want your code to do^[My attitude on this has shifted substantially over the last several years, as I have started to produce methods. When I spent more time in open-source and primarily acted as a methods consumer, I used to be frustrated by papers that didn't come accompanied by production-ready code. In contrast, these days I find myself frustrated by papers without easy-to-read reference implementations.]. 

On the other hand, I want to clarify that not all code is created equal, and I am not actually giving most academics any additional credit for the code that they release. To be exceptionally clear on this last point: if you share barely-legible, thousand-line spaghetti script, you don't get to pretend you made a useful tool for other people, and you don't get to pretend you've done anything meaningful to disseminate your method. You have simply completed the research, and have done so in a way that will frustrate future researchers. It really still is good to learn some programming so that others can more easily build on your work.

### Some expectations for methods consumers

I want method consumers to:

- pay software developers and methods translators to turn research products into tools that research and open source communities can use,
- understand a lot of software is not intended for methods consumers, as odd as that may sound,^[I find it somewhat entertaining when a software engineering at FAANG, paid a quarter of a million dollars per year to write code, is frustrated that an academic has not implemented code for them already. That's where your job security comes from! Instead of pushing the academic to write code for you, push the academic to share material that makes it easier for you to write the code yourself.] and
- if they can, learn the technical and software engineering skills to write the software that you want to use. The best software is often software that is frequently used by the people who wrote it.

I have become somewhat ambivalent about citing code in academic papers. It seems like a fine thing to do, but I'm not sure how much it actually helps people get tenure. I guess do it in the hopes that eventually people do get credit for citations to their software packages.

There is separate set of standards for methods consumer producing data analytic code (as opposed to methods code), but I'm not going to get into that here.
<!-- Briefly: those standards should be high^[I mean this lovingly, and also with concern: nothing makes me discount empirical work faster than poor data management or messy analytical code.]. -->

### Methods translation is difficult and largely unincentivized

Writing code for methods consumers is tough. First of all, it takes a lot of software engineering labor. But beyond this, once you start writing hobbyist or production grade code, you start getting evaluated by two different sets of standards at the same time. Methods producers evaluate the technical and theoretical quality of your work. But methods consumers evaluate the quality and usability of your software. This is rough, and also correct. The challenges run deeper yet, because not only are the methods producers and the methods consumers both holding your work to high standards, neither is giving you any academic credit for your work: good software is neither a novel method, nor is it an intriguing piece of empirical work.

And this is the heart of it: translating new methods into software is *infrastructure work*. It enables research, but is rarely research itself.
<!-- 
Instead, methods translators often perform the labor to bridge the gap between software designed for other methods producers and software designed for methods consumers. Some of the rough edges in hobbyist software a direct result of this split audience issue. Hobbyist code is typically written by someone much more background as either a method producer (who often have limited development background) or a method consumer (who often have less technical background). A key failure mode is when users cannot trust correctness of the code, in my experience this happens either because due to software quality issues, or concerns that the developer didn't understand the technical details of the original method fully.  It's important to note that the move from a proof-of-concept implementation to a middling implementation provides no personal benefit to the researcher, the benefit here. Oftentimes, hobbyist quality software is not written by the original methods producer. As a result, methods producers in academia are typically disincentivized from producing this kind of code.

failure mode: can't trust correctness. in my experience this happens for two reasons: software quality, or lacking technical knowledge.
i would feel better about if they were more consistently functional. half of time someone releases a methods package and i take a close look i lose confidence in any empirical results in the paper on the basis of the code -->

### Conclusion

I, presumably like many others, would like more methods software and better methods software. To improve the quality of software in the sciences, I think we need to think carefully about the different reasons that software exists, and to be honest about existing incentivize structures. I've proposed some what I think are a fairly reasonable and acheivable set of expectations for methods producers. Namely, I don't think that methods producers need to be programmers turning out pristine code. Rather, I think methods producers need to produce percursor material that can easily be turned into high quality code. When methods producers try to produce code for methods consumers, it often is not usuable by the those very same methods consumers, and additionally, the attempt to make code more broadly usable often results in work that is challenging to use as the basis for future development work.

I hope that methods consumers can have somewhat more realistic expectations in light of this. I think methods translators are excellent and I would like more of them, but I suspect that tenure committees by and large value research output over software infrastructure, and as such, I don't anticipate a reliable influx of translation labor in the near future. In the more distant future, I am optimistic about that values will evolve, although I still struggle to envision an academy that incentivizes software as living infrastructure rather than one-time outputs.

#### Acknowledgements

My thoughts have been heavily influenced by conversations with [Ben Listyg](https://github.com/benlistyg), who also provided extensive feedback on this post.
