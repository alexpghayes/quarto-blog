@article{alencar2022,
  title = {Bayesian {{Multilateration}}},
  author = {Alencar, Alisson S. C. and Mattos, Cesar L. C. and Gomes, Joao P. P. and Mesquita, Diego},
  year = {2022},
  journal = {IEEE Signal Processing Letters},
  volume = {29},
  pages = {962--966},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2022.3161122},
  url = {https://ieeexplore.ieee.org/document/9739862/},
  urldate = {2024-03-02},
  abstract = {Multilateration (MLAT) is the de facto technique to localize points of interest (POIs) in navigation and surveillance systems. Despite sensors being inherently noisy, most existing techniques i) are oblivious to noise patterns in sensor measurements; and ii) only provide point estimates of the POI. This often results in unreliable estimates with high variance, i.e., that are highly sensitive to measurement noise. To overcome this caveat, we advocate the use of Bayesian modeling. Using Bayesian statistics, we provide a comprehensive guide to handle uncertainties in MLAT, including principled choices for the likelihood function and the prior distributions. Notably, the resulting model is easy to implement and can leverage off-the-shelf Markov Chain Monte Carlo (MCMC) software for inference. Besides coping with unreliable measurements, our framework can also deal with sensors whose location is not completely known, which is an asset in mobile systems. Our solution also naturally incorporates multiple measurements per reference point, a common practical situation that is usually not handled directly by other approaches. Comprehensive experiments with both synthetic and real-world data indicate that our Bayesian approach to the MLAT task provides better position estimation and uncertainty quantification when compared to the available alternatives.},
  langid = {english},
  file = {/home/alex/Zotero/storage/3FICTT7X/Alencar et al. - 2022 - Bayesian Multilateration.pdf}
}

@article{balch2019,
  title = {Satellite Conjunction Analysis and the False Confidence Theorem},
  author = {Balch, Michael Scott and Martin, Ryan and Ferson, Scott},
  year = {2019},
  month = jul,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {475},
  number = {2227},
  pages = {20180565},
  issn = {1364-5021, 1471-2946},
  doi = {10.1098/rspa.2018.0565},
  url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2018.0565},
  urldate = {2020-05-25},
  langid = {english},
  file = {/home/alex/Zotero/storage/WDP444XI/Balch et al. - 2019 - Satellite conjunction analysis and the false confi.pdf}
}

@article{martin2019,
  title = {False Confidence, Non-Additive Beliefs, and Valid Statistical Inference},
  author = {Martin, Ryan},
  year = {2019},
  month = oct,
  journal = {International Journal of Approximate Reasoning},
  volume = {113},
  eprint = {1607.05051},
  pages = {39--73},
  issn = {0888613X},
  doi = {10.1016/j.ijar.2019.06.005},
  url = {http://arxiv.org/abs/1607.05051},
  urldate = {2020-05-25},
  abstract = {Statistics has made tremendous advances since the times of Fisher, Neyman, Jeffreys, and others, but the fundamental and practically relevant questions about probability and inference that puzzled our founding fathers remain unanswered. To bridge this gap, I propose to look beyond the two dominating schools of thought and ask the following three questions: what do scientists need out of statistics, do the existing frameworks meet these needs, and, if not, how to fill the void? To the first question, I contend that scientists seek to convert their data, posited statistical model, etc., into calibrated degrees of belief about quantities of interest. To the second question, I argue that any framework that returns additive beliefs, i.e., probabilities, necessarily suffers from false confidence---certain false hypotheses tend to be assigned high probability---and, therefore, risks systematic bias. This reveals the fundamental importance of non-additive beliefs in the context of statistical inference. But non-additivity alone is not enough so, to the third question, I offer a sufficient condition, called validity, for avoiding false confidence, and present a framework, based on random sets and belief functions, that provably meets this condition. Finally, I discuss characterizations of p-values and confidence intervals in terms of valid non-additive beliefs, which imply that users of these classical procedures are already following the proposed framework without knowing it.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/alex/Zotero/storage/KAC3XV26/Martin - 2019 - False confidence, non-additive beliefs, and valid .pdf}
}

@misc{martin2022,
  title = {Valid and Efficient Imprecise-Probabilistic Inference with Partial Priors, {{I}}. {{First}} Results},
  author = {Martin, Ryan},
  year = {2022},
  month = nov,
  number = {arXiv:2203.06703},
  eprint = {2203.06703},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.06703},
  url = {http://arxiv.org/abs/2203.06703},
  urldate = {2024-11-22},
  abstract = {Between Bayesian and frequentist inference, it's commonly believed that the former is for cases where one has a prior and the latter is for cases where one has no prior. But the prior/no-prior classification isn't exhaustive, and most real-world applications fit somewhere in between these two extremes. That neither of the two dominant schools of thought are suited for these applications creates confusion and slows progress. A key observation here is that ``no prior information'' actually means no prior distribution can be ruled out, so the classically-frequentist context is best characterized as every prior. From this perspective, it's now clear that there's an entire spectrum of contexts depending on what, if any, partial prior information is available, with Bayesian (one prior) and frequentist (every prior) on opposite extremes. This paper ties the two frameworks together by formally treating those cases where only partial prior information is available using the theory of imprecise probability. The end result is a unified framework of (imprecise-probabilistic) statistical inference with a new validity condition that implies both frequentist-style error rate control for derived procedures and Bayesian-style coherence properties, relative to the given partial prior information. This new theory contains both the Bayesian and frequentist frameworks as special cases, since they're both valid in this new sense relative to their respective partial priors. Different constructions of these valid inferential models are considered, and compared based on their efficiency.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/home/alex/Zotero/storage/KWF9TWJB/Martin - 2022 - Valid and efficient imprecise-probabilistic inference with partial priors, I. First results.pdf}
}

@misc{martin2023,
  title = {Valid and Efficient Imprecise-Probabilistic Inference with Partial Priors, {{II}}. {{General}} Framework},
  author = {Martin, Ryan},
  year = {2023},
  month = sep,
  number = {arXiv:2211.14567},
  eprint = {2211.14567},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.14567},
  url = {http://arxiv.org/abs/2211.14567},
  urldate = {2024-11-22},
  abstract = {Bayesian inference requires specification of a single, precise prior distribution, whereas frequentist inference only accommodates a vacuous prior. Since virtually every real-world application falls somewhere in between these two extremes, a new approach is needed. This series of papers develops a new framework that provides valid and efficient statistical inference, prediction, etc., while accommodating partial prior information and imprecisely-specified models more generally. This paper fleshes out a general inferential model construction that not only yields tests, confidence intervals, etc. with desirable error rate control guarantees, but also facilitates valid probabilistic reasoning with de Finetti-style no-sure-loss guarantees. The key technical novelty here is a so-called outer consonant approximation of a general imprecise probability which returns a data- and partial prior-dependent possibility measure to be used for inference and prediction. Despite some potentially unfamiliar imprecise-probabilistic concepts in the development, the result is an intuitive, likelihood-driven framework that will, as expected, agree with the familiar Bayesian and frequentist solutions in the respective extreme cases. More importantly, the proposed framework accommodates partial prior information where available and, therefore, leads to new solutions that were previously out of reach for both Bayesians and frequentists. Details are presented here for a wide range of examples, with more practical details to come in later installments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology,Statistics - Statistics Theory},
  file = {/home/alex/Zotero/storage/QZJ7ZYUE/Martin - 2023 - Valid and efficient imprecise-probabilistic inference with partial priors, II. General framework.pdf}
}

@misc{martin2024d,
  title = {Which Statistical Hypotheses Are Afflicted with False Confidence?},
  author = {Martin, Ryan},
  year = {2024},
  month = apr,
  number = {arXiv:2404.16228},
  eprint = {2404.16228},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.16228},
  url = {http://arxiv.org/abs/2404.16228},
  urldate = {2024-11-23},
  abstract = {The false confidence theorem establishes that, for any data-driven, precise-probabilistic method for uncertainty quantification, there exists (non-trivial) false hypotheses to which the method tends to assign high confidence. This raises concerns about the reliability of these widely-used methods, and shines new light on the consonant belief function-based methods that are provably immune to false confidence. But an existence result alone is insufficient. Towards a partial answer to the title question, I show that, roughly, complements of convex hypotheses are afflicted by false confidence.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/home/alex/Zotero/storage/T6H4TTHT/Martin - 2024 - Which statistical hypotheses are afflicted with false confidence.pdf;/home/alex/Zotero/storage/9TDRNRDB/2404.html}
}

@book{milner2017,
  title = {Pinpoint: How {{GPS}} Is Changing Technology, Culture, and Our Minds},
  shorttitle = {Pinpoint},
  author = {Milner, Greg},
  year = {2017},
  edition = {Paperback edition},
  publisher = {W.W. Norton \& Company},
  address = {New York},
  abstract = {Your Global Positioning System guides you across town; it also helps land planes, and anticipates earthquakes. Milner takes us on a fascinating tour of a hidden system that touches almost every aspect of our modern life, and shows how it has created new forms of human behavior. But the potential misuse of GPS data by government and corporations raise disturbing questions about ethics and privacy. GPS satisfies the scientific urge toward precision-- but may be altering the very nature of human cognition},
  isbn = {978-0-393-35436-2},
  langid = {english},
  annotation = {OCLC: 959872288}
}
