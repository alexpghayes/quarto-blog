---
title: "multilaterion"
subtitle: |
  a clever way to use `stats::nls()` and a modeling problem that seems tricky for bayesians
date: "2024-11-21"
bibliography: multilateration.bib
---

2024-11-23 update: some code in the initial version of this blog post was incorrect, and as a result the second half of the post is now incoherent ðŸ˜¬. Thanks to Jouni Helske for pointing me in the direction of the bug.

## Motivation

In a somewhat unfortunate turn of events my car was totalled a month ago. It was parked on the side of the road, someone driving down the road fell asleep, and then they woke up when they hit my car. Luckily, the driver wasn't injured, and no one else was involved.

The crash happened early in the morning, so I heard it through my window. I made to the crash at almost the same time as the Madison police, which shocked me. It turned out that the driver's iPhone automatically detected a crash and called the police. How did they know where to go?

The real answer, of course, is GPS, but in the interest of telling a simpler story, I'm going to pretend that GPS doesn't exist[^gps-note]. Instead, we'll assume that emergency services can detect how close the iPhone was to several cell towers in the Madison area, and show how these multiple distance measurements can be used to back out the location of the crash, a process known as *trilaterion*, or *multilaterion* when more than three distance measurements are used.

[^gps-note]: For readers interested in learning far more about multilaterion and GPS, I highly recommend *Pinpoint* by Greg Milner.

    [![](pinpoint.jpg){width=35% fig-align="center"}](https://www.goodreads.com/book/show/32191741-pinpoint)

In this post, I'll demonstrate how to perform multilateration using non-linear least squares in R, and then I'll point out a setting where Bayesian methods (at least, my naive approaches) run into fairly substantial problems. I would love to see how Bayesians approach this problem.

## Multilaterion via `stats::nls()` 

Alright, so let's assume that the emergency services department has access to several different distance measurements, each measurement coming from a different [cell tower](https://www.city-data.com/towers/cell-Madison-Wisconsin.html) in the Madison area.

```{r}
#| messsage: false
#| warning: false
#| echo: false
library(brms)
library(ellipse)
library(geodist)
library(glue)
library(leaflet)
library(sf)
library(tidygeocoder)
library(tidyverse)
```

```{r}
#| eval: false
#| echo: false

# pulled from https://www.city-data.com/towers/cell-Madison-Wisconsin.html
cell_towers <- tribble(
  ~address,                ~latitude,  ~longitude,
  "149 Wabesa Street",     43.097222,  -89.342306,
  "1844 Fordem Avenue",    43.096389,  -89.363333,
  "122 W. Main Street",    43.072722,  -89.385194,
  "600 Highland Ave",      43.075556,  -89.431944,
  "1410 Regent Street",    43.068056,  -89.409444
)

breese_stevens_address <- "917 E Mifflin St, Madison, WI 53703"

crash <- tibble(address = breese_stevens_address) |> 
  geocode(address, method = "osm")

# this matches the Stan implementation of havdist later on
havdist <- function(long1, lat1, long2, lat2) {
  long1_rad <- long1 * pi / 180
  lat1_rad <- lat1 * pi / 180
  long2_rad <- long2 * pi / 180
  lat2_rad <- lat2 * pi / 180
  
  diff_long <- (long2_rad - long1_rad)
  diff_lat <- (lat2_rad - lat1_rad)
  
  a <- sin(diff_lat / 2)^2 + cos(lat1_rad) * cos(lat2_rad) * sin(diff_long / 2)^2
  b <- 2 * atan2(sqrt(a), sqrt(1 - a))
  
  6378137 * b
}

distances_check <- cell_towers |>
  mutate(
    dist_meters = drop(
      geodist(
        cell_towers,
        crash,
        measure = "geodesic"
      )
    ),
    havdist_meters = havdist(longitude, latitude, crash$long, crash$lat)
  )

# geodesic and haversine distances match to within a few meters
distances_check
```

```{r}
distances <- tribble(
  ~address,                ~latitude,  ~longitude, ~dist_meters,
  "149 Wabesa Street",     43.097222,  -89.342306,         3008,
  "1844 Fordem Avenue",    43.096389,  -89.363333,         1690,
  "122 W. Main Street",    43.072722,  -89.385194,         1490,
  "600 Highland Ave",      43.075556,  -89.431944,         4794,
  "1410 Regent Street",    43.068056,  -89.409444,         3347
)
```

To find the crash site, we would like to find a point such that distances from the cell towers to the point match the measurements as closely as possible. If we have three or more measurements, the point that minimizes these these deviations is uniquely determined. As [Mike Tuupola](https://www.appelsiini.net/2017/trilateration-with-n-points/) demonstrates, we can use `stats::nls()` to find this minimizer with a minimum of fuss^[It would also be straightforward to solve this problem in Jax, or using a number of other approaches, and `stats::nls()` is a little less stable than you might hope, but the convenience factor is hard to beat, especially for quick scratch work.].

To understand how this works, let's first consider the `geodist::geodist()` function, which is a function that calculates distances between (latitude, longitude) pairs using a performant C++ implementation.

```{r}
mean_latitude <- mean(distances$latitude)
mean_longitude <- mean(distances$longitude)

geodist(
  distances,
  c(
    longitude = mean_longitude,
    latitude = mean_latitude
  ),
  measure = "geodesic"
)
```

We can pass this distance-calculation almost directly to `stats::nls()`, asking for the point that minimizes average deviation from the observed distances^[Note that we do not need to provide any gradient information to `nls()`, and that the `geodist()` can be an essentially arbitrary function, provided the non-linear least squares minimizer is unique.].

```{r}
nls_fit <- nls(

  # dist_meters is the outcome to predict, geodist() is a performant 
  # C++ function that calculates distances between (lat, long) pairs 
  dist_meters ~ geodist(
    distances,
    c(
      longitude = longitude,
      latitude = latitude
    ),
    measure = "geodesic"
  ),

  data = distances,

  # initialize estimated position of crash site as the "average"
  # position of the cell towers
  start = c(
    longitude = mean_longitude,
    latitude = mean_latitude
  ),

  # add a fudge factor to avoid optimization issues
  # related to (near) perfect fits
  control = list(
    scaleOffset = 1
  )
)

nls_fit
```

The estimated parameters are the estimated location of the crash site. We see that the estimated crash site (red) is right at Breese Stevens field, exactly where it should be.

<!-- https://stackoverflow.com/questions/48383990/convert-sequence-of-longitude-and-latitude-to-polygon-via-sf-in-r -->
<!-- https://r-charts.com/spatial/interactive-maps-leaflet/#polygons -->

```{r}
#| message: false
#| code-fold: true
leaflet_data <- distances |>
  mutate(
    color = "blue",
    label_chr = glue(
      "<p>Cell Tower</p>
       <p>{address}</p>
       <p>{round(dist_meters)} meters from crash</p>"
    ),
    label_html = map(label_chr, htmltools::HTML),
  )

leaflet_nls_data <- tibble(
  latitude = coef(nls_fit)["latitude"],
  longitude = coef(nls_fit)["longitude"],
  color = "red",
  label_html = htmltools::HTML("Estimated crash site")
)

base_map <- leaflet() |>
  addTiles() |>
  setView(
    lng = mean_longitude,
    lat = mean_latitude,
    zoom = 13
  ) |>
  addCircleMarkers(
    data = leaflet_data,
    color = leaflet_data$color,
    label = leaflet_data$label_html
  )
  
base_map |>
  addCircleMarkers(
    data = leaflet_nls_data,
    color = leaflet_nls_data$color,
    label = leaflet_nls_data$label_html
  )
```


In this constructed data, the distance measurements are all exact, and so it doesn't make a ton of sense to consider estimation uncertainty, but we can also consider the case where distances are subject to some kind of measurement error or truncation. For instance, suppose that the measurement devices in the cell towers only report two significant digits and round the rest of the data. In this case, we see that there `nls()` estimates, under a Gaussian error model, have substantial uncertainty in the location of the crash site. If you're struggling to see the difference between the maps, try zooming in on the estimated crash sites.

```{r}
#| message: false
#| code-fold: true
distances_rounded <- distances  |>
  mutate(
    dist_meters_rounded = signif(dist_meters, digits = 2)
  ) |> 
  # subset to only the necessary columns of data to avoid nls() errors
  select(latitude, longitude, dist_meters_rounded)

nls_fit_rounded <- nls(
  dist_meters_rounded ~ geodist(
    distances_rounded,
    c(
      longitude = longitude,
      latitude = latitude
    ),
    measure = "geodesic"
  ),
  data = distances_rounded,
  start = c(
    longitude = mean_longitude,
    latitude = mean_latitude
  ),
  control = list(
    scaleOffset = 1
  )
)

# ellipse should be a two-column matrix of longitude, latitude data points
make_leaflet_polygon <- function(ellipse) {
  ellipse |>
    as.data.frame() |>
    set_names(
      c("longitude", "latitude")
    ) |>

    # turn uncertainty ellipse into sf object
    st_as_sf(coords = c("longitude", "latitude"), crs = 4326) |>
    summarize(geometry = st_combine(geometry)) |>
    st_cast("POLYGON") |>

    # convert to leaflet projection
    st_transform(crs = '+proj=longlat +datum=WGS84')
}

nls_ellipse <- nls_fit_rounded |> 
  ellipse() |> 
  make_leaflet_polygon()

leaflet_data_rounded <- distances  |>
  mutate(
    dist_meters_rounded = signif(dist_meters, digits = 2)
  ) |>
  mutate(
    color = "blue",
    label_chr = glue(
      "<p>Cell Tower</p>
       <p>{address}</p>
       <p>{dist_meters_rounded} meters from crash</p>"
    ),
    label_html = map(label_chr, htmltools::HTML),
  )

base_map_rounded <- leaflet() |>
  addTiles() |>
  setView(
    lng = mean_longitude,
    lat = mean_latitude,
    zoom = 13
  ) |>
  addCircleMarkers(
    data = leaflet_data_rounded,
    color = leaflet_data_rounded$color,
    label = leaflet_data_rounded$label_html
  )

base_map_rounded |> 
  addPolygons(
    data = nls_ellipse,
    color = "red",
    stroke = 1,
    label = htmltools::HTML("95% confidence ellipse for crash site")
  )
```

This looks fairly reasonable to me! The uncertainty in the estimate of the crash site, however, could be poorly estimated. In particular, `nls()` is appropriate for Gaussian errors, but here the distances are interval censored, and so we might want to model that more directly.

To my eye, the easiest way to try to model interesting and potentially varied error structure, such as censored responses, is to use `brms`. We can start by matching the analysis we've done so far, and then once we have that working, it should hopefully be easy to iterate and consider various assumptions about errors in distance measurements.

## Reaching for the flexible Bayesian modelling toolkit

To replicate our analysis in Stan, we run into an issue. `geodist::geodist()` is implemented in C++, and the `brms` non-linear interface only supports non-linearities as implemented in either `R` or `Stan` code. We will start by implemented a simpler estimate of geodesic distance, the Haversine distance, in `Stan`. This turns out to look like

```{r}
stan_funs <- "
real havdist(real long1, real lat1, real long2, real lat2) {
  real long1_rad = long1 * pi() / 180;
  real lat1_rad = lat1 * pi() / 180;
  real long2_rad = long2 * pi() / 180;
  real lat2_rad = lat2 * pi() / 180;
  
  real diff_long = (long2_rad - long1_rad);
  real diff_lat = (lat2_rad - lat1_rad);
  
  real a = sin(diff_lat / 2)^2 + cos(lat1_rad) * cos(lat2_rad) * sin(diff_long / 2)^2;
  real b = 2 * atan2(sqrt(a), sqrt(1 - a));
  
  return 6378137 * b;
}
"
```

With this `Stan` code in hand, we specify priors for the latitude and longitude of the crash, and then we'll be ready to model. We'll start with an informative prior that places most of the prior density around the mean latitude and longitude of the cell towers.

```{r}
make_prior <- function(sigma2 = 0.05) {
  c(
    prior_string(
      glue("normal({mean_longitude}, {sigma2})"),
      nlpar = "crashlong"
    ),
    prior_string(
      glue("normal({mean_latitude}, {sigma2})"),
      nlpar = "crashlat"
    )
  )
}

informative_prior <- make_prior(0.001)
uninformative_prior <- make_prior(1)

informative_prior[, 1:2]
```

```{r}
#| message: false
brm_informative <- brm(
  bf(
    dist_meters ~ havdist(longitude, latitude, crashlong, crashlat),
    crashlong + crashlat ~ 1,
    nl = TRUE
  ),
  data = distances,
  stanvars = stanvar(scode = stan_funs, block = "functions"),
  prior =  informative_prior,
  backend = "cmdstanr",
  chains = 4,
  cores = 4,
  seed = 27,
  control = list(adapt_delta = 0.98),
  silent = 2,
  refresh = 0
)
```

This fit seems reasonable (summary omitted for verbosity reasons) and Stan doesn't issue any warnings, so we visualize the result. Unfortunately, the estimated position of the crash site isn't great, and suggests that the Bayesian variant of our model thinks the crash probably happened in a lake, which is impossible.

::: {.callout-tip}
## An unrelated question I've been curious about for a while

Is there any way to express a prior like "the probability of this event is proportional to population density" or "this event must have happened on a road", and then use it is for Bayesian computations in an applied problem?
:::

```{r}
#| message: false
#| code-fold: true
brm_params <- fixef(brm_informative)[, "Estimate"] |>
  set_names(
    c("longitude", "latitude")
  )

brm_ellipse <- ellipse(vcov(brm_informative), center = brm_params) |>
  make_leaflet_polygon()

prior_vcov <- diag(0.001, nrow = 2)
prior_mean <- c(mean_longitude, mean_latitude)

prior_ellipse <- ellipse(prior_vcov, center = prior_mean) |>
  make_leaflet_polygon()

base_map |> 
  addPolygons(
    data = brm_ellipse,
    color = "red",
    stroke = 1,
    label = htmltools::HTML("95% credible ellipse for crash site")
  )
```

Maybe the issue was that our prior was too informative and we can improve the Bayesian estimate with a more diffuse prior that regularizes the estimate less stronger towards the mean latitude and longitude of the towers? I went ahead and did this and I am again omitting the model summary for the sake of brevity, but the diagnostics are terrible and Stan warns us we need stronger priors. If we visualize the 95% credible interval for the crash site we get the following.

```{r}
#| message: false
#| code-fold: true
brm_uninformative <- brm(
  bf(
    dist_meters ~ havdist(longitude, latitude, crashlong, crashlat),
    crashlong + crashlat ~ 1,
    nl = TRUE
  ),
  data = distances,
  stanvars = stanvar(scode = stan_funs, block = "functions"),
  prior =  uninformative_prior,
  backend = "cmdstanr",
  chains = 4,
  cores = 4,
  control = list(adapt_delta = 0.99),
  seed = 27,
  silent = 2,
  refresh = 0
)

brm_uninformative_params <- fixef(brm_uninformative)[, "Estimate"] |>
  set_names(
    c("longitude", "latitude")
  )

brm_uninformative_ellipse <- ellipse(vcov(brm_uninformative), center = brm_uninformative_params) |>
  make_leaflet_polygon()

base_map |> 
  addPolygons(
    data = brm_uninformative_ellipse,
    color = "red",
    stroke = 1,
    label = htmltools::HTML("95% credible ellipse for crash site")
  )
```

**Note:** from here on is incoherent.

This is obviously not what we'd hope for. The crash site is in fact contained in this interval, but we have more than enough data to completely determine the exact location of the crash site. We're only using five observations, and so the Bayesian posterior has only been slightly updated away from the prior, which in this case continues a fair portion of the Midwest.

Is there any way for a Bayesian to get a reasonable estimate of the position of the crash site? I would love to hear from dedicated Bayesians how they would approach this problem^[If you search for "Bayesian Multilaterion," you are likely to come across @alencar2022, which suggests that a Bayesian approach works well. However, the "prior" in this approach is a normal centered on the frequentist point estimate, with variance equal to the corresponding bootstrapped standard error.].

If you care about coverage properties of your credible interval, @balch2019 states are always some parameters that will be poorly covered by Bayesian credible intervals^[Note that the crash site my multilaterion problem is a simpler variant of the satellite collision problem considered in this paper]. @martin2024d discusses which parameters are suspectible to poor coverage in more detail. 
