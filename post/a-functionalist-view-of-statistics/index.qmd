---
title: "a functionalist view of academic statistics"
subtitle: |
  how tenure considerations influence the relationship between statistics and data analysis
date: "2025-06-05"
bibliography: functionalist-view.bib
draft: true
draft-mode: unlinked
---

This post is my attempt to explain some peculiarities about academic statistics. My premise is that academic statistics is fundamentally interested in ensuring its continued existence as an institution, and it does primarily this by producing tenure-track statistics faculty. Tenure considerations thus shape our institutional priorities in ways that may not be obvious to outsiders. I try to illustrate what these considerations are and some of their consequences.

## Incentives in academic statistics

One very natural misconception is that academic statistics is the study of data analysis. This isn't entirely wrong, but it is misleading. Academic statisticians, by and large, are not paid to analyze data, or even to explain how to analyze data. Rather, we are paid to describe how to estimate the parameters of probability models under assumptions that plausibly obtain in the real world. There are other contributions you can make to the statistics literature, but theory work for estimators is canonical. Theoretical work is most easily understood as a statistical contribution; it is idiomatic and socially legible. So academic statisticians are, first and foremost, people who derive theoretical properties of estimators under probability models.

To be concrete, examples of this kind of work including things like:

- developing novel statistical models and showing how to estimate the parameters of those models [@dunson2009],
- deriving [minimax rates](https://www.stat.cmu.edu/~ryantibs/statml/lectures/minimax.pdf) of convergence and finding minimax estimators or statistical tests [@berrett2021a], and
- developing new ways to sample from complicated probability distributions [@neal2003],

amongst others^[See @gelman2020 for a short review of important recent ideas in statistics and @efron2021 for a textbook-length treatment of influential methods].

This emphasis on theoretical and methodological work has some surprising consequences. The first is that academic statistics has an ambivalent relationship to data analysis, by which I mean that we don't actually analyze data very much in our research or in our training (we nonetheless expect to be treated as experts at data analysis). Over the years, there have been numerous calls within the discipline to engage with data analysis more seriously. For instance, nearly 80 years ago Tukey took great care to describe himself as a data analyst first, and a statistician second, and to plead with statisticians to contribute more to data analysis [@tukey1962]. @breiman2001 similarly lambasted statisticians for studying models with little practical relevance. In contrast, at tenure time, one of the best things you can do is have an *Annals* papers on your CV^[That is, *Annals of Statistics* papers. There is a lot of interesting work in *Annals*, but it's very theoretical, and pointing an applied scientist to an *Annals* paper for  guidance is rarely helpful.].  

In statistics graduate programs, the emphasis on mathematical statistics over data analysis is clear, with coursework covering primarily probability, linear algebra and analysis. Most graduate programs in statistics do include a sequence on regression and experimental design, but emphasize derivation of $F$-tests and textbook data examples over, say, data cleaning. Indeed, the fact that statisticians routinely distinguish between "real-world data" and "toy data" is telling. I should also mention that most graduate programs also include a course on statistical consulting. You may be surprised to learn that, for a non-trivial portion of graduate students, this consulting course is their first and only experience working with data . Writing theory papers simply requires different skills than doing data analysis. Data analysis requires computational skills, domain knowledge, and most importantly, the ability to determine if a formal probability model is a reliable representation of the real world. This requires you know a lot about the process that generated the data in the world, a topic notably absent from graduate statistics education^[Perhaps as a consequence, applied statisticians often describe a rule of thumb is that you aren't capable of doing meaningful applied work until you've been working in and learning about a particular domain for at least three years.]. The end result is that statistics graduates actually might not be able to analyze data very well, although of course this varies substantially from program to program.

## Data analysis in statistics papers

When statisticians do work with data, it's useful in one of two ways: as data examples to illustrate how an estimator works, or a complicated standalone data analysis. Crucially, the point of data applications is to help readers understand the method, not the data or the world^[Two representative examples of data applications in my own sub-field are @qing2022 and @chen2020. @qing2022 develops a new network formation model and shows it fits better data better than an older model. There is no really scientific inference on the data itself in the paper. @chen2020 develops a new method for locally clustering social networks, and then applies that method to Twitter data.]^[On plus side of this methodological focus is that statisticians are much less beholden to reviewers wanting p-values below 0.05 or splashy results. Rather, our data applications are evaluated on the correctness of the methodological application, with some indifference to scientific conclusions. This is really, really nice, and I do not envy publication pressures in more scientific fields.]. Data analysis can also sometimes end up in statistics journals. Typically, this happens when there is a complicated dataset and someone comes up with a method specifically to analyze that data. For instance, my own paper @hayes2025a does some methods work, but it's all in the service of analyzing one very specific dataset. To get this type of work published, the analysis needs to be new or interesting or impactful or just very complicated. Biostatistics departments tend to be more applied than statistics departments, and have a lot more high impact data, and so they do this kind of work more than academic statistics proper.

When statisticians do careful analyses on interesting data without statistically novel elements, these papers *do not end up in statistics journals*. They end up in scientific or social scientific journals, and in a lot of departments, the tenure payoff is not proportional to the labor it takes to actually do good science. This ambivalence towards data analysis is frustrating to people trying to do science or learn about the world:

<!-- > As someone who has spent good entire adult life in academic statistics departments I can say with certainty that we have lost the Mandate of Heaven. I would certainly prefer hiring someone trained as an economist or a political scientist if my goal was finding truth from data https://x.com/pli_cachete/status/1836042146505244807 -->

{{< tweet user=pli_cachete id=1836042146505244807 >}}

## Service courses

So far I've described how the graduate curriculum in statistics prioritizes mathematical statistics over data analysis. So what happens in service courses designed for students in different departments? Do those classes teach useful data analysis skills? Here the answer is again mostly no. It is typical to teach a barrage of hypothesis tests and call it a day. I have never heard a statistician recommend one of these courses, and I would not typically describe them as teaching the methods that scientists will actually use in their careers. 

This is understandably a source of animus that flares up on occasion. A notable episode from several years back on Twitter involved a prominent psychologist making the claim that psychologists could teach data analysis better than statisticians:

{{< tweet user=siminevazire id=1144941216259354625 >}}

I think Simine is onto something here, and that the training we provide to graduate students in other fields is rarely helpful, because we often provide the wrong type of training. Statistics courses are designed to produce more tenured statistics faculty, and this is the wrong kind of training for practicing scientists. Improving the practice of statistics is not a training problem, because we are providing the wrong type of training.

## Tools for science

The fallout from the methodological focus in statistics is far-reaching. For instance, since statisticians don't do a ton of data analysis, it's somewhat rare for statisticians to repeatedly use the methods that they develop and to dogfood their own research outputs. Unsurprisingly, a frequent complaint is that methods from academic statistics are not actually useful:

{{< tweet user=Apoorva__Lal id=1836149850444021978 >}}

Outside fields are further frustrated with statisticians because they perceive us as more generally having abdicated any sense of service or responsibility to be helpful to other disciplines. The issue is not malice so much as prioritization. Actual science is just not on our critical path all that often. There is a substantial difference between a *methodological contribution* and a *scientific contribution*, and statisticians are rewarded for methodological contributions. Because of this, statisticians are some risk of solving the wrong problem (i.e., a tractable mathematical problem with little bearing on real life), or communicating insufficiently how our tools work, or never implementing our methods in usable software, or just generally developing a method so complicated that the effort to learn the method is not worthwhile.

## How other fields have responded

Since statisticians don't really do service work, what tends to happen is that in many applied departments, especially in the social sciences, the department will hire a small number of methodologists with a mixture of statistical and domain background. These methodologists teach statistics courses to graduate students in their own departments and do lots of translational labor, turning esoteric theory papers into tutorials and software. I have extremely mixed feelings about this state of affairs. On one hand, I recognize it as a rational response to statistical indifference. And indeed a lot of the work that these folks do is excellent applied statistics. On the other hand, a lot of it is also mediocre, or worse.

## Cultural capital and isolation

An especially ironic consequence of this state of affairs is that statisticians to have minimal impact on how data analysis happens in practice. This is a definite blow to the ego of statisticians, as we would very much like to be seen as and treated like experts. Which, notably, we are. Even if you don't do a lot of data analysis yourself as a statistician, you are still probably far better at it than folks blind firing regressions in the social sciences. It's very easy to feel jaded about this, because a lot of data analysis in science is irredeemably bad, a lot of it is sloppy but probably redeemable, and a small portion is done very well^[Methodologists are on the end of a weird and limited inversion of Brandolini's law, where it is far easier to recognize a bad data analysis than it is to produce a good one. There is a common adage that data science is the intersection of statistical expertise, data manipulation skills, and domain expertise, and that you need all three of these skills to analyze data well. To realize that a data analysis is hopelessly flawed, statistical expertise alone suffices. Small surprise that methodologists are often in a bad mood and grumpy at the state of the world. To be a methodologist with any sense of taste means frequent aesthetic pain.]. Here is, for instance, a somewhat hyperbolic statement that is clearly inspired by too much, rather than too little, contact with reality:

<!-- > Almost all the statistics used in econometrics is completely wrong, to the extent its questionable whether the field actually understands it at all. This is why the discipline of statistics persists https://x.com/drStuartGilmour/status/1836312249344077831 -->

{{< tweet user=drStuartGilmour id=1836312249344077831 >}}

When academic statistics is in fact interested in influencing the practice of statistics, we are often hamstrung by the fact that we are a somewhat insular institution, and that we have rarely spent developing cultural capital. In some sense, the field of statistics is constantly in a state of crisis, on the verge of irrelevance. On the other hand, there is never any risk of irrelevance because the practice of statistics in science is far from optimal.

## A summary in stylized facts

I am especially interested in how statistics relates to other disciplines, and I think a reasonable summary goes along these lines:

- Academics respond strongly to tenure incentives, which are often opaque and very slow to change
- Academic statistics has largely opted out of service responsibilities to other disciplines. It's not that we don't do service, but rather that statistical consulting is not a huge priority and service courses largely aren't useful.
- we are often seen as, at best, aloof, or, at worst, apathetic and indifferent (c.f. computer science, economics)
- Other disciplines have responded by building in-house statistical capacity of variable quality, and by ignoring statistics proper to some extent.
- A lot of statistics isn't useful to scientists for various reasons.
- A lot of data analysis by scientists is quite bad.
- Frustrations abound on all sides. Scientists want help. Statisticians want to be taken seriously.

I also want to temper the slightly pessimistic sketch above with the observations that most statisticians I know genuinely want to develop and teach useful methods, most scientists I know genuinely want to do good science, and science and statistics on the whole are successful enterprises that have clearly made enormous progress over the last decades.

## Some comments

Personally, I am interested in what it would take to improve the practice of statistics, and thereby the quality of scientific research. If other statisticians feel similarly, and I suspect many do, it feels to me like a reasonable time to contemplate how we want to relate to other fields. Are we interested in the technical problem of producing a rich statistical literature, or the social problem of producing a rich practice of statistics? Do our tenure review practices reflect these priorities? What would statistics look like if tenure packets should be evaluated for tutorial papers, for scientific contributions, for code, for outreach work?

I don't think there are necessarily right or wrong choices, but there are certainly tradeoffs. My view is that statistics currently prioritizes a rich statistical literature, and the cost of this is that we have a less rich statistical practice in the sciences. Regardless of our precise priorities, I think it is important to articulate them precisely and explain them to others, so that outsiders can understand what is on offer and what is not. Some sets of priorities may lead to conflicts that are difficult to resolve. For instance, it is somewhat rude to point out bad data analysis without providing meaningful resources to learn good data analysis. It is also important to point out bad science. I find these tensions worth exploring and explaining.

Regardless, I am optimistic about the future. I suspect the data science revolution has pushed statistics in a more applied direction. At the very least, statisticians are somewhat more aware of the potential for our own irrelevance, especially at the hands of computer scientists building large-scale prediction systems. Statistics departments grew substantially during the data science boom, and the field feels invigorated and open to new priorities at the moment. 

### Acknowledgements

I would once again like to thank [Ben Listyg](https://github.com/benlistyg) for helpful comments on this post.

<!-- Cognitive limits, scaffolds, familiarity/super users as dramatically misjudging the depth of their schema/scaffolds, at least in math. Why shared understanding is hard. Why XXXXX is hard to work with. Why I myself am hard to work with on projects outside of my expertise -->

<!-- Another thing I have been thinking about recently: it surprises me that statistics departments do not offer classes that require little scaffolding. A stat 101 course on critical thinking/quantitative reasoning, along the lines of Art of Statistics or calling bullshit; i.e. a class design for people who are exclusively going to consume, but never produce, statistics, and who don't know any math. this would be a super valuable thing for us to do and for some reason we do not. -->



<!-- the replications crisis people were like "the house is on fire, look at all these questionable research practices and outright fraud" in fields where, from a statistical viewpoint, there is no need to distinguish between fraudulent work and non-fraudulent work because the quality is often indistinguishable. also, it's been clear that the house has been on fire to outsiders for years, so the loud yelling all of a sudden is a bit irritating. -->


<!-- 
- every field has it's own data analytic traditions
- don't get that much data analysis training, folks stumbling along
- teaching data analysis within own department, taught by methodologists -->

<!-- 

other fields to comment on:
- psychology
- econometrics
- computer science
- applied math & engineering


## Collaboration does not solve the misaligned incentives problem

- doesn't super work to develop methods -- shared understanding hard to develop with social scientists missing years of training (hourly rates meme)
- outsourcing data analysis can work, but then once again in the misaligned incentives category


some interdisciplinary collaborations are great. that said, past experiences lead me to be extremely hesitant to collaborate on methodology -- outsource your problem to me and i solve it independently

 Some weird dynamics when collaborating across disciplinary boundaries

seems to work best when data analysis is fully outsourced
joint methods development has, so far i can tell, mostly been a disaster

really hard to get to shared understanding of the project

- maybe you want help analyzing your data, and you do reach out to a statistican, at which point you discover: they maybe think your whole field is non-sense, they are maybe a grumpy methodologist, and they aren't incentivized to publish with you, so your collaboration is going to be a low priority for them. even if they are super excited to do data analytic work with you, pre-tenure folks are still going to be subject to incentives that are not in favor of collaboration.

near-universal adage that working with statisticians is a royal pain in the ass. slow. unenthused, etc.

Here's a rough model of how things actually work:

statistics culture: somewhat nitpicky, apathetic towards other, and not particularly charismatic. sociologically outcompeted at every turn by computer scientists, who build impressive large systems that kick ass at prediction, who have money, who are charisma, and tend not to sweat the inferential details, actually market their work, etc, etc

- joint methods development with social scientists is often quite painful. hourly rates meme
- what passes muster in social sciences is desk reject worthy in statistics
- i have been involved in 2-3 collaborative projects where i bowed out because the methodological content of the projects would have been embarrassing and potentially detrimental to my career to attach my name to. weird weird dynamics here when getting paid to help a social scientist with a thing.

Advice for people who need statistical help:
- pay statisticians
- do a transactional data analysis for authorship trade
- make sure the statistician understands the scientific question
 -->
