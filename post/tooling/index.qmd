---
title: "questions for the future of statistics"
subtitle: |
  the purpose, audience, and value of methods software
date: "2024-11-14"
categories: [statistical software]
draft: true
---


largely concerned with the programming ergonomics of our tooling
also concerned with conceptual ergonomics, that fact that people don't use things that they don't understand, and we continue to develop more and more complicated methods that require increasingly levels of training to grok

- how can any statistical methods compete with *ergonomics* of `stats::lm()` or `Jax` or `brms()`? inferentially, are we offering substantial improvements over what is available in the most ergonomic toolkits?

- when does it actually matter that we get a good and careful estimate of uncertainty? when do we need something beyond a highly predictive estimate?

- is a practitioner going to reach for a neural net over the tool that you are developing?




these questions i think reflect a substantial amount of anxiety about the methods that applied folks use, the training that they recieve


- prediction is largely solved, mostly by neural nets

experiments, causal

- what kinds of questions can we reduce to prediction problems?

- for folks who do want to use neural networks but don't want to fiddle with the 80 million knobs, how we can we incorporate neural models seriously into our work without incurring an enormous headache

- how should we think about embeddings? what can we use them for? is inference based on neural embeddings well-specified?

- are our methods getting too complicated to teach to the people who we want to use them?

- how parametric do we want to be? is it good that folks are increasingly afraid of (semi-)parametric modeling assumptions in favor of non-parametric assumptions? if folks don't have any intuition for how non-parametric methods work, and they are complicated, why would anyone want to use them?

- pedagogical, can we construct a two-course sequence that leads from the gaussian linear model and logistic regression to doubly-robust causal estimators using machine learning and cross-fitting?

- VS Code
